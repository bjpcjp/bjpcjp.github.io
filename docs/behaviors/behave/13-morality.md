# 13-morality

T
Thirteen
Morality and Doing the Right
Thing, Once You’ve Figured
Out What That Is
he two previous chapters examined the thoroughly unique contexts for
some human behaviors that are on a continuum with behaviors in other
species. Like some other species, we make automatic Us/Them dichotomies and
favor the former—though only humans rationalize that tendency with ideology.
Like many other species, we are implicitly hierarchical—though only humans
view the gap between haves and have-nots as a divine plan.
This chapter considers another domain rife with human uniqueness, namely
morality. For us, morality is not only belief in norms of appropriate behavior but
also the belief that they should be shared and transmitted culturally.
Work in the field is dominated by a familiar sort of question. When we make
a decision regarding morality, is it mostly the outcome of moral reasoning or of
moral intuition? Do we mostly think or feel our way to deciding what is right?
This raises a related question. Is human morality as new as the cultural
institutions we’ve hatched in recent millennia, or are its rudiments a far older
primate legacy?
This raises more questions. What’s more impressive, consistencies and
universalities of human moral behavior or variability and its correlation with
cultural and ecological factors?
Finally, there will be unapologetically prescriptive questions. When it comes
to moral decision making, when is it “better” to rely on intuition, when on
reasoning? And when we resist temptation, is it mostly an act of will or of grace?
People have confronted these issues since students attended intro philosophy
in togas. Naturally, these questions are informed by science.
O
THE PRIMACY OF REASONING IN MORAL
DECISION MAKING
ne single fact perfectly demonstrates moral decision making being based
on cognition and reasoning. Have you ever picked up a law textbook?
They’re humongous.
Every society has rules about moral and ethical behavior that are reasoned
and call upon logical operations. Applying the rules requires reconstructing
scenarios, understanding proximal and distal causes of events, and assessing
magnitudes and probabilities of consequences of actions. Assessing individual
behavior requires perspective taking, Theory of Mind, and distinguishing
between outcome and intent. Moreover, in many cultures rule implementation is
typically entrusted to people (e.g., lawyers, clergy) who have undergone long
training.
Harking back to chapter 7, the primacy of reasoning in moral decision
making is anchored in child development. The Kohlbergian emergence of
increasingly complex stages of moral development is built on the Piagetian
emergence of increasingly complex logical operations. They are similar,
neurobiologically. Logical and moral reasoning about the correctness of an
economic or ethical decision, respectively, both activate the (cognitive) dlPFC.
People with obsessive-compulsive disorder get mired in both everyday decision
making and moral decision making, and their dlPFCs go wild with activity for
both.1
Similarly, there’s activation of the temporoparietal junction (TPJ) during
Theory of Mind tasks, whether they are perceptual (e.g., visualizing a complex
scene from another viewer’s perspective), amoral (e.g., keeping straight who’s in
love with whom in A Midsummer Night’s Dream), or moral/social (e.g., inferring
the ethical motivation behind a person’s act). Moreover, the more the TPJ
activation, the more people take intent into account when making moral
judgments, particularly when there was intent to harm but no actual harm done.
Most important, inhibit the TPJ with transcranial magnetic stimulation, and
subjects become less concerned about intent.2
The cognitive processes we bring to moral reasoning aren’t perfect, in that
there are fault lines of vulnerability, imbalances, and asymmetries.3 For example,
doing harm is worse than allowing it—for equivalent outcomes we typically
judge commission more harshly than omission and must activate the dlPFC more
to judge them as equal. This makes sense—when we do one thing, there are
innumerable other things we didn’t do; no wonder the former is psychologically
weightier. As another cognitive skew, as discussed in chapter 10, we’re better at
detecting violations of social contracts that have malevolent rather than
benevolent consequences (e.g., giving less versus more than promised). We also
search harder for causality (and come up with more false attributions) for
malevolent than for benevolent events.
This was shown in one study. First scenario: A worker proposes a plan to the
boss, saying, “If we do this, there’ll be big profits, and we’ll harm the
environment in the process.” The boss answers: “I don’t care about the
environment. Just do it.” Second scenario: Same setup, but this time there’ll be
big profits and benefits to the environment. Boss: “I don’t care about the
environment. Just do it.” In the first scenario 85 percent of subjects stated that
the boss harmed the environment in order to increase profits; however, in the
second scenario only 23 percent said that the boss helped the environment in
order to increase profits.4
—
Okay, we’re not perfect reasoning machines. But that’s our goal, and numerous
moral philosophers emphasize the preeminence of reasoning, where emotion and
intuition, if they happen to show up, just soil the carpet. Such philosophers range
from Kant, with his search for a mathematics of morality, to Princeton
philosopher Peter Singer, who kvetches that if things like sex and bodily
functions are pertinent to philosophizing, time to hang up his spurs: “It would be
best to forget all about our particular moral judgments.” Morality is anchored in
reason.5
E
YEAH, SURE IT IS: SOCIAL INTUITIONISM
xcept there’s a problem with this conclusion—people often haven’t a clue
why they’ve made some judgment, yet they fervently believe it’s correct.
This is straight out of chapter 11’s rapid implicit assessments of Us versus
Them and our post-hoc rational justifications for visceral prejudice. Scientists
studying moral philosophy increasingly emphasize moral decision making as
implicit, intuitive, and anchored in emotion.
The king of this “social intuitionist” school is Jonathan Haidt, whom we’ve
encountered previously.6 Haidt views moral decisions as primarily based on
intuition and believes reasoning is what we then use to convince everyone,
including ourselves, that we’re making sense. In an apt phrase of Haidt’s, “moral
thinking is for social doing,” and sociality always has an emotional component.
The evidence for the social intuitionist school is plentiful:
When contemplating moral decisions, we don’t just activate the
eggheady dlPFC.7 There’s also activation of the usual emotional cast—
amygdala, vmPFC and the related orbitofrontal cortex, insular cortex,
anterior cingulate. Different types of moral transgressions preferentially
activate different subsets of these regions. For example, moral
quandaries eliciting pity preferentially activate the insula; those eliciting
indignation activate the orbitofrontal cortex. Quandaries generating
intense conflict preferentially activate the anterior cingulate. Finally, for
acts assessed as equally morally wrong, those involving nonsexual
transgression (e.g., stealing from a sibling) activate the amygdala,
whereas those involving sexual transgressions (e.g., sex with a sibling)
also activate the insula.*
Moreover, when such activation is strong enough, we also activate the
sympathetic nervous system and feel arousal—and we know how those
peripheral effects feedback and influence behavior. When we confront a
moral choice, the dlPFC doesn’t adjudicate in contemplative silence. The
waters roil below.
The pattern of activation in these regions predicts moral decisions better
than does the dlPFC’s profile. And this matches behavior—people
punish to the extent that they feel angered by someone acting
unethically.8
People tend toward instantaneous moral reactions; moreover, when
subjects shift from judging nonmoral elements of acts to moral ones,
they make assessments faster, the antithesis of moral decision making
being about grinding cognition. Most strikingly, when facing a moral
quandary, activation in the amygdala, vmPFC, and insula typically
precedes dlPFC activation.9
Damage to these intuitionist brain regions makes moral judgments more
pragmatic, even coldhearted. Recall from chapter 10 how people with
damage to the (emotional) vmPFC readily advocate sacrificing one
relative to save five strangers, something control subjects never do.
Most telling is when we have strong moral opinions but can’t tell why,
something Haidt calls “moral dumbfounding”—followed by clunky
post-hoc rationalizing.10 Moreover, such moral decisions can differ
markedly in different affective or visceral circumstances, generating very
different rationalizations. Recall from the last chapter how people
become more conservative in their social judgments when they’re
smelling a foul odor or sitting at a dirty desk. And then there’s that
doozy of a finding—knowing a judge’s opinions about Plato, Nietzsche,
Rawls, and any other philosopher whose name I just looked up gives you
less predictive power about her judicial decisions than knowing if she’s
hungry.
The social intuitionist roots of morality are bolstered further by evidence of
moral judgment in two classes of individuals with limited capacities for moral
reasoning.
M
AGAIN WITH BABIES AND ANIMALS
uch as infants demonstrate the rudiments of hierarchical and Us/Them
thinking, they possess building blocks of moral reasoning as well. For
starters, infants have the bias concerning commission versus omission. In one
clever study, six-month-olds watched a scene containing two of the same objects,
one blue and one red; repeatedly, the scene would show a person picking the
blue object. Then, one time, the red one is picked. The kid becomes interested,
looks more, breathes faster, showing that this seems discrepant. Now, the scene
shows two of the same objects, one blue, one a different color. In each repetition
of the scene, a person picks the one that is not blue (its color changes with each
repetition). Suddenly, the blue one is picked. The kid isn’t particularly interested.
“He always picks the blue one” is easier to comprehend than “He never picks the
blue one.” Commission is weightier.11
Infants and toddlers also have hints of a sense of justice, as shown by Kiley
Hamlin of the University of British Columbia, and Paul Bloom and Karen Wynn
of Yale. Six- to twelve-month-olds watch a circle moving up a hill. A nice
triangle helps to push it. A mean square blocks it. Afterward the infants can
reach for a triangle or a square. They choose the triangle.* Do infants prefer nice
beings, or shun mean ones? Both. Nice triangles were preferred over neutral
shapes, which were preferred over mean squares.
Such infants advocate punishing bad acts. A kid watches puppets, one good,
one bad (sharing versus not). The child is then presented with the puppets, each
sitting on a pile of sweets. Who should lose a sweet? The bad puppet. Who
should gain one? The good puppet.
Remarkably, toddlers even assess secondary punishment. The good and bad
puppets then interact with two additional puppets, who can be nice or bad. And
whom did kids prefer of those second-layer puppets? Those who were nice to
nice puppets and those who punished mean ones.
Other primates also show the beginnings of moral judgments. Things started
with a superb 2003 paper by Frans de Waal and Sarah Brosnan.12 Capuchin
monkeys were trained in a task: A human gives them a mildly interesting small
object—a pebble. The human then extends her hand palm up, a capuchin
begging gesture. If the monkey puts the pebble in her hand, there’s a food
reward. In other words, the animals learned how to buy food.
Now there are two capuchins, side by side. Each gets a pebble. Each gives it
to the human. Each gets a grape, very rewarding.
Now change things. Both monkeys pay their pebble. Monkey 1 gets a grape.
But monkey 2 gets some cucumber, which blows compared with grapes—
capuchins prefer grapes to cucumber 90 percent of the time. Monkey 2 was
shortchanged.
And monkey 2 would then typically fling the cucumber at the human or bash
around in frustration. Most consistently, they wouldn’t give the pebble the next
time. As the Nature paper was entitled, “Monkeys reject unequal pay.”
This response has since been demonstrated in various macaque monkey
species, crows, ravens, and dogs (where the dog’s “work” would be shaking her
paw).*13
Subsequent work by Brosnan, de Waal, and others fleshed out this
phenomenon further:14
One criticism of the original study was that maybe capuchins
refused to work for cucumbers because grapes were visible,
regardless of whether the other guy was getting paid in grapes.
But no—the phenomenon required unfair payment.
Both animals are getting grapes, then one gets switched to
cucumber. What’s key—that the other guy is still getting grapes,
or that I no longer am? The former—if doing the study with a
single monkey, switching from grapes to cucumbers would not
evoke refusal. Nor would it if both monkeys got cucumbers.
Across the various species, males were more likely than females
to reject “lower pay”; dominant animals were more likely than
subordinates to reject.
It’s about the work—give one monkey a free grape, the other
free cucumber, and the latter doesn’t get pissed.
The closer in proximity the two animals are, the more likely the
one getting cucumber is to go on strike.
Finally, rejection of unfair pay isn’t seen in species that are
solitary (e.g., orangutans) or have minimal social cooperation
(e.g., owl monkeys).
Okay, very impressive—other social species show hints of a sense of justice,
reacting negatively to unequal reward. But this is worlds away from juries
awarding money to plaintiffs harmed by employers. Instead it’s self-interest
—“This isn’t fair; I’m getting screwed.”
How about evidence of a sense of fairness in the treatment of another
individual? Two studies have examined this in a chimp version of the Ultimatum
Game. Recall the human version—in repeated rounds, player 1 in a pair decides
how money is divided between the two of them. Player 2 is powerless in the
decision making but, if unhappy with the split, can refuse, and no one gets any
money. In other words, player 2 can forgo immediate reward to punish selfish
player 1. As we saw in chapter 10, Player 2s tend to accept 60:40 splits.
In the chimp version, chimp 1, the proposer, has two tokens. One indicates
that each chimp gets two grapes. The other indicates that the proposer gets three
grapes, the partner only one. The proposer chooses a token and passes it to
chimp 2, who then decides whether to pass the token to the human grape
dispenser. In other words, if chimp 2 thinks chimp 1 is being unfair, no one gets
grapes.
In one such study, Michael Tomasello (a frequent critic of de Waal—stay
tuned) at the Max Planck Institutes in Germany, found no evidence of chimp
fairness—the proposer always chose, and the partner always accepted unfair
splits.15 De Waal and Brosnan did the study in more ethologically valid
conditions and reported something different: proposer chimps tended toward
equitable splits, but if they could give the token directly to the human (robbing
chimp 2 of veto power), they’d favor unfair splits. So chimps will opt for fairer
splits—but only when there is a downside to being unfair.
Sometimes other primates are fair when it’s at no cost to themselves. Back to
capuchin monkeys. Monkey 1 chooses whether both he and the other guy get
marshmallows or it’s a marshmallow for him and yucky celery for the other guy.
Monkeys tended to choose marshmallows for the other guy.* Similar “other-
regarding preference” was shown with marmoset monkeys, where the first
individual got nothing and merely chose whether the other guy got a cricket to
eat (of note, a number of studies have failed to find other-regarding preference in
chimps).16
Really interesting evidence for a nonhuman sense of justice comes in a small
side study in a Brosnan/de Waal paper. Back to the two monkeys getting
cucumbers for work. Suddenly one guy gets shifted to grapes. As we saw, the
one still getting the cucumber refuses to work. Fascinatingly, the grape mogul
often refuses as well.
What is this? Solidarity? “I’m no strike-breaking scab”? Self-interest, but
with an atypically long view about the possible consequences of the cucumber
victim’s resentment? Scratch an altruistic capuchin and a hypocritical one
bleeds? In other words, all the questions raised by human altruism.
Given the relatively limited reasoning capacities of monkeys, these findings
support the importance of social intuitionism. De Waal perceives even deeper
implications—the roots of human morality are older than our cultural
institutions, than our laws and sermons. Rather than human morality being
spiritually transcendent (enter deities, stage right), it transcends our species
boundaries.17
M
MR. SPOCK AND JOSEPH STALIN
any moral philosophers believe not only that moral judgment is built on
reasoning but also that it should be. This is obvious to fans of Mr. Spock,
since the emotional component of moral intuitionism just introduces
sentimentality, self-interest, and parochial biases. But one remarkable finding
counters this.
Relatives are special. Chapter 10 attests to that. Any social organism would
tell you so. Joseph Stalin thought so concerning Pavlik Morozov ratting out his
father. As do most American courts, where there is either de facto or de jure
resistance to making someone testify against their own parent or child. Relatives
are special. But not to people lacking social intuitionism. As noted, people with
vmPFC damage make extraordinarily practical, unemotional moral decisions.
And in the process they do something that everyone, from clonal yeast to Uncle
Joe to the Texas Rules of Criminal Evidence considers morally suspect: they
advocate harming kin as readily as strangers in an “Is it okay to sacrifice one
person to save five?” scenario.18
Emotion and social intuition are not some primordial ooze that gums up that
human specialty of moral reasoning. Instead, they anchor some of the few moral
judgments that most humans agree upon.
S
CONTEXT
o social intuitions can have large, useful roles in moral decision making.
Should we now debate whether reasoning or intuition is more important?
This is silly, not least of all because there is considerable overlap between the
two. Consider, for example, protesters shutting down a capital to highlight
income inequity. This could be framed as the Kohlbergian reasoning of people in
a postconventional stage. But it could also be framed à la Haidt in a social
intuitionist way—these are people who resonate more with moral intuitions
about fairness than with respect for authority.
More interesting than squabbling about the relative importance of reasoning
and intuition are two related questions: What circumstances bias toward
emphasizing one over the other? Can the differing emphases produce different
decisions?
As we’ve seen, then–graduate student Josh Greene and colleagues helped
jump-start “neuroethics” by exploring these questions using the poster child of
“Do the ends justify the means?” philosophizing, namely the runaway trolley
problem. A trolley’s brake has failed, and it is hurtling down the tracks and will
hit and kill five people. Is it okay to do something that saves the five but kills
someone else in the process?
People have pondered this since Aristotle took his first trolley ride;* Greene
et al. added neuroscience. Subjects were neuroimaged while pondering trolley
ethics. Crucially, they considered two scenarios. Scenario 1: Here comes the
trolley; five people are goners. Would you pull a lever that diverts the trolley
onto a different track, where it will hit