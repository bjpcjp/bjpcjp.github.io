# ch19-illusion-understand-TFaS

The Illusion of Understanding
The trader-philosopher-statistician Nassim Taleb could also be
considered a psychologist. In The Black Swan, Taleb introduced the notion
of a narrative fallacy to describe how flawed stories of the past shape our
views of the world and our expectations for the future. Narrative fallacies
arise inevitably from our continuous attempt to make sense of the world.
The explanatory stories that people find compelling are simple; are
concrete rather than abstract; assign a larger role to talent, stupidity, and
intentions than to luck; and focus on a few striking events that happened
rather than on the countless events that failed to happen. Any recent salient
event is a candidate to become the kernel of a causal narrative. Taleb
suggests that we humans constantly fool ourselves by constructing flimsy
accounts of the past and believing they are true.
Good stories provide a simple and coherent account >
A compelling narrative fosters an illusion of inevitability. Consider the
story of how Google turned into a giant of the technology industry. Two
creative graduate students in the computer science department at
Stanford University come up with a superior way of searching information
on the Internet. They seek and obtain funding to start a company and make
a series of decisions that work out well. Within a few years, the company
they started is one of the most valuable stocks in America, and the two
former graduate students are among the richest people on the planet. On
one memorable occasion, they were lucky, which makes the story even
more compelling: a year after founding Google, they were willing to sell
their company for less than $1 million, but the buyer said the price was too
high. Mentioning the single lucky incident actually makes it easier to
underestimate the multitude of ways in which luck affected the outcome.
A detailed history would specify the decisions of Google’s founders, but
for our purposes it suffices to say that almost every choice they made had
a good outcome. A more complete narrative would describe the actions of
the firms that Google defeated. The hapless competitors would appear to
be blind, slow, and altogether inadequate in dealing with the threat that
eventually overwhelmed them.
I intentionally told this tale blandly, but you get the idea: there is a very
good story here. Fleshed out in more detail, the story could give you the
sense that you understand what made Google succeed; it would also
make you feel that you have learned a valuable general lesson about what
makes businesses succeed. Unfortunately, there is good reason to believe
that your sense of understanding and learning from the Google story is
largely illusory. The ultimate test of an explanation is whether it would have
made the event predictable in advance. No story of Google’s unlikely
success will meet that test, because no story can include the myriad of
events that would have caused a different outcome. The human mind does
not deal well with nonevents. The fact that many of the important events that
did occur involve choices further tempts you to exaggerate the role of skill
and underestimate the part that luck played in the outcome. Because every
critical decision turned out well, the record suggests almost flawless
prescience—but bad luck could have disrupted any one of the successful
steps. The halo effect adds the final touches, lending an aura of invincibility
to the heroes of the story.
Like watching a skilled rafter avoiding one potential calamity after
another as he goes down the rapids, the unfolding of the Google story is
thrilling because of the constant risk of disaster. However, there is foр an
instructive difference between the two cases. The skilled rafter has gone
down rapids hundreds of times. He has learned to read the roiling water in
front of him and to anticipate obstacles. He has learned to make the tiny
adjustments of posture that keep him upright. There are fewer
opportunities for young men to learn how to create a giant company, and
fewer chances to avoid hidden rocks—such as a brilliant innovation by a
competing firm. Of course there was a great deal of skill in the Google
story, but luck played a more important role in the actual event than it does
in the telling of it. And the more luck was involved, the less there is to be
learned.
At work here is that powerful WY SIATI rule. You cannot help dealing with
the limited information you have as if it were all there is to know. You build
the best possible story from the information available to you, and if it is a
good story, you believe it. Paradoxically, it is easier to construct a coherent
story when you know little, when there are fewer pieces to fit into the puzzle.
Our comforting conviction that the world makes sense rests on a secure
foundation: our almost unlimited ability to ignore our ignorance.
I have heard of too many people who “knew well before it happened that
the 2008 financial crisis was inevitable.” This sentence contains a highly
objectionable word, which should be removed from our vocabulary in
discussions of major events. The word is, of course, knew. Some people
thought well in advance that there would be a crisis, but they did not know
it. They now say they knew it because the crisis did in fact happen. This is
a misuse of an important concept. In everyday language, we apply the
word know only when what was known is true and can be shown to be true.
We can know something only if it is both true and knowable. But the people
who thought there would be a crisis (and there are fewer of them than now
remember thinking it) could not conclusively show it at the time. Many
intelligent and well-informed people were keenly interested in the future of
the economy and did not believe a catastrophe was imminent; I infer from
this fact that the crisis was not knowable. What is perverse about the use
of know in this context is not that some individuals get credit for prescience
that they do not deserve. It is that the language implies that the world is
more knowable than it is. It helps perpetuate a pernicious illusion.
The core of the illusion is that we believe we understand the past, which
implies that the future also should be knowable, but in fact we understand
the past less than we believe we do. Know is not the only word that fosters
this illusion. In common usage, the words intuition and premonition also
are reserved for past thoughts that turned out to be true. The statement “I
had a premonition that the marriage would not last, but I was wrong”
sounds odd, as does any sentence about an intuition that turned out to be
false. To think clearly about the future, we need to clean up the language
that we use in labeling the beliefs we had in the past.
The Social Costs of Hindsight
The mind that makes up narratives about the past is a sense-making
organ. When an unpredicted event occurs, we immediately adjust our view
of the world to accommodate the surprise. Imagine yourself before a
football game between two teams that have the same record of wins and
losses. Now the game is over, and one team trashed the other. In your
revised model of the world, the winning team is much stronger than the
loser, and your view of the past as well as of the future has been altered be
fрy that new perception. Learning from surprises is a reasonable thing to
do, but it can have some dangerous consequences.
A general limitation of the human mind is its imperfect ability to
reconstruct past states of knowledge, or beliefs that have changed. Once
you adopt a new view of the world (or of any part of it), you immediately
lose much of your ability to recall what you used to believe before your
mind changed.
Many psychologists have studied what happens when people change
their minds. Choosing a topic on which minds are not completely made up
—say, the death penalty—the experimenter carefully measures people’s
attitudes. Next, the participants see or hear a persuasive pro or con
message. Then the experimenter measures people’s attitudes again; they
usually are closer to the persuasive message they were exposed to.
Finally, the participants report the opinion they held beforehand. This task
turns out to be surprisingly difficult. Asked to reconstruct their former
beliefs, people retrieve their current ones instead—an instance of
substitution—and many cannot believe that they ever felt differently.
Your inability to reconstruct past beliefs will inevitably cause you to
underestimate the extent to which you were surprised by past events.
Baruch Fischh off first demonstrated this “I-knew-it-all-along” effect, or
hindsight bias, when he was a student in Jerusalem. Together with Ruth
Beyth (another of our students), Fischh off conducted a survey before
President Richard Nixon visited China and Russia in 1972. The
respondents assigned probabilities to fifteen possible outcomes of
Nixon’s diplomatic initiatives. Would Mao Zedong agree to meet with
Nixon? Might the United States grant diplomatic recognition to China?
After decades of enmity, could the United States and the Soviet Union
agree on anything significant?
After Nixon’s return from his travels, Fischh off and Beyth asked the
same people to recall the probability that they had originally assigned to
each of the fifteen possible outcomes. The results were clear. If an event
had actually occurred, people exaggerated the probability that they had
assigned to it earlier. If the possible event had not come to pass, the
participants erroneously recalled that they had always considered it
unlikely. Further experiments showed that people were driven to overstate
the accuracy not only of their original predictions but also of those made by
others. Similar results have been found for other events that gripped public
attention, such as the O. J. Simpson murder trial and the impeachment of
President Bill Clinton. The tendency to revise the history of one’s beliefs in
light of what actually happened produces a robust cognitive illusion.
Hindsight bias has pernicious effects on the evaluations of decision
makers. It leads observers to assess the quality of a decision not by
whether the process was sound but by whether its outcome was good or
bad. Consider a low-risk surgical intervention in which an unpredictable
accident occurred that caused the patient’s death. The jury will be prone to
believe, after the fact, that the operation was actually risky and that the
doctor who ordered it should have known better. This outcome bias makes
it almost impossible to evaluate a decision properly—in terms of the
beliefs that were reasonable when the decision was made.
Hindsight is especially unkind to decision makers who act as agents for
others—physicians, financial advisers, third-base coaches, CEOs, social
workers, diplomats, politicians. We are prone to blame decision makers
for good decisions that worked out badly and to give them too little credit
for successful movesecaр that appear obvious only after the fact. There is
a clear outcome bias. When the outcomes are bad, the clients often blame
their agents for not seeing the handwriting on the wall—forgetting that it
was written in invisible ink that became legible only afterward. Actions that
seemed prudent in foresight can look irresponsibly negligent in hindsight.
Based on an actual legal case, students in California were asked whether
the city of Duluth, Minnesota, should have shouldered the considerable
cost of hiring a full-time bridge monitor to protect against the risk that
debris might get caught and block the free flow of water. One group was
shown only the evidence available at the time of the city’s decision; 24% of
these people felt that Duluth should take on the expense of hiring a flood
monitor. The second group was informed that debris had blocked the river,
causing major flood damage; 56% of these people said the city should
have hired the monitor, although they had been explicitly instructed not to
let hindsight distort their judgment.
The worse the consequence, the greater the hindsight bias. In the case
of a catastrophe, such as 9/11, we are especially ready to believe that the
officials who failed to anticipate it were negligent or blind. On July 10,
2001, the Central Intelligence Agency obtained information that al-Qaeda
might be planning a major attack against the United States. George Tenet,
director of the CIA, brought the information not to President George W.
Bush but to National Security Adviser Condoleezza Rice. When the facts
later emerged, Ben Bradlee, the legendary executive editor of The
Washington Post, declared, “It seems to me elementary that if you’ve got
the story that’s going to dominate history you might as well go right to the
president.” But on July 10, no one knew—or could have known—that this
tidbit of intelligence would turn out to dominate history.
Because adherence to standard operating procedures is difficult to
second-guess, decision makers who expect to have their decisions
scrutinized with hindsight are driven to bureaucratic solutions—and to an
extreme reluctance to take risks. As malpractice litigation became more
common, physicians changed their procedures in multiple ways: ordered
more tests, referred more cases to specialists, applied conventional
treatments even when they were unlikely to help. These actions protected
the physicians more than they benefited the patients, creating the potential
for conflicts of interest. Increased accountability is a mixed blessing.
Although hindsight and the outcome bias generally foster risk aversion,
they also bring undeserved rewards to irresponsible risk seekers, such as
a general or an entrepreneur who took a crazy gamble and won. Leaders
who have been lucky are never punished for having taken too much risk.
Instead, they are believed to have had the flair and foresight to anticipate
success, and the sensible people who doubted them are seen in hindsight
as mediocre, timid, and weak. A few lucky gambles can crown a reckless
leader with a halo of prescience and boldness.
Recipes for Success
The sense-making machinery of System 1 makes us see the world as
more tidy, simple, predictable, and coherent than it really is. The illusion
that one has understood the past feeds the further illusion that one can
predict and control the future. These illusions are comforting. They reduce
the anxiety that we would experience if we allowed ourselves to fully
acknowledge the uncertainties of existence. We all have a need for the
reassuring message that actions have appropriate consequences, and
that success will reward wisdom and courage. Many bdecрusiness books
are tailor-made to satisfy this need.
Do leaders and management practices influence the outcomes of firms
in the market? Of course they do, and the effects have been confirmed by
systematic research that objectively assessed the characteristics of CEOs
and their decisions, and related them to subsequent outcomes of the firm.
In one study, the CEOs were characterized by the strategy of the
companies they had led before their current appointment, as well as by
management rules and procedures adopted after their appointment. CEOs
do influence performance, but the effects are much smaller than a reading
of the business press suggests.
Researchers measure the strength of relationships by a correlation
coefficient, which varies between 0 and 1. The coefficient was defined
earlier (in relation to regression to the mean) by the extent to which two
measures are determined by shared factors. A very generous estimate of
the correlation between the success of the firm and the quality of its CEO
might be as high as .30, indicating 30% overlap. To appreciate the
significance of this number, consider the following question:
Suppose you consider many pairs of firms. The two firms in each
pair are generally similar, but the CEO of one of them is better
than the other. How often will you find that the firm with the
stronger CEO is the more successful of the two?
In a well-ordered and predictable world, the correlation would be perfect
(1), and the stronger CEO would be found to lead the more successful firm
in 100% of the pairs. If the relative success of similar firms was determined
entirely by factors that the CEO does not control (call them luck, if you
wish), you would find the more successful firm led by the weaker CEO 50%
of the time. A correlation of .30 implies that you would find the stronger
CEO leading the stronger firm in about 60% of the pairs—an improvement
of a mere 10 percentage points over random guessing, hardly grist for the
hero worship of CEOs we so often witness.
If you expected this value to be higher—and most of us do—then you
should take that as an indication that you are prone to overestimate the
predictability of the world you live in. Make no mistake: improving the odds
of success from 1:1 to 3:2 is a very significant advantage, both at the
racetrack and in business. From the perspective of most business writers,
however, a CEO who has so little control over performance would not be
particularly impressive even if her firm did well. It is difficult to imagine
people lining up at airport bookstores to buy a book that enthusiastically
describes the practices of business leaders who, on average, do
somewhat better than chance. Consumers have a hunger for a clear
message about the determinants of success and failure in business, and
they need stories that offer a sense of understanding, however illusory.
In his penetrating book The Halo Effect, Philip Rosenzweig, a business
school professor based in Switzerland, shows how the demand for illusory
certainty is met in two popular genres of business writing: histories of the
rise (usually) and fall (occasionally) of particular individuals and
companies, and analyses of differences between successful and less
successful firms. He concludes that stories of success and failure
consistently exaggerate the impact of leadership style and management
practices on firm outcomes, and thus their message is rarely useful.
To appreciate what is going on, imagine that business experts, such as
other CEOs, are asked to comment on the reputation of the chief executive
of a company. They poрare keenly aware of whether the company has
recently been thriving or failing. As we saw earlier in the case of Google,
this knowledge generates a halo. The CEO of a successful company is
likely to be called flexible, methodical, and decisive. Imagine that a year
has passed and things have gone sour. The same executive is now
described as confused, rigid, and authoritarian. Both descriptions sound
right at the time: it seems almost absurd to call a successful leader rigid
and confused, or a struggling leader flexible and methodical.
Indeed, the halo effect is so powerful that you probably find yourself
resisting the idea that the same person and the same behaviors appear
methodical when things are going well and rigid when things are going
poorly. Because of the halo effect, we get the causal relationship
backward: we are prone to believe that the firm fails because its CEO is
rigid, when the truth is that the CEO appears to be rigid because the firm is
failing. This is how illusions of understanding are born.
The halo effect and outcome bias combine to explain the extraordinary
appeal of books that seek to draw operational morals from systematic
examination of successful businesses. One of the best-known examples of
this genre is Jim Collins and Jerry I. Porras’s Built to Last. The book
contains a thorough analysis of eighteen pairs of competing companies, in
which one was more successful than the other. The data for these
comparisons are ratings of various aspects of corporate culture, strategy,
and management practices. “We believe every CEO, manager, and
entrepreneur in the world should read this book,” the authors proclaim.
“You can build a visionary company.”
The basic message of Built to Last