# Thinking-Fast-and-Slow-exported

In memory of Amos Tversky
Contents
Introduction
Part I. Two Systems
1. The Characters of the Story
2. Attention and Effort
3. The Lazy Controller
4. The Associative Machine
5. Cognitive Ease
6. Norms, Surprises, and Causes
7. A Machine for Jumping to Conclusions
8. How Judgments Happen
9. Answering an Easier Question
Part II. Heuristics and Biases
10. The Law of Small Numbers
<5>
11. Anchors
12. The Science of Availability
13. Availability, Emotion, and Risk
14. Tom W’s Specialty
15. Linda: Less is More
16. Causes Trump Statistics
17. Regression to the Mean
18. Taming Intuitive Predictions
Part III. Overconfidence
19. The Illusion of Understanding
20. The Illusion of Validity
21. Intuitions Vs. Formulas
22. Expert Intuition: When Can We Trust It?
23. The Outside View
24. The Engine of Capitalism
Part IV. Choices
25. Bernoulli’s Errors
26. Prospect Theory
27. The Endowment Effect
28. Bad Events
29. The Fourfold Pattern
30. Rare Events
31. Risk Policies
32. Keeping Score
33. Reversals
34. Frames and Reality
Part V. Two Selves
35. Two Selves
36. Life as a Story
37. Experienced Well-Being
38. Thinking About Life
Conclusions
Appendix 
A: 
Judgment 
Under
Uncertainty
Appendix B: Choices, Values, and Frames
Acknowledgments
Notes
Index
Introduction
Every author, I suppose, has in mind a setting in which readers of his or her
work could benefit from having read it. Mine is the proverbial office
watercooler, where opinions are shared and gossip is exchanged. I hope
to enrich the vocabulary that people use when they talk about the
judgments and choices of others, the company’s new policies, or a
colleague’s investment decisions. Why be concerned with gossip?
Because it is much easier, as well as far more enjoyable, to identify and
label the mistakes of others than to recognize our own. Questioning what
we believe and want is difficult at the best of times, and especially difficult
when we most need to do it, but we can benefit from the informed opinions
of others. Many of us spontaneously anticipate how friends and colleagues
will evaluate our choices; the quality and content of these anticipated
judgments therefore matters. The expectation of intelligent gossip is a
powerful motive for serious self-criticism, more powerful than New Year
resolutions to improve one’s decision making at work and at home.
To be a good diagnostician, a physician needs to acquire a large set of
labels for diseases, each of which binds an idea of the illness and its
symptoms, possible antecedents and causes, possible developments and
consequences, and possible interventions to cure or mitigate the illness.
Learning medicine consists in part of learning the language of medicine. A
deeper understanding of judgments and choices also requires a richer
vocabulary than is available in everyday language. The hope for informed
gossip is that there are distinctive patterns in the errors people make.
Systematic errors are known as biases, and they recur predictably in
particular circumstances. When the handsome and confident speaker
bounds onto the stage, for example, you can anticipate that the audience
will judge his comments more favorably than he deserves. The availability
of a diagnostic label for this bias—the halo effect—makes it easier to
anticipate, recognize, and understand.
When you are asked what you are thinking about, you can normally
answer. You believe you know what goes on in your mind, which often
consists of one conscious thought leading in an orderly way to another. But
that is not the only way the mind works, nor indeed is that the typical way.
Most impressions and thoughts arise in your conscious experience without
your knowing how they got there. You cannot tracryd>e how you came to
the belief that there is a lamp on the desk in front of you, or how you
detected a hint of irritation in your spouse’s voice on the telephone, or how
you managed to avoid a threat on the road before you became consciously
aware of it. The mental work that produces impressions, intuitions, and
many decisions goes on in silence in our mind.
Much of the discussion in this book is about biases of intuition. However,
the focus on error does not denigrate human intelligence, any more than
the attention to diseases in medical texts denies good health. Most of us
are healthy most of the time, and most of our judgments and actions are
appropriate most of the time. As we navigate our lives, we normally allow
ourselves to be guided by impressions and feelings, and the confidence
we have in our intuitive beliefs and preferences is usually justified. But not
always. We are often confident even when we are wrong, and an objective
observer is more likely to detect our errors than we are.
So this is my aim for watercooler conversations: improve the ability to
identify and understand errors of judgment and choice, in others and
eventually in ourselves, by providing a richer and more precise language to
discuss them. In at least some cases, an accurate diagnosis may suggest
an intervention to limit the damage that bad judgments and choices often
cause.
Origins
This book presents my current understanding of judgment and decision
making, which has been shaped by psychological discoveries of recent
decades. However, I trace the central ideas to the lucky day in 1969 when I
asked a colleague to speak as a guest to a seminar I was teaching in the
Department of Psychology at the Hebrew University of Jerusalem. Amos
Tversky was considered a rising star in the field of decision research—
indeed, in anything he did—so I knew we would have an interesting time.
Many people who knew Amos thought he was the most intelligent person
they had ever met. He was brilliant, voluble, and charismatic. He was also
blessed with a perfect memory for jokes and an exceptional ability to use
them to make a point. There was never a dull moment when Amos was
around. He was then thirty-two; I was thirty-five.
Amos told the class about an ongoing program of research at the
University of Michigan that sought to answer this question: Are people
good intuitive statisticians? We already knew that people are good
intuitive grammarians: at age four a child effortlessly conforms to the rules
of grammar as she speaks, although she has no idea that such rules exist.
Do people have a similar intuitive feel for the basic principles of statistics?
Amos reported that the answer was a qualified yes. We had a lively debate
in the seminar and ultimately concluded that a qualified no was a better
answer.
Amos and I enjoyed the exchange and concluded that intuitive statistics
was an interesting topic and that it would be fun to explore it together. That
Friday we met for lunch at Café Rimon, the favorite hangout of bohemians
and professors in Jerusalem, and planned a study of the statistical
intuitions of sophisticated researchers. We had concluded in the seminar
that our own intuitions were deficient. In spite of years of teaching and
using statistics, we had not developed an intuitive sense of the reliability of
statistical results observed in small samples. Our subjective judgments
were biased: we were far too willing to believe research findings based on
inadequate evidence and prone to collect too few observations in our own
research. The goal of our study was to examine whether other researchers
suffered from the same affliction.
We prepared a survey that included realistic scenarios of statistical
issues that arise in research. Amos collected the responses of a group of
expert participants in a meeting of the Society of Mathematical
Psychology, including the authors of two statistical textbooks. As expected,
we found that our expert colleagues, like us, greatly exaggerated the
likelihood that the original result of an experiment would be successfully
replicated even with a small sample. They also gave very poor advice to a
fictitious graduate student about the number of observations she needed
to collect. Even statisticians were not good intuitive statisticians.
While writing the article that reported these findings, Amos and I
discovered that we enjoyed working together. Amos was always very
funny, and in his presence I became funny as well, so we spent hours of
solid work in continuous amusement. The pleasure we found in working
together made us exceptionally patient; it is much easier to strive for
perfection when you are never bored. Perhaps most important, we
checked our critical weapons at the door. Both Amos and I were critical
and argumentative, he even more than I, but during the years of our
collaboration neither of us ever rejected out of hand anything the other
said. Indeed, one of the great joys I found in the collaboration was that
Amos frequently saw the point of my vague ideas much more clearly than I
did. Amos was the more logical thinker, with an orientation to theory and
an unfailing sense of direction. I was more intuitive and rooted in the
psychology of perception, from which we borrowed many ideas. We were
sufficiently similar to understand each other easily, and sufficiently different
to surprise each other. We developed a routine in which we spent much of
our working days together, often on long walks. For the next fourteen years
our collaboration was the focus of our lives, and the work we did together
during those years was the best either of us ever did.
We quickly adopted a practice that we maintained for many years. Our
research was a conversation, in which we invented questions and jointly
examined our intuitive answers. Each question was a small experiment,
and we carried out many experiments in a single day. We were not
seriously looking for the correct answer to the statistical questions we
posed. Our aim was to identify and analyze the intuitive answer, the first
one that came to mind, the one we were tempted to make even when we
knew it to be wrong. We believed—correctly, as it happened—that any
intuition that the two of us shared would be shared by many other people
as well, and that it would be easy to demonstrate its effects on judgments.
We once discovered with great delight that we had identical silly ideas
about the future professions of several toddlers we both knew. We could
identify the argumentative three-year-old lawyer, the nerdy professor, the
empathetic and mildly intrusive psychotherapist. Of course these
predictions were absurd, but we still found them appealing. It was also
clear that our intuitions were governed by the resemblance of each child to
the cultural stereotype of a profession. The amusing exercise helped us
develop a theory that was emerging in our minds at the time, about the role
of resemblance in predictions. We went on to test and elaborate that
theory in dozens of experiments, as in the following example.
As you consider the next question, please assume that Steve was
selected at random from a representative sample:
An individual has been described by a neighbor as follows:
“Steve is very shy and withdrawn, invariably helpful but with little
interest in people or in the world of reality. A meek and tidy soul,
he has a need for order and structurut and stre, and a passion for
detail.” Is Steve more likely to be a librarian or a farmer?
The resemblance of Steve’s personality to that of a stereotypical librarian
strikes 
everyone 
immediately, 
but 
equally 
relevant 
statistical
considerations are almost always ignored. Did it occur to you that there
are more than 20 male farmers for each male librarian in the United
States? Because there are so many more farmers, it is almost certain that
more “meek and tidy” souls will be found on tractors than at library
information desks. However, we found that participants in our experiments
ignored the relevant statistical facts and relied exclusively on resemblance.
We proposed that they used resemblance as a simplifying heuristic
(roughly, a rule of thumb) to make a difficult judgment. The reliance on the
heuristic caused predictable biases (systematic errors) in their
predictions.
On another occasion, Amos and I wondered about the rate of divorce
among professors in our university. We noticed that the question triggered
a search of memory for divorced professors we knew or knew about, and
that we judged the size of categories by the ease with which instances
came to mind. We called this reliance on the ease of memory search the
availability heuristic. In one of our studies, we asked participants to answer
a simple question about words in a typical English text:
Consider the letter K.
Is K more likely to appear as the first letter in a word OR as the
third letter?
As any Scrabble player knows, it is much easier to come up with words
that begin with a particular letter than to find words that have the same
letter in the third position. This is true for every letter of the alphabet. We
therefore expected respondents to exaggerate the frequency of letters
appearing in the first position—even those letters (such as K, L, N, R, V)
which in fact occur more frequently in the third position. Here again, the
reliance on a heuristic produces a predictable bias in judgments. For
example, I recently came to doubt my long-held impression that adultery is
more common among politicians than among physicians or lawyers. I had
even come up with explanations for that “fact,” including the aphrodisiac
effect of power and the temptations of life away from home. I eventually
realized that the transgressions of politicians are much more likely to be
reported than the transgressions of lawyers and doctors. My intuitive
impression could be due entirely to journalists’ choices of topics and to my
reliance on the availability heuristic.
Amos and I spent several years studying and documenting biases of
intuitive thinking in various tasks—assigning probabilities to events,
forecasting the future, assessing hypotheses, and estimating frequencies.
In the fifth year of our collaboration, we presented our main findings in
Science magazine, a publication read by scholars in many disciplines. The
article (which is reproduced in full at the end of this book) was titled
“Judgment Under Uncertainty: Heuristics and Biases.” It described the
simplifying shortcuts of intuitive thinking and explained some 20 biases as
manifestations of these heuristics—and also as demonstrations of the role
of heuristics in judgment.
Historians of science have often noted that at any given time scholars in
a particular field tend to share basic re share assumptions about their
subject. Social scientists are no exception; they rely on a view of human
nature that provides the background of most discussions of specific
behaviors but is rarely questioned. Social scientists in the 1970s broadly
accepted two ideas about human nature. First, people are generally
rational, and their thinking is normally sound. Second, emotions such as
fear, affection, and hatred explain most of the occasions on which people
depart from rationality. Our article challenged both assumptions without
discussing them directly. We documented systematic errors in the thinking
of normal people, and we traced these errors to the design of the
machinery of cognition rather than to the corruption of thought by emotion.
Our article attracted much more attention than we had expected, and it
remains one of the most highly cited works in social science (more than
three hundred scholarly articles referred to it in 2010). Scholars in other
disciplines found it useful, and the ideas of heuristics and biases have
been used productively in many fields, including medical diagnosis, legal
judgment, intelligence analysis, philosophy, finance, statistics, and military
strategy.
For example, students of policy have noted that the availability heuristic
helps explain why some issues are highly salient in the public’s mind while
others are neglected. People tend to assess the relative importance of
issues by the ease with which they are retrieved from memory—and this is
largely determined by the extent of coverage in the media. Frequently
mentioned topics populate the mind even as others slip away from
awareness. In turn, what the media choose to report corresponds to their
view of what is currently on the public’s mind. It is no accident that
authoritarian regimes exert substantial pressure on independent media.
Because public interest is most easily aroused by dramatic events and by
celebrities, media feeding frenzies are common. For several weeks after
Michael Jackson’s death, for example, it was virtually impossible to find a
television channel reporting on another topic. In contrast, there is little
coverage of critical but unexciting issues that provide less drama, such as
declining educational standards or overinvestment of medical resources in
the last year of life. (As I write this, I notice that my choice of “little-covered”
examples was guided by availability. The topics I chose as examples are
mentioned often; equally important issues that are less available did not
come to my mind.)
We did not fully realize it at the time, but a key reason for the broad
appeal of “heuristics and biases” outside psychology was an incidental
feature of our work: we almost always included in our articles the full text of
the questions we had asked ourselves and our respondents. These
questions served as demonstrations for the reader, allowing him to
recognize how his own thinking was tripped up by cognitive biases. I hope
you had such an experience as you read the question about Steve the
librarian, which was intended to help you appreciate the power of
resemblance as a cue to probability and to see how easy it is to ignore
relevant statistical facts.
The use of demonstrations provided scholars from diverse disciplines—
notably philosophers and economists—an unusual opportunity to observe
possible flaws in their own thinking. Having seen themselves fail, they
became more likely to question the dogmatic assumption, prevalent at the
time, that the human mind is rational and logical. The choice of method
was crucial: if we had reported results of only conventional experiments,
the article would have been less noteworthy and less memorable.
Furthermore, skeptical readers would have distanced themselves from the
results by attributing judgment errors to the familiar l the famifecklessness
of undergraduates, the typical participants in psychological studies. Of
course, we did not choose demonstrations over standard experiments
because we wanted to influence philosophers and economists. We
preferred demonstrations because they were more fun, and we were lucky
in our choice of method as well as in many other ways. A recurrent theme
of this book is that luck plays a large role in every story of success; it is
almost always easy to identify a small change in the story that would have
turned a remarkable achievement into a mediocre outcome. Our story was
no exception.
The reaction to our work was not uniformly positive. In particular, our
focus on biases was criticized as suggesting an unfairly negative view of
the mind. As expected in normal science, some investigators refined our
ideas and others offered plausible alternatives. By and large, though, the
idea that our minds are susceptible to systematic errors is now generally
accepted. Our research on judgment had far more effect on social science
than we thought possible when we were working on it.
Immediately after completing our review of judgment, we switched our
attention to decision making under uncertainty. Our goal was to develop a
psychological theory of how people make decisions about simple
gambles. For example: Would you accept a bet on the toss of a coin where
you win $130 if the coin shows heads and lose $100 if it shows tails?
These elementary choices had long been used to examine broad
questions about decision making, such as the relative weight that people
assign to sure things and to uncertain outcomes. Our method did not
change: we spent many days making up choice