id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
939720537,An Opinionated Evals Reading List — Apollo Research,,A long reading list of evals papers with recommendations and comments by the evals team.,https://www.apolloresearch.ai/blog/an-opinionated-evals-reading-list?utm_source=chatgpt.com,my_library,"arxiv, llms",2025-01-07T11:52:23.395Z,http://static1.squarespace.com/static/6593e7097565990e65c886fd/65940f02f1fcb826ed2a7229/670e662d065b1f4ab82aab5e/1734295113640/Screenshot+2024-10-15+at+15.25.10.png?format=1500w,,False
944921267,The 2025 AI Engineering Reading List,gist: https://gist.github.com/bjpcjp/117ecf144c062c3c933f2690e7ece5c2,"We picked 50 paper/models/blogs across 10 fields in AI Eng: LLMs, Benchmarks, Prompting, RAG, Agents, CodeGen, Vision, Voice, Diffusion, Finetuning. If you're starting from scratch, start here.",https://www.latent.space/p/2025-papers?utm_campaign=post&utm_medium=web,my_library,"llms, nlp, deep-learning, arxiv",2025-01-14T01:25:14.377Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242370c-229f-453d-924a-7a5aa4d20a4c_1090x502.png",,True
944039579,100 Must-Read Generative AI Papers from 2024,,A comprehensive list of some of the most impactful generative papers from last year,https://open.substack.com/pub/thenuancedperspective/p/100-must-read-generative-ai-papers?r=oc5d&utm_medium=ios,my_library,"arxiv, llms",2025-01-12T20:45:13.528Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcfe5315-38ae-475d-9b8a-05df56910378_1800x829.png",,False
924587133,Blt patches scale better than tokens,,,https://dl.fbaipublicfiles.com/blt/BLT__Patches_Scale_Better_Than_Tokens.pdf,my_library,"arxiv, llms",2024-12-18T22:13:27.027Z,,,False
908864605,eugeneyan/llm-paper-notes: Notes from the Latent Space paper club. Follow along or start your own!,,Notes from the Latent Space paper club. Follow along or start your own! - eugeneyan/llm-paper-notes,https://github.com/eugeneyan/llm-paper-notes,my_library,"llms, arxiv",2024-11-26T01:17:05.667Z,https://opengraph.githubassets.com/9c85f10ad2727a21d7a3aa7d87a46809444643e5d716c492825f63c34d60b7bd/eugeneyan/llm-paper-notes,,True
898393417,"Analyzing the homerun year for LLMs: the top-100 most cited AI papers in 2023, with all medals for open models.",,"9 October 2024, Mathias Parisot, Jakub Zavrel.Even in the red hot global race for AI dominance, you publish and you perish, unless your peers pick up your work, build further on it, and you manage to drive real progress in the field. And of course, we are all very curious who is currently having that kind of impact. Are the billions of dollars spent on AI R&D paying off in the long run? So here is, in continuation of our popular publication impact analysis of last year, Zeta Alpha's ranking of t",https://www.zeta-alpha.com/post/analyzing-the-homerun-year-for-llms-the-top-100-most-cited-ai-papers-in-2023-with-all-medals-for-o,my_library,"llms, arxiv",2024-11-11T22:54:09.878Z,,,False
876797178,[2406.01506] The Geometry of Categorical and Hierarchical Concepts in Large,,The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has...,https://arxiv.org/abs/2406.01506,my_library,"arxiv, llms, taxonomy",2024-06-11T17:02:00.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876796893,Title:You Only Cache Once: Decoder-Decoder Architectures for Language Model,,"We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a...",https://arxiv.org/abs/2405.05254,my_library,"arxiv, llms",2024-05-11T21:23:10.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876796864,[2404.19737] Better & Faster Large Language Models via Multi-token Predicti,,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results...",https://arxiv.org/abs/2404.19737,my_library,"arxiv, llms",2024-05-08T06:24:55.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876795630,Tips for LLM Pretraining and Evaluating Reward Models,,Discussing AI Research Papers in March 2024,https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms?isFreemail=true&post_id=142924793&publication_id=1174659&r=oc5d&triedRedirect=true,my_library,"arxiv, llms, machine-learning",2024-04-15T23:05:34.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6fabd9ec-29f1-4c1c-85fb-8781e7c6ce0b_1600x436.png",,False
876793745,10 Noteworthy AI Research Papers of 2023,,"This year has felt distinctly different. I've been working in, on, and with machine learning and AI for over a decade, yet I can't recall a time when these fields were as popular and rapidly evolving as they have been this year. To conclude an eventful 2023 in machine learning and AI research, I'm excited to share 10 noteworthy papers I've read this year. My personal focus has been more on large language models, so you'll find a heavier emphasis on large language model (LLM) papers than computer vision papers this year.",https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023,my_library,"arxiv, llms, machine-learning",2024-01-07T21:34:39.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0046298-1059-4538-bcd8-dfcfc863d7c5_1254x810.png",,False
876793643,[2302.07730] Transformer models: an introduction and catalog,,"In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory,...",https://arxiv.org/abs/2302.07730,my_library,"arxiv, llms, transformers",2023-10-05T22:46:06.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876791884,Distilling Step-by-Step! Outperforming Larger Language Models with...,,"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific...",https://arxiv.org/abs/2305.02301,my_library,"arxiv, llms",2023-05-05T02:25:21.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876791883,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,,"We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of...",https://arxiv.org/abs/2301.00774,my_library,"arxiv, llms",2023-05-05T02:25:09.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876791809,Eight Things to Know about Large Language Models,,"The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields....",https://arxiv.org/abs/2304.00612,my_library,"arxiv, llms",2023-04-21T23:43:55.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876791748,A Survey of Large Language Models,,"Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and...",https://arxiv.org/abs/2303.18223,my_library,"arxiv, deep-learning, llms",2023-04-14T02:26:32.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
927093704,LLM Research Papers: The 2024 List,,"A curated list of interesting LLM-related research papers from 2024, shared for those looking for something to read over the holidays.",https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list?utm_campaign=post&utm_medium=web,to_read,"llms, arxiv",2024-12-22T15:14:46.481Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ba1c2ab-7ae7-4f22-86df-f116f2914cd5_1272x1232.png",,False
