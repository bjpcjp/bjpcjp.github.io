id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876797939,FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-preci,,"Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most libraries to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.",https://pytorch.org/blog/flashattention-3,my_library,"attention, gpus, pytorch",2024-07-14T05:59:19.000Z,https://pytorch.org/assets/images/social-share.jpg,,False
876793649,How AMD May Get Across the CUDA Moat,,"When discussing GenAI, the term ""GPU"" almost always enters the conversation and the topic often moves toward performance and access. Interestingly, the word ""GPU"" is assumed to mean ""Nvidia"" products. (As an aside, the popular Nvidia hardware used in GenAI are not technically...",https://www.hpcwire.com/2023/10/05/how-amd-may-get-across-the-cuda-moat,my_library,"cuda, gpus, pytorch, tensorflow",2023-10-07T10:28:32.000Z,https://www.hpcwire.com/wp-content/uploads/2023/10/AMD-MI300A.png,,False
876788769,How to Accelerate your PyTorch GPU Training with XLA,,The Power of PyTorch/XLA and how Amazon SageMaker Training Compiler Simplifies its use,https://towardsdatascience.com/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9,my_library,"gpus, pytorch",2022-10-20T12:55:33.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*U3cCzkY7bhrJxvv-,,False
876778230,Using RAPIDS with PyTorch,,"In this post we take a look at how to use cuDF, the RAPIDS dataframe library, to do some of the preprocessing steps required to get the mortgage data in a format that PyTorch can process so that weâ€¦",https://developer.nvidia.com/blog/using-rapids-with-pytorch,my_library,"gpus, nvidia, pytorch",2021-03-15T20:28:23.000Z,https://developer-blogs.nvidia.com/wp-content/uploads/2021/01/RAPIDSPyTorch_Image1-1.jpg,,False
