id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
881001334,FlashSigmoid: A Hardware-Aware and Memory-Efficient Implementation of Sigmoid Attention Yielding a 1,,"Large Language Models (LLMs) have gained significant prominence in modern machine learning, largely due to the attention mechanism. This mechanism employs a sequence-to-sequence mapping to construct context-aware token representations. Traditionally, attention relies on the softmax function (SoftmaxAttn) to generate token representations as data-dependent convex combinations of values. However, despite its widespread adoption and effectiveness, SoftmaxAttn faces several challenges. One key issue is the tendency of the softmax function to concentrate attention on a limited number of features, potentially overlooking other informative aspects of the input data. Also, the application of SoftmaxAttn necessitates a row-wise reduction along the input sequence length,",https://www.marktechpost.com/2024/09/13/flashsigmoid-a-hardware-aware-and-memory-efficient-implementation-of-sigmoid-attention-yielding-a-17-inference-kernel-speed-up-over-flashattention-2-on-h100-gpus,my_library,"llms, attention",2014-09-24T00:00:00.000Z,https://www.marktechpost.com/wp-content/uploads/2024/09/Screenshot-2024-09-13-at-5.26.21-PM.png,,False
876797961,Understanding Positional Embeddings in Transformers: From Absolute to Rotar,,"A deep dive into absolute, relative, and rotary positional embeddings with code examples",https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26,my_library,"attention, llms, transformers",2024-07-20T23:01:10.000Z,https://miro.medium.com/v2/resize:fit:1200/1*EWz8ImltNHpDjMB8bOq_tQ.png,,False
876795641,"Deep Learning Architectures From CNN, RNN, GAN, and Transformers To Encoder",,"Deep learning architectures have revolutionized the field of artificial intelligence, offering innovative solutions for complex problems across various domains, including computer vision, natural language processing, speech recognition, and generative models. This article explores some of the most influential deep learning architectures: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), Transformers, and Encoder-Decoder architectures, highlighting their unique features, applications, and how they compare against each other. Convolutional Neural Networks (CNNs) CNNs are specialized deep neural networks for processing data with a grid-like topology, such as images. A CNN automatically detects the important features without any human supervision.",https://www.marktechpost.com/2024/04/12/deep-learning-architectures-from-cnn-rnn-gan-and-transformers-to-encoder-decoder-architectures,my_library,"attention, convolutions, deep-learning, gans, llms, rnns",2024-04-15T23:39:35.000Z,https://www.marktechpost.com/wp-content/uploads/2024/04/Hn6UWRfcQGS1sXgZTUWw6A.png,,False
876795475,How Chain-of-Thought Reasoning Helps Neural Networks Compute,,Large language models do better at solving problems when they show their work. Researchers are beginning to understand why.,https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321,my_library,"attention, llms",2024-03-29T00:25:59.000Z,https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/03/ChainOfThought-byNickSlater-Social.webp,,False
876794096,How do transformers work?+Design a Multi-class Sentiment Analysis for Custo,,We will deep dive into understanding how transformer model work like BERT(Non-mathematical Explanation of course!). system design to use the transformer to build a Sentiment Analysis,https://open.substack.com/pub/nintyzeros/p/how-do-transformer-workdesign-a-multi,my_library,"attention, llms",2024-02-22T06:19:27.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png",,False
926989129,Why AI language models choke on too much text,,Compute costs scale with the square of the input size. That’s not great.,https://arstechnica.com/ai/2024/12/why-ai-language-models-choke-on-too-much-text/,to_read,"llms, attention",2024-12-22T11:05:52.244Z,https://cdn.arstechnica.net/wp-content/uploads/2024/12/LLM-soup-1152x648.jpg,"Highlight:This is probably why Google charges twice as much, per token, for Gemini 1.5 Pro once the context gets longer than 128,000 tokens. Generating token number 128,001 requires comparisons with all 128,000 previous tokens, making it significantly more expensive than producing the first or 10th or 100th token.

Highlight:In a series of papers, Princeton computer scientist Tri Dao and several collaborators have developed FlashAttention, which calculates attention in a way that minimizes the number of these slow memory operations.

Highlight:OpenAI’s GPT-4o can handle 128,000 tokens (about 200 pages of text).
Anthropic’s Claude 3.5 Sonnet can accept 200,000 tokens (about 300 pages of text).
Google’s Gemini 1.5 Pro allows 2 million tokens (about 2,000 pages of text).

Highlight:This meant that if you fed it more than about 15 pages of text, it would “forget” information from the beginning of its context.

Highlight:the most popular way to build an LLM-based system to handle large amounts of information is called retrieval-augmented generation (RAG)

Highlight:These systems try to find documents relevant to a user’s query and then insert the most relevant documents into an LLM’s context window.

Highlight:RAG doesn’t enable an LLM to reason in more sophisticated ways over large numbers of documents:

Highlight:attention

Highlight:In the early 2010s, recurrent neural networks (RNNs) were a popular architecture for understanding natural language.

Highlight:transformer

Highlight:It is inherently linear: it has to complete its analysis of the first word, “How,” before passing the hidden state back to the bottom layer

Highlight:The transformer-based model shown here does roughly as many computations as the RNN in the previous diagram. So it might not run any faster on a (single-core) CPU.

Highlight:Classic RNN-based models could not have grown that large because their linear architecture prevented them from being trained efficiently on a GPU.

Highlight:But they don’t do exactly the same amount of work

Highlight:This means that the total computing power required for attention grows quadratically with the total number of tokens

Highlight:Before a GPU can start doing math, it must move data from slow shared memory (called high-bandwidth memory) to much faster memory inside a particular execution unit (called SRAM).

Highlight:FlashAttention

Highlight:One widely cited paper describes ring attention, which divides input tokens into blocks and assigns each block to a different GPU. It’s called ring attention because GPUs are organized into a conceptual ring, with each GPU passing data to its neighbor.

Highlight:In April, Google announced a new model called Infini-attention.

Highlight:it stores older tokens in a “compressive memory”

Highlight:Mamba

Highlight:they got the best performance from a hybrid architecture that interleaved 24 Mamba layers with four attention layers",True
