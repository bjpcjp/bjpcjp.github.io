id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876795480,Mamba Explained,,"Is Attention all you need? Mamba, a novel AI model based on State Space Models (SSMs), emerges as a formidable alternative to the widely used Transformer models, addressing their inefficiency in processing long sequences.",https://thegradient.pub/mamba-explained,my_library,"deep-learning, llms, transformers",2024-03-30T02:53:57.000Z,https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&ixlib=rb-4.0.3&q=80&w=2000,,False
876791626,Hacker News,,I explain what is so unique about the RWKV language model.,https://johanwind.github.io/2023/03/23/rwkv_overview.html,my_library,"deep-learning, nlp, rnns, transformers",2023-03-31T15:36:22.000Z,,,False
876790650,Hacker News,,"Many new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post — restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length. Notations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size.",https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2,my_library,"deep-learning, transformers",2023-02-07T22:09:44.000Z,,,False
876789737,"lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch",,"Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch - lucidrains/vit-pytorch",https://github.com/lucidrains/vit-pytorch,my_library,"deep-learning, machine-vision, pytorch, transformers",2022-12-18T22:40:22.000Z,https://opengraph.githubassets.com/9edc731f2a99d5a99777494dd7aaa43716ebad2f0f59b90b9e38e48aaecebb2c/lucidrains/vit-pytorch,,False
876788593,All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Unders,,"Attention, Self-Attention, Multi-head Attention, Masked Multi-head Attention, Transformers, BERT, and GPT",https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada,my_library,"deep-learning, transformers",2022-09-20T22:31:27.000Z,https://miro.medium.com/v2/resize:fit:1200/0*Wvg_pNDViACfg-IK.png,,False
876788592,All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Unders,,"Attention, Self-Attention, Multi-head Attention, and Transformers",https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021,my_library,"deep-learning, transformers",2022-09-20T22:31:11.000Z,https://miro.medium.com/v2/resize:fit:925/1*Rv_pntt-N2WL7LMbIptHxQ.png,,False
876780118,Transformers,,,https://e2eml.school/transformers.html,my_library,"deep-learning, transformers",2021-11-29T16:07:52.000Z,,,False
876779329,GPT-J-6B: 6B JAX-Based Transformer – Aran Komatsuzaki,,"Summary: We have released GPT-J-6B, 6B JAX-based (Mesh) Transformer LM (Github).GPT-J-6B performs nearly on par with 6.7B GPT-3 (or Curie) on various zero-shot down-streaming tasks.You can try out …",https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j,my_library,"deep-learning, transformers",2021-07-05T12:05:38.000Z,https://arankomatsuzaki.wordpress.com/wp-content/uploads/2021/05/jax_logo.png?w=1200,,False
876779215,2106,,,https://arxiv.org/pdf/2106.04554.pdf,my_library,"deep-learning, transformers",2021-06-15T00:14:36.000Z,,,False
876779143,NielsRogge/Transformers-Tutorials: This repository contains demos I made with the Transformers library by HuggingFace.,,This repository contains demos I made with the Transformers library by HuggingFace. - NielsRogge/Transformers-Tutorials,https://email.mg2.substack.com/c/eJwlkE1vwyAMhn9NuS3iIwFy4LDLjjtMvUd8uJSVQARkVf79SCtZtmzLev0-VjfwuRxqy7WhMy3t2EAleNYIrUFBe4WyBKcIYdMoZ-TU6IicJAp1uRWAVYeoWtkBbbuJweoWcjoPqJwZFeiuRiLsbI1xYJnUTFvDRi2oBYuJJOP81tW7C5AsKPiDcuQEKKp7a1u9sM8L_erhQ7vvZrB57c13gFh_svfQm2vRqd5yWaHUj-vecgk6VhQUxZRgjll_fqR8IAOfhaZMymniBBNiwFCwhhOjRw5YssuIV0-HupvatH2cYqgo87vZ363v_On2Nexml17XPYV2LJC0ieDeHNqb5ovM4iFB6ZTdopsinFJBMRZYdjIv2x0UE1hwwQjqqi73q6QcwBZBlxSSfwI84vEPRvWPwA,my_library,"deep-learning, transformers",2021-06-03T22:24:23.000Z,https://opengraph.githubassets.com/4f3c8f4c810cac2343d9c600ee9266cf541122b3afad7d56b25d31a26739f230/NielsRogge/Transformers-Tutorials,,False
876779090,The Illustrated Transformer – Jay Alammar – Visualizing machine learning on,,"Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)   Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese  Watch: MIT’s Deep Learning State of the Art lecture referencing this post  Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others  In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.  The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.  2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:     A High-Level Look Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.",https://jalammar.github.io/illustrated-transformer,my_library,"deep-learning, transformers",2021-05-29T00:40:47.000Z,,,False
876779051,"Understanding Transformers, the machine learning model behind GPT-3",,"How this novel neural network architecture changes the way we analyze complex data types, and powers revolutionary models like GPT-3 and BERT.",https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication,my_library,"chatbots, deep-learning, nlp, transformers",2021-05-22T14:26:39.000Z,https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2021%2F05%2FAI-Transformers-abstract-hed.jpg&signature=dab3715e95a415da68eaecb9b4aebcc7,,False
876779027,How Transformers work in deep learning and NLP: an intuitive introduction | AI Summer,,"An intuitive understanding on Transformers and how they are used in Machine Translation. After analyzing all subcomponents one by one such as self-attention and positional encodings , we explain the principles behind the Encoder and Decoder and why Transformers work so well",https://theaisummer.com/transformer,my_library,"deep-learning, nlp, transformers",2021-05-18T16:56:07.000Z,https://theaisummer.com/static/6122618d7e1466853e88473ba375cdc7/ee604/transformer.png,,False
