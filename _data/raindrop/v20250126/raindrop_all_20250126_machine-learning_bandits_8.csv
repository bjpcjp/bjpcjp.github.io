id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876793877,An Overview of Contextual Bandits,,A dynamic approach to treatment personalization,https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=rss----7f60cf5620c9---4,my_library,"bandits, machine-learning",2024-02-03T03:02:05.000Z,https://miro.medium.com/v2/resize:fit:1200/1*YOsWxbHyU-J7C0s9KOyUDw.png,,False
876792937,Dynamic Pricing with Multi-Armed Bandit: Learning by Doing!,,"Applying Reinforcement Learning strategies to real-world use cases, especially in dynamic pricing, can reveal many surprises",https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac,my_library,"bandits, machine-learning, pricing, prodmgmt",2023-08-19T13:36:19.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*PtB_85QrbCPNJXXb,,False
876789864,Introduction to Multi-Armed Bandit Problems,,"Delve deeper into the concept of multi-armed bandits, reinforcement learning, and exploration vs. exploitation dilemma.",https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html,my_library,"algorithms-math, bandits, machine-learning",2023-01-07T20:33:29.000Z,https://www.kdnuggets.com/wp-content/uploads/popovic_introduction_multiarmed_bandit_problems_1.png,,False
876783756,Multi-Armed Bandit Algorithms: Thompson Sampling,,"Intuition, Bayes, and an example",https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db?source=rss----7f60cf5620c9---4,my_library,"bandits, machine-learning",2022-03-27T19:06:08.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*9YMbE-00b8VRceig,,False
876778267,"stitchfix/mab: Library for multi-armed bandit selection strategies, including efficient deterministic implementations of Thompson sampling and epsilon-greedy.",,"Library for multi-armed bandit selection strategies, including efficient deterministic implementations of Thompson sampling and epsilon-greedy. - stitchfix/mab",https://github.com/stitchfix/mab,my_library,"bandits, golang, machine-learning",2021-03-20T10:53:46.000Z,https://repository-images.githubusercontent.com/340162521/d1711100-774c-11eb-86cb-e4a5c793ebc8,,False
876778190,Thompson Sampling using Conjugate Priors,,Multi-Armed Bandits: Part 5b,https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d,my_library,"bandits, machine-learning",2021-03-10T10:29:56.000Z,https://miro.medium.com/v2/da:true/resize:fit:712/1*_Gqr8su3G4inxRnEuTbC4Q.gif,,False
876776282,A Comparison of Bandit Algorithms,,Multi-Armed Bandits: Part 6,https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb?source=rss----7f60cf5620c9---4,my_library,"algorithms-math, bandits, machine-learning",2020-11-10T17:51:02.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*e7T9wQtgE2cro-mC,,False
876773934,Exploring the fundamentals of multi-armed bandits,,"Multi-armed bandits are a simple but very powerful framework for algorithms that make decisions over time under uncertainty. “Introduction to Multi-Armed Bandits” by Alex Slivkins provides an accessible, textbook-like treatment of the subject.",https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits,my_library,"bandits, machine-learning",2020-03-09T20:16:47.000Z,https://www.microsoft.com/en-us/research/uploads/prod/2020/02/MSFT_Research_MultiarmedBandit_final_1400X788_final.png,,False
