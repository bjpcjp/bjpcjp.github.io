id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
944921267,The 2025 AI Engineering Reading List,gist: https://gist.github.com/bjpcjp/117ecf144c062c3c933f2690e7ece5c2,"We picked 50 paper/models/blogs across 10 fields in AI Eng: LLMs, Benchmarks, Prompting, RAG, Agents, CodeGen, Vision, Voice, Diffusion, Finetuning. If you're starting from scratch, start here.",https://www.latent.space/p/2025-papers?utm_campaign=post&utm_medium=web,my_library,"llms, nlp, deep-learning, arxiv",2025-01-14T01:25:14.377Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9242370c-229f-453d-924a-7a5aa4d20a4c_1090x502.png",,True
876795641,"Deep Learning Architectures From CNN, RNN, GAN, and Transformers To Encoder",,"Deep learning architectures have revolutionized the field of artificial intelligence, offering innovative solutions for complex problems across various domains, including computer vision, natural language processing, speech recognition, and generative models. This article explores some of the most influential deep learning architectures: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), Transformers, and Encoder-Decoder architectures, highlighting their unique features, applications, and how they compare against each other. Convolutional Neural Networks (CNNs) CNNs are specialized deep neural networks for processing data with a grid-like topology, such as images. A CNN automatically detects the important features without any human supervision.",https://www.marktechpost.com/2024/04/12/deep-learning-architectures-from-cnn-rnn-gan-and-transformers-to-encoder-decoder-architectures,my_library,"attention, convolutions, deep-learning, gans, llms, rnns",2024-04-15T23:39:35.000Z,https://www.marktechpost.com/wp-content/uploads/2024/04/Hn6UWRfcQGS1sXgZTUWw6A.png,,False
876795480,Mamba Explained,,"Is Attention all you need? Mamba, a novel AI model based on State Space Models (SSMs), emerges as a formidable alternative to the widely used Transformer models, addressing their inefficiency in processing long sequences.",https://thegradient.pub/mamba-explained,my_library,"deep-learning, llms, transformers",2024-03-30T02:53:57.000Z,https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&ixlib=rb-4.0.3&q=80&w=2000,,False
876793786,"Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attent",,This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama.,https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention?isFreemail=true&post_id=140464659&publication_id=1174659&r=oc5d,my_library,"deep-learning, llms",2024-01-16T08:43:24.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69bfee26-ea3b-42a6-8a1a-6b8187852082_738x564.png",,False
876792782,ELI5: FlashAttention,,Step by step explanation of how one of the most important MLSys breakthroughs work‚Ää‚Äî‚Ääin gory detail.,https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad,my_library,"deep-learning, llms",2023-07-24T00:37:16.000Z,https://miro.medium.com/v2/resize:fit:1200/1*i4tDdwgvGtXuTIyJpFUn8A.png,,False
876792005,The Case for Running AI on CPUs Isn‚Äôt Dead Yet,,"GPUs may dominate, but CPUs could be perfect for smaller AI models",https://spectrum.ieee.org/ai-cpu,my_library,"cpus, deep-learning, gpus, llms, semiconductors",2023-06-02T00:01:45.000Z,https://spectrum.ieee.org/media-library/an-intel-xeon-processor-on-a-black-backdrop-the-processor-is-shown-from-both-above-and-below-displaying-the-thousands-of-conta.jpg?id=33743986&width=1200&height=600&coordinates=0%2C698%2C0%2C698,,False
876791748,A Survey of Large Language Models,,"Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and...",https://arxiv.org/abs/2303.18223,my_library,"arxiv, deep-learning, llms",2023-04-14T02:26:32.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876791747,New Ebook: A Beginner‚Äôs Guide to Large Language Models,,"Explore what LLMs are, how they work, and gain insights into real-world examples, use cases, and best practices.",https://www.nvidia.com/en-us/lp/ai-data-science/large-language-models-ebook,my_library,"deep-learning, llms",2023-04-14T02:26:29.000Z,https://www.nvidia.com/content/dam/en-zz/Solutions/lp/large-language-models-ebook/nvidia-llm-ebook-og.jpg,,False
876791724,üìù Guest Post: Caching LLM Queries for Improved Performance and Cost Savings*,,"If you're looking for a way to improve the performance of your large language model (LLM) application while reducing costs, consider utilizing a semantic cache to store LLM responses.",https://thesequence.substack.com/p/guest-post-caching-llm-queries-for,my_library,"caching, deep-learning, llms",2023-04-12T02:16:11.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1763eb47-236b-4dfd-851c-2a388c7a5671_3200x1454.png",,False
