id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
920322852,Transformers Key-Value (KV) Caching Explained,,Speed up your LLM inference,https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d?source=rss----7f60cf5620c9---4,my_library,"llms, transformers",2024-12-12T19:27:10.808Z,https://miro.medium.com/v2/resize:fit:1200/1*ub2DQhz0aHT0-Tyaw3hGkQ.png,,False
876797961,Understanding Positional Embeddings in Transformers: From Absolute to Rotar,,"A deep dive into absolute, relative, and rotary positional embeddings with code examples",https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26,my_library,"attention, llms, transformers",2024-07-20T23:01:10.000Z,https://miro.medium.com/v2/resize:fit:1200/1*EWz8ImltNHpDjMB8bOq_tQ.png,,False
876795480,Mamba Explained,,"Is Attention all you need? Mamba, a novel AI model based on State Space Models (SSMs), emerges as a formidable alternative to the widely used Transformer models, addressing their inefficiency in processing long sequences.",https://thegradient.pub/mamba-explained,my_library,"deep-learning, llms, transformers",2024-03-30T02:53:57.000Z,https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&ixlib=rb-4.0.3&q=80&w=2000,,False
876793643,[2302.07730] Transformer models: an introduction and catalog,,"In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory,...",https://arxiv.org/abs/2302.07730,my_library,"arxiv, llms, transformers",2023-10-05T22:46:06.000Z,https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,False
876793067,Cracking Open the Hugging Face Transformers Library,,A quick-start guide to using open-source LLMs,https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161,my_library,"generative, llms, transformers",2023-09-25T23:22:21.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*Rkoquyw55K6qbFWF,,False
876792736,Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorc,,This article provides a series of techniques that can lower memory consumption in PyTorch (when training vision transformers and LLMs) by approximately 20x without sacrificing modeling performance and prediction accuracy.,https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm,my_library,"llms, pytorch, transformers",2023-07-23T23:59:46.000Z,https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/pytorch-memory-hero.png,,False
876791935,Edge 291: Reinforcement Learning with Human Feedback,,"1) Reinforcement Learning with Human Feedback(RLHF) 2) The RLHF paper, 3) The transformer reinforcement learning framework.",https://thesequence.substack.com/p/edge-291-reinforcement-learning-with,my_library,"chatgpt, llms, reinforcement-learning, transformers",2023-05-18T10:15:49.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1aaa4cc-10d6-4ada-bba0-f1f2f0793427_1024x1024.png",,False
876791811,Meta has built a massive new language AI—and it’s giving it away for free,,Facebook’s parent company is inviting researchers to pore over and pick apart the flaws in its version of GPT-3,https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency,my_library,"llms, transformers",2023-04-21T23:44:11.000Z,"https://wp.technologyreview.com/wp-content/uploads/2022/05/tiles2-1.jpeg?resize=1200,600",,False
876791789,Hacker News,,A Cross-Section of the Most Relevant Literature To Get Up to Speed,https://magazine.sebastianraschka.com/p/understanding-large-language-models,my_library,"llms, transformers",2023-04-19T00:46:03.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png",,False
921928109,How LLMs Store and Use Knowledge? This AI Paper Introduces Knowledge Circuits: A Framework for Understanding and Improving Knowledge Storage in Transformer-Based LLMs,,"Large language models (LLMs) can understand and generate human-like text by encoding vast knowledge repositories within their parameters. This capacity enables them to perform complex reasoning tasks, adapt to various applications, and interact effectively with humans. However, despite their remarkable achievements, researchers continue to investigate the mechanisms underlying the storage and utilization of knowledge in these systems, aiming to enhance their efficiency and reliability further. A key challenge in using large language models is their propensity to generate inaccurate, biased, or hallucinatory outputs. These problems arise from a limited understanding of how such models organize and access knowledge. Without clear",https://www.marktechpost.com/2024/12/14/how-llms-store-and-use-knowledge-this-ai-paper-introduces-knowledge-circuits-a-framework-for-understanding-and-improving-knowledge-storage-in-transformer-based-llms/,to_read,"llms, transformers",2024-12-15T11:42:06.068Z,https://www.marktechpost.com/wp-content/uploads/2024/12/Screenshot-2024-12-14-at-8.35.53%E2%80%AFPM.png,,False
