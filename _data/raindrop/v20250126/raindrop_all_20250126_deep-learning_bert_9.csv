id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876776541,A Beginner’s Guide to Use BERT for the First Time,,From predicting single sentence to fine-tuning using custom dataset to finding the best hyperparameter configuration.,https://towardsdatascience.com/a-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423?source=rss----7f60cf5620c9---4,my_library,"bert, deep-learning, nlp",2020-12-18T11:21:03.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*w3-dy3whMFXPz2XI,,False
876776470,A version of the BERT language model that’s 20 times as fast,,Determining the optimal architectural parameters reduces network size by 84% while improving performance on natural-language-understanding tasks.,https://www.amazon.science/blog/a-version-of-the-bert-language-model-thats-20-times-as-fast,my_library,"bert, deep-learning, nlp",2020-12-10T22:30:14.000Z,https://assets.amazon.science/dims4/default/7d37b6f/2147483647/strip/true/crop/951x499+0+17/resize/1200x630!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F91%2Fc9%2F4aab043e4973805be90d1737bba1%2Fagora.png,,False
876776126,"AI devs created a lean, mean, GPT-3-beating machine that uses 99.9% fewer p",,AI researchers from the Ludwig Maximilian University (LMU) of Munich have developed a bite-sized text generator capable of besting OpenAI’s state of the art GPT-3 using only a tiny fraction of its parameters. GPT-3 is a monster of an AI sys,https://thenextweb.com/neural/2020/09/21/ai-devs-created-a-lean-mean-gpt-3-beating-machine-that-uses-99-9-fewer-parameters,my_library,"bert, chatbots, deep-learning, nlp",2020-11-03T00:14:03.000Z,https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2017%2F07%2Frobots.jpg&signature=b11955d66bd7e3e547541908e602d0b2,,False
876776108,AI Democratization in the Era of GPT-3,,"What does Microsoft getting an ""exclusive license"" to GPT-3 mean for the future of AI democratization?",https://thegradient.pub/ai-democratization-in-the-era-of-gpt-3,my_library,"bert, deep-learning, nlp",2020-11-03T00:09:18.000Z,https://thegradient.pub/content/images/2020/09/main.jpg,,False
876774642,NLP — BERT & Transformer - Jonathan Hui - Medium,,Google published an article “Understanding searches better than ever before” and positioned BERT as one of the most important updates to…,https://medium.com/@jonathan_hui/nlp-bert-transformer-7f0ac397f524,my_library,"bert, deep-learning",2020-04-01T11:18:52.000Z,https://miro.medium.com/v2/resize:fit:792/1*AaRWGD95loQWAHq_ulm2LA.jpeg,,False
876774631,google-research/bert: TensorFlow code and pre-trained models for BERT,,TensorFlow code and pre-trained models for BERT.,https://github.com/google-research/bert,my_library,"bert, deep-learning, nlp",2020-04-01T11:14:44.000Z,https://opengraph.githubassets.com/23e10f4b72d311b346e00c30e935be92f78d0085246b7dadd1e4fe41d1457c37/google-research/bert,,False
876774630,"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – J",,"Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)   Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian, Spanish  2021 Update: I created this brief and highly accessible video intro to BERT      The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).",http://jalammar.github.io/illustrated-bert,my_library,"bert, deep-learning, nlp",2020-04-01T11:14:25.000Z,,,False
876774629,Jay Alammar – Visualizing machine learning one concept at a time,,Visualizing machine learning one concept at a time.,http://jalammar.github.io,my_library,"bert, deep-learning",2020-04-01T11:14:07.000Z,,,False
876773956,Vincent Boucher on LinkedIn: #transformer #bert #nlp,,Pre-training SmallBERTa - A tiny model to train on a tiny dataset  An end to end colab notebook that allows you to train your own LM (using HuggingFace…,https://www.linkedin.com/posts/activity-6639302449037406208-LJJ1,my_library,"bert, deep-learning, nlp",2020-03-09T20:16:47.000Z,https://media.licdn.com/dms/image/v2/C4E22AQERV7utC98QDw/feedshare-shrink_800/feedshare-shrink_800/0/1582933054953?e=2147483647&v=beta&t=2rnxyQHfr19Qeo7X9VvVj-AiFlBIbxTBoDMZdyYOgzk,,False
