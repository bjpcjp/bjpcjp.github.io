id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876778356,"The Dying ReLU Problem, Clearly Explained",,Keep your neural network alive by understanding the downsides of ReLU,https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24?source=rss----7f60cf5620c9---4,my_library,"activations, deep-learning",2021-03-30T23:19:09.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*w80ldgn1iri7dhsM,,False
876777527,Math | Obviously Awesome,,Activation functions are functions which take an input signal and convert it to an output signal. Activation functions introduce…,https://medium.com/@matelabs_ai/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046,my_library,"activations, deep-learning",2021-02-04T11:32:01.000Z,https://miro.medium.com/v2/resize:fit:1200/1*zwp7ZQLR4cXgLjI2qrMOKQ.png,,False
876777524,Math | Obviously Awesome,,"Recently, a colleague of mine asked me a few questions like “why do we have so many activation functions?”, “why is that one works better…",https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0,my_library,"activations, deep-learning",2021-02-04T02:36:57.000Z,https://miro.medium.com/v2/resize:fit:325/0*8U8_aa9hMsGmzMY2.,,False
876777523,Math | Obviously Awesome,,"Activation functions are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.",https://paperswithcode.com/methods/category/activation-functions,my_library,"activations, deep-learning",2021-02-04T02:36:40.000Z,https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.49.45_PM.png,,False
876771974,Learning the Differences between Softmax and Sigmoid for Image Classificati,,Week Two - 100 Days of Code Challenge,https://dev.to/rosejcday/learning-the-differences-between-softmax-and-sigmoid-for-image-classification--59c,my_library,"activations, deep-learning, image-classification",2019-08-30T02:46:30.000Z,"https://media.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fimages6.alphacoders.com%2F368%2F368992.jpg",,False
876771966,Deep Learning: Which Loss and Activation Functions should I use?,,The purpose of this post is to provide guidance on which combination of final-layer activation function and loss function should be used in…,https://medium.com/@srnghn/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8,my_library,"activations, deep-learning, loss-functions",2019-08-30T01:10:12.000Z,https://miro.medium.com/v2/resize:fit:284/1*85yYbdUgMBXpcKw1uHgqNg.png,,False
876769198,Choosing the right activation function in a neural network,,"Stay up-to-date on the latest data science and AI news in the worlds of artificial intelligence, machine learning, deep learning, implementation, and more.",https://opendatascience.com/blog/choosing-the-right-activation-function-in-a-neural-network,my_library,"activations, deep-learning",2018-02-12T18:43:43.000Z,https://opendatascience.com/wp-content/uploads/2020/02/Untitled-1.png,,False
