id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876788727,NSVQ: Improved Vector Quantization technique for Neural Networks Training,,"Efficient vector quantization for machine learning optimizations (eps. vector quantized variational autoencoders), better than straight…",https://towardsdatascience.com/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce,my_library,"autoencoders, compression-encoding, deep-learning, machine-learning, search",2022-10-13T14:08:29.000Z,https://miro.medium.com/v2/da:true/resize:fit:1200/0*mQbE21i6fsfqpU3L,,False
876772829,Research Guide: Model Distillation Techniques for Deep Learning,,Knowledge distillation is a model compression technique whereby a small network (student) is taught by a larger trained neural network (teacher). The smaller network is trained to behave like the large neural network. This enables the deployment of such models… Continue reading Research Guide: Model Distillation Techniques for Deep Learning,https://heartbeat.fritz.ai/research-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb,my_library,"compression-encoding, deep-learning",2019-12-14T20:34:44.000Z,https://fritz.ai/wp-content/uploads/2023/09/1Xoll0HT4YUfF_DhJZqiiuA.jpeg,,False
