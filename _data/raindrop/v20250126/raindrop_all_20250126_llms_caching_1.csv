id,title,note,excerpt,url,folder,tags,created,cover,highlights,favorite
876791724,üìù Guest Post: Caching LLM Queries for Improved Performance and Cost Savings*,,"If you're looking for a way to improve the performance of your large language model (LLM) application while reducing costs, consider utilizing a semantic cache to store LLM responses.",https://thesequence.substack.com/p/guest-post-caching-llm-queries-for,my_library,"caching, deep-learning, llms",2023-04-12T02:16:11.000Z,"https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1763eb47-236b-4dfd-851c-2a388c7a5671_3200x1454.png",,False
