{"status": 1, "complete": 1, "list": {"2848510220": {"item_id": "2848510220", "resolved_id": "2848510220", "given_url": "http://ieeexplore.ieee.org/document/8621059", "given_title": "Feature Boosting Network For 3D Pose Estimation", "favorite": "0", "status": "1", "time_added": "1578658294", "time_updated": "1638708525", "time_read": "1582142817", "time_favorited": "0", "sort_id": 0, "resolved_title": "Feature Boosting Network For 3D Pose Estimation", "resolved_url": "https://ieeexplore.ieee.org/document/8621059", "excerpt": "In this paper, a feature boosting network is proposed for estimating 3D hand pose and 3D body pose from a single RGB image. In this method, the features learned by the convolutional layers are boosted with a new long short-term dependence-aware (LSTD) module, which enables the intermediate convolutional feature maps to perceive the graphical long short-term dependency among different hand (or body) parts using the designed Graphical ConvLSTM. Learning a set of features that are reliable and discriminatively representative of the pose of a hand (or body) part is difficult due to the ambiguities, texture and illumination variation, and self-occlusion in the real application of 3D pose estimation. To improve the reliability of the features for representing each body part and enhance the LSTD module, we further introduce a context consistency gate (CCG) in this paper, with which the convolutional feature maps are modulated according to their consistency with the context representations. We evaluate the proposed method on challenging benchmark datasets for 3D hand pose estimation and 3D full body pose estimation. Experimental results show the effectiveness of our method that achieves state-of-the-art performance on both of the tasks.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "en", "top_image_url": "https://ieeexplore.ieee.org/assets/img/ieee_logo_smedia_200X200.png", "tags": {"boosting": {"item_id": "2848510220", "tag": "boosting"}, "deep-learning": {"item_id": "2848510220", "tag": "deep-learning"}, "pose-estimation": {"item_id": "2848510220", "tag": "pose-estimation"}}, "authors": {"1133402": {"item_id": "2848510220", "author_id": "1133402", "name": "Jun Liu", "url": ""}, "183370692": {"item_id": "2848510220", "author_id": "183370692", "name": "Henghui Ding", "url": ""}}, "listen_duration_estimate": 0}, "2560109132": {"item_id": "2560109132", "resolved_id": "2557165370", "given_url": "https://blog.nanonets.com/human-pose-estimation-2d-guide/?utm_source=reddit&utm_medium=social&utm_campaign=pose&utm_content=GROUP_NAME", "given_title": "A 2019 guide to Human Pose Estimation with Deep Learning", "favorite": "0", "status": "1", "time_added": "1555469373", "time_updated": "1638708525", "time_read": "1555553498", "time_favorited": "0", "sort_id": 1, "resolved_title": "", "resolved_url": "https://blog.nanonets.com/human-pose-estimation-2d-guide/", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"deep-learning": {"item_id": "2560109132", "tag": "deep-learning"}, "pose-estimation": {"item_id": "2560109132", "tag": "pose-estimation"}}, "listen_duration_estimate": 0}, "2779527809": {"item_id": "2779527809", "resolved_id": "2779527809", "given_url": "https://github.com/FORTH-ModelBasedTracker/MocapNET", "given_title": "FORTH-ModelBasedTracker/MocapNET: We present MocapNET, an ensemble of SNN e", "favorite": "0", "status": "1", "time_added": "1594391205", "time_updated": "1594426577", "time_read": "1594426577", "time_favorited": "0", "sort_id": 2, "resolved_title": "MocapNET Project", "resolved_url": "https://github.com/FORTH-ModelBasedTracker/MocapNET", "excerpt": "A new version of MocapNET has landed! It contains a very big list of improvements that have been carried out during 2020 over the original work that allows higher accuracy, smoother BVH output and better occlusion robustness while maintaining realtime perfomance.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2674", "lang": "en", "time_to_read": 12, "top_image_url": "https://opengraph.githubassets.com/5078d72a22f228000428da6b51c644935879b705eba2b629e1312dd36641744c/FORTH-ModelBasedTracker/MocapNET", "tags": {"pose-estimation": {"item_id": "2779527809", "tag": "pose-estimation"}, "vision": {"item_id": "2779527809", "tag": "vision"}}, "image": {"item_id": "2779527809", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/mnet2.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2779527809", "image_id": "1", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/mnet2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2779527809", "image_id": "2", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/shuffle.gif", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2779527809", "image_id": "3", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/youtube.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2779527809", "image_id": "4", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/bvh.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2779527809", "image_id": "5", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/blender.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2779527809", "image_id": "6", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/leedsDataset.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2779527809", "image_id": "7", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/youtubevideolink2.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2779527809", "image_id": "8", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/youtubevideolink.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2779527809", "image_id": "9", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/ICPR2020_posterYoutubeVideoLink.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2779527809", "image_id": "10", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/transparentTab.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "2779527809", "image_id": "11", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show0.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "2779527809", "image_id": "12", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show3.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "2779527809", "image_id": "13", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show1.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "2779527809", "image_id": "14", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show0ogl.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "15": {"item_id": "2779527809", "image_id": "15", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/mocapnet_rosnode/main/doc/screenshot.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "2779527809", "image_id": "16", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/CSVClusterPlot.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "17": {"item_id": "2779527809", "image_id": "17", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/BVHGUI2.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 1035}, "2476227623": {"item_id": "2476227623", "resolved_id": "2476227623", "given_url": "https://paperswithcode.com/task/pose-estimation", "given_title": "Browse the State-of-the-Art in Machine Learning | Papers With Code", "favorite": "0", "status": "1", "time_added": "1608572506", "time_updated": "1638708525", "time_read": "1608603769", "time_favorited": "0", "sort_id": 3, "resolved_title": "Papers with Code : Pose Estimation", "resolved_url": "https://paperswithcode.com/task/pose-estimation", "excerpt": "Stay informed on the latest trending ML papers with code, research developments, libraries, methods, and datasets.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "16", "lang": "en", "top_image_url": "https://production-media.paperswithcode.com/tasks/Screenshot_2019-11-28_at_22.28.25_6MeKR2X.png", "tags": {"deep-learning": {"item_id": "2476227623", "tag": "deep-learning"}, "pose-estimation": {"item_id": "2476227623", "tag": "pose-estimation"}}, "listen_duration_estimate": 6}, "2870968440": {"item_id": "2870968440", "resolved_id": "2870968440", "given_url": "https://thenextweb.com/syndication/2020/02/01/machine-learning-for-everyone-how-to-implement-pose-estimation-in-a-browser-using-your-webcam/", "given_title": "Machine learning for everyone: How to implement pose estimation in a browse", "favorite": "0", "status": "1", "time_added": "1580553199", "time_updated": "1608262962", "time_read": "1582142608", "time_favorited": "0", "sort_id": 4, "resolved_title": "Machine learning for everyone: How to implement pose estimation in a browser using your webcam", "resolved_url": "https://thenextweb.com/syndication/2020/02/01/machine-learning-for-everyone-how-to-implement-pose-estimation-in-a-browser-using-your-webcam/", "excerpt": "The 20th century turned out to be an era of exponential growth in the field of machine learning.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1513", "lang": "en", "time_to_read": 7, "top_image_url": "https://img-cdn.tnwcdn.com/image/tnw?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2020%2F01%2FCopy-of-Copy-of-Copy-of-Copy-of-...-5.png&signature=08774abbc166a02109c04953d5f74e89", "tags": {"pose-estimation": {"item_id": "2870968440", "tag": "pose-estimation"}}, "authors": {"126442678": {"item_id": "2870968440", "author_id": "126442678", "name": "Kartik Nighania", "url": "https://thenextweb.com/author/kartik-nighania/"}}, "image": {"item_id": "2870968440", "src": "https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/01/Copy-of-Copy-of-Copy-of-Copy-of-...-5-796x417.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2870968440", "image_id": "1", "src": "https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2020/01/Copy-of-Copy-of-Copy-of-Copy-of-...-5-796x417.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2870968440", "image_id": "2", "src": "https://miro.medium.com/max/1537/1*L0UCQeynNlzH8kwdDtM9LQ.png", "width": "1537", "height": "1025", "credit": "", "caption": ""}, "3": {"item_id": "2870968440", "image_id": "3", "src": "https://miro.medium.com/max/825/1*YVjUpFHXlBQetBk46F0KxA.png", "width": "825", "height": "879", "credit": "", "caption": ""}, "4": {"item_id": "2870968440", "image_id": "4", "src": "https://miro.medium.com/max/555/1*7XPisaXgva97r2beNrfrpg.png", "width": "555", "height": "442", "credit": "", "caption": ""}, "5": {"item_id": "2870968440", "image_id": "5", "src": "https://miro.medium.com/max/490/1*aepbsCeUatjTvS2zSaA06Q.png", "width": "490", "height": "234", "credit": "", "caption": ""}, "6": {"item_id": "2870968440", "image_id": "6", "src": "https://miro.medium.com/max/809/1*dClYxLdf-7b-TK4QigPdBg.png", "width": "809", "height": "805", "credit": "", "caption": ""}, "7": {"item_id": "2870968440", "image_id": "7", "src": "https://miro.medium.com/max/650/1*dMS9-tknztdcSumiP59zGQ.png", "width": "650", "height": "525", "credit": "", "caption": ""}, "8": {"item_id": "2870968440", "image_id": "8", "src": "https://miro.medium.com/max/501/1*4_Hv2UqmXKE9sNZME0TSlg.png", "width": "501", "height": "86", "credit": "", "caption": ""}, "9": {"item_id": "2870968440", "image_id": "9", "src": "https://miro.medium.com/max/953/1*zO7VSOAv82soG2gYu0l5Qw.png", "width": "953", "height": "178", "credit": "", "caption": ""}, "10": {"item_id": "2870968440", "image_id": "10", "src": "https://miro.medium.com/max/493/1*mMQwuHWBCmqKm2bbh_vlhQ.png", "width": "493", "height": "260", "credit": "", "caption": ""}, "11": {"item_id": "2870968440", "image_id": "11", "src": "https://miro.medium.com/max/600/1*pVPUKC-SPT2Vg4vLWdN6iA.gif", "width": "600", "height": "602", "credit": "", "caption": ""}, "12": {"item_id": "2870968440", "image_id": "12", "src": "https://miro.medium.com/max/600/1*RPBw9EuFTWPtVWcGdHYG-Q.gif", "width": "600", "height": "600", "credit": "", "caption": ""}}, "domain_metadata": {"name": "The Next Web", "logo": "https://logo.clearbit.com/thenextweb.com?size=800", "greyscale_logo": "https://logo.clearbit.com/thenextweb.com?size=800&greyscale=true"}, "listen_duration_estimate": 586}, "3316426075": {"item_id": "3316426075", "resolved_id": "3316426107", "given_url": "https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-approach-explained-4348d6d79b11?source=rss----7f60cf5620c9---4", "given_title": "Gentle introduction to 2D Hand Pose Estimation: Approach Explained", "favorite": "0", "status": "1", "time_added": "1619441999", "time_updated": "1619455522", "time_read": "1619455522", "time_favorited": "0", "sort_id": 5, "resolved_title": "Gentle introduction to 2D Hand Pose Estimation: Approach Explained", "resolved_url": "https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-approach-explained-4348d6d79b11", "excerpt": "In 2018 I spent 6 months working on my master’s thesis on Hand Pose Estimation. That was a challenging and insightful period of my life that resulted in 40-page research.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1949", "lang": "en", "time_to_read": 9, "top_image_url": "https://miro.medium.com/max/1200/0*CvYL8OI0js7MWUlM", "tags": {"machine-learning": {"item_id": "3316426075", "tag": "machine-learning"}, "machine-vision": {"item_id": "3316426075", "tag": "machine-vision"}, "pose-estimation": {"item_id": "3316426075", "tag": "pose-estimation"}}, "authors": {"149901236": {"item_id": "3316426075", "author_id": "149901236", "name": "Olga Chernytska", "url": "https://olga-chernytska.medium.com"}}, "image": {"item_id": "3316426075", "src": "https://miro.medium.com/fit/c/56/56/1*K36RJKHrrxD3izmCK3tZKw.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3316426075", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*K36RJKHrrxD3izmCK3tZKw.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3316426075", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*CvYL8OI0js7MWUlM", "width": "700", "height": "465", "credit": "", "caption": "Image by Author"}, "3": {"item_id": "3316426075", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*PrMoyDC0-fFt2_mnmw_aGQ.png", "width": "700", "height": "352", "credit": "", "caption": "Image 1. Hand keypoints. Order matters here. Image by Author"}, "4": {"item_id": "3316426075", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*mThXrH5ioCkZNHAOvtfYaA.png", "width": "700", "height": "399", "credit": "", "caption": "Image 2. Visualization of typical 2D Hand Pose Estimator. Image by Author"}, "5": {"item_id": "3316426075", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*ZxLvJprsRlLu8h-Hxqn1dw.png", "width": "700", "height": "350", "credit": "", "caption": "Image 3. Left: how hand pose is visualized. Right: how hand pose is estimated. Image by Author"}, "6": {"item_id": "3316426075", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*t0V4Q9Bv3X8_EuSuxPm1ZA.png", "width": "700", "height": "483", "credit": "", "caption": "Image 4. Random samples from FreiHAND dataset with 2D keypoint labels.  Colors: green for thumb, cyan for index finger, blue for middle, pink for ring and red for little finger."}, "7": {"item_id": "3316426075", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*dnATfJDH_TPnLuGJpxB22w.png", "width": "700", "height": "545", "credit": "", "caption": "Image 5. When resizing an image, keypoint locations should be also “resized”. Image by Author"}, "8": {"item_id": "3316426075", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*tcMFmcFRG7o3xiZ-9b6azg.png", "width": "700", "height": "318", "credit": "", "caption": "Image 6. How to create a heatmap for a keypoint. Image by Author"}, "9": {"item_id": "3316426075", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*ovqDOUIAhpJ4hNlMHa8gqg.png", "width": "700", "height": "378", "credit": "", "caption": "Image 7. My simple custom UNet-like model for 2D Hand Pose Estimation. Image by Author"}, "10": {"item_id": "3316426075", "image_id": "10", "src": "https://miro.medium.com/max/1104/1*78bZt9dM2FDukswdsYVopg.png", "width": "552", "height": "342", "credit": "", "caption": "Image 8. How to calculate IoU loss for heatmaps. yi — predicted values, ti — target values for a pixel in a heatmap. Loss is calculated for each heatmap separately, then averaged among all 21 heatmaps, and then averaged among the images in the batch."}, "11": {"item_id": "3316426075", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*2J2RWoSlEGzq1Eg7J_QMKg.png", "width": "700", "height": "481", "credit": "", "caption": "Image 9. Model output for a random image from the test set. Image by Author"}, "12": {"item_id": "3316426075", "image_id": "12", "src": "https://miro.medium.com/max/1400/1*1d4R7Dv-W4zyY1aWp2Vi5w.png", "width": "700", "height": "331", "credit": "", "caption": "Image 10. How to calculate keypoint location from a heatmap by averaging. Image by Author"}, "13": {"item_id": "3316426075", "image_id": "13", "src": "https://miro.medium.com/max/1400/1*MiVDWs2MTZlHO3dOnl0VCg.png", "width": "700", "height": "959", "credit": "", "caption": "Image 11. Visualizing predictions on test set."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 754}, "3449750075": {"item_id": "3449750075", "resolved_id": "3449750096", "given_url": "https://towardsdatascience.com/hrnet-explained-human-pose-estimation-sematic-segmentation-and-object-detection-63f1ce79ef82?source=rss----7f60cf5620c9---4", "given_title": "HRNet explained: Human Pose Estimation, Semantic Segmentation and Object De", "favorite": "0", "status": "1", "time_added": "1633606526", "time_updated": "1633630869", "time_read": "1633630869", "time_favorited": "0", "sort_id": 6, "resolved_title": "HRNet explained: Human Pose Estimation, Sematic Segmentation and Object Detection", "resolved_url": "https://towardsdatascience.com/hrnet-explained-human-pose-estimation-sematic-segmentation-and-object-detection-63f1ce79ef82", "excerpt": "HRNet is a state-of-the-art algorithm in the field of semantic segmentation, facial landmark detection, and human pose estimation. It has shown superior results in semantic segmentation on datasets like PASCAL Context, LIP, Cityscapes, AFLW, COFW, and 300W.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1874", "lang": "en", "time_to_read": 9, "top_image_url": "https://miro.medium.com/max/1200/1*rLhbwmHhjFF2u9NrO3iWpg.jpeg", "tags": {"deep-learning": {"item_id": "3449750075", "tag": "deep-learning"}, "machine-vision": {"item_id": "3449750075", "tag": "machine-vision"}, "pose-estimation": {"item_id": "3449750075", "tag": "pose-estimation"}, "semantic-segmentation": {"item_id": "3449750075", "tag": "semantic-segmentation"}}, "authors": {"129050273": {"item_id": "3449750075", "author_id": "129050273", "name": "Hucker Marius", "url": "https://medium.com/@hucker.marius"}}, "image": {"item_id": "3449750075", "src": "https://miro.medium.com/fit/c/56/56/2*7IaQcIIvxhB-crtlmJWGoA.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3449750075", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*7IaQcIIvxhB-crtlmJWGoA.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3449750075", "image_id": "2", "src": "https://miro.medium.com/max/9036/1*rLhbwmHhjFF2u9NrO3iWpg.jpeg", "width": "4518", "height": "3011", "credit": "Christian Lue on Unsplash", "caption": ""}, "3": {"item_id": "3449750075", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*9ATXgoGvrOXukaOAgyWuCQ.png", "width": "700", "height": "317", "credit": "", "caption": "Source: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#devkit"}, "4": {"item_id": "3449750075", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*rekAeufx3gMVi3qUL3_UEw.png", "width": "700", "height": "307", "credit": "", "caption": "Source: https://learnopencv.com/face-swap-using-opencv-c-python/"}, "5": {"item_id": "3449750075", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*pcvyvpgNMuy4MCOG5ecKbw.png", "width": "700", "height": "603", "credit": "", "caption": "Source: https://arxiv.org/abs/2103.02440"}, "6": {"item_id": "3449750075", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*4U2mB9ZE46Uj2LQz989MHQ.png", "width": "700", "height": "158", "credit": "7", "caption": "Source: https://cs231n.github.io/convolutional-networks/"}, "7": {"item_id": "3449750075", "image_id": "7", "src": "https://miro.medium.com/max/1052/1*Z87z69ufkGnNolH6R_Ln_w.gif", "width": "526", "height": "384", "credit": "", "caption": "Source: https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"}, "8": {"item_id": "3449750075", "image_id": "8", "src": "https://miro.medium.com/max/1148/1*TCGAV3JvaWABGC34jaN4LQ.png", "width": "574", "height": "304", "credit": "7", "caption": "Source: https://cs231n.github.io/convolutional-networks/"}, "9": {"item_id": "3449750075", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*SGjMTghF8SQdD2l-DDD9vQ.png", "width": "700", "height": "115", "credit": "", "caption": "Source: https://arxiv.org/pdf/1904.04514.pdf"}, "10": {"item_id": "3449750075", "image_id": "10", "src": "https://miro.medium.com/max/928/1*BIYwJGwbm4VGQAOoazg5SQ.png", "width": "464", "height": "468", "credit": "", "caption": "Source: https://arxiv.org/pdf/1904.04514.pdf"}, "11": {"item_id": "3449750075", "image_id": "11", "src": "https://miro.medium.com/max/560/1*5lhntnXLH1YUTJKpX5-EqA.png", "width": "280", "height": "462", "credit": "", "caption": "Source: https://arxiv.org/pdf/1904.04514.pdf"}, "12": {"item_id": "3449750075", "image_id": "12", "src": "https://miro.medium.com/max/1400/1*W2DJWlD--JgtgVLoz_03EA.png", "width": "700", "height": "65", "credit": "", "caption": "Previous CNN with serial convolution and not parallel. Source: https://arxiv.org/pdf/1904.04514.pdf"}, "13": {"item_id": "3449750075", "image_id": "13", "src": "https://miro.medium.com/max/952/1*vh1XNEWgUxn5vF0aP7BjCQ.png", "width": "476", "height": "298", "credit": "", "caption": "HRNetV1 illustration. Source: https://arxiv.org/pdf/1904.04514.pdf"}, "14": {"item_id": "3449750075", "image_id": "14", "src": "https://miro.medium.com/max/928/1*Fq-e4ExsbpZKAJxaN3sR8A.png", "width": "464", "height": "362", "credit": "", "caption": "HRNetV2 illustration. Source: https://arxiv.org/pdf/1904.04514.pdf"}, "15": {"item_id": "3449750075", "image_id": "15", "src": "https://miro.medium.com/max/960/1*keRNOE64DQXTthrITaGOOQ.png", "width": "480", "height": "360", "credit": "", "caption": "HRNetV2p illustration. Source: https://arxiv.org/pdf/1904.04514.pdf"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 725}, "3952949726": {"item_id": "3952949726", "resolved_id": "3952949726", "given_url": "https://www.marktechpost.com/2023/10/20/meet-swimxyz-a-synthetic-dataset-of-swimming-motions-and-videos-containing-3-4m-frames-annotated-with-ground-truth-2d-and-3d-joints/", "given_title": "Meet SwimXYZ: A Synthetic Dataset of Swimming Motions and Videos Containing", "favorite": "0", "status": "1", "time_added": "1697798224", "time_updated": "1697837804", "time_read": "1697837804", "time_favorited": "0", "sort_id": 7, "resolved_title": "Meet SwimXYZ: A Synthetic Dataset of Swimming Motions and Videos Containing 3.4M Frames Annotated with Ground Truth 2D and 3D Joints", "resolved_url": "https://www.marktechpost.com/2023/10/20/meet-swimxyz-a-synthetic-dataset-of-swimming-motions-and-videos-containing-3-4m-frames-annotated-with-ground-truth-2d-and-3d-joints/", "excerpt": "Human motion capture has emerged as a key tool in various industries, including sports, medical, and character animation for the entertainment sector.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "523", "lang": "en", "amp_url": "https://www.marktechpost.com/2023/10/20/meet-swimxyz-a-synthetic-dataset-of-swimming-motions-and-videos-containing-3-4m-frames-annotated-with-ground-truth-2d-and-3d-joints/?amp", "top_image_url": "https://www.marktechpost.com/wp-content/uploads/2023/10/ezgif-5-b8f61baff5.gif", "tags": {"datasets": {"item_id": "3952949726", "tag": "datasets"}, "pose-estimation": {"item_id": "3952949726", "tag": "pose-estimation"}}, "authors": {"166902541": {"item_id": "3952949726", "author_id": "166902541", "name": "Aneesh Tickoo", "url": "https://www.marktechpost.com/author/aneesh-tickoo/"}}, "image": {"item_id": "3952949726", "src": "https://www.marktechpost.com/wp-content/uploads/2023/10/ezgif-5-b8f61baff5.gif", "width": "600", "height": "338"}, "images": {"1": {"item_id": "3952949726", "image_id": "1", "src": "https://www.marktechpost.com/wp-content/uploads/2023/10/ezgif-5-b8f61baff5.gif", "width": "600", "height": "338", "credit": "", "caption": "https://g-fiche.github.io/research-pages/swimxyz/"}}, "listen_duration_estimate": 202}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709419545}