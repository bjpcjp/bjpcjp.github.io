{"status": 1, "complete": 1, "list": {"1433834": {"item_id": "1433834", "resolved_id": "1433834", "given_url": "https://en.wikipedia.org/wiki/Gini_coefficient", "given_title": "", "favorite": "1", "status": "1", "time_added": "1580818479", "time_updated": "1624293719", "time_read": "1581276792", "time_favorited": "1581276792", "sort_id": 0, "resolved_title": "Gini coefficient", "resolved_url": "https://en.wikipedia.org/wiki/Gini_coefficient", "excerpt": "In economics, the Gini coefficient (/ˈdʒiːni/ JEE-nee), also the Gini index and the Gini ratio, is a measure of statistical dispersion intended to represent the income inequality or the wealth inequality within a nation or a social group.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "9032", "lang": "en", "time_to_read": 41, "top_image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/GINI_index_World_Bank_up_to_2018.svg/1200px-GINI_index_World_Bank_up_to_2018.svg.png", "tags": {"info-theory": {"item_id": "1433834", "tag": "info-theory"}}, "authors": {"7331684": {"item_id": "1433834", "author_id": "7331684", "name": "From Wikipedia, the free", "url": ""}}, "image": {"item_id": "1433834", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/GINI_index_World_Bank_up_to_2018.svg/400px-GINI_index_World_Bank_up_to_2018.svg.png", "width": "400", "height": "203"}, "images": {"1": {"item_id": "1433834", "image_id": "1", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/GINI_index_World_Bank_up_to_2018.svg/400px-GINI_index_World_Bank_up_to_2018.svg.png", "width": "400", "height": "203", "credit": "as %", "caption": "World map of income inequality Gini coefficients by country"}, "2": {"item_id": "1433834", "image_id": "2", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/39/Gini_Coefficient_of_Wealth_Inequality_source.png/400px-Gini_Coefficient_of_Wealth_Inequality_source.png", "width": "400", "height": "217", "credit": "2", "caption": "A map showing Gini coefficients for Wealth within countries for 2019."}, "3": {"item_id": "1433834", "image_id": "3", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Global_Wealth_Distribution_2020_%28Property%29.svg/400px-Global_Wealth_Distribution_2020_%28Property%29.svg.png", "width": "400", "height": "399", "credit": "", "caption": "Global share of wealth by wealth group, Credit Suisse, 2021"}, "4": {"item_id": "1433834", "image_id": "4", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Economics_Gini_coefficient2.svg/310px-Economics_Gini_coefficient2.svg.png", "width": "310", "height": "310", "credit": "A + B", "caption": "Graphical representation of the Gini coefficient:\nThe graph shows that the Gini coefficient is equal to the area marked A divided by the sum of the areas marked A and B, that is, Gini = A/"}, "5": {"item_id": "1433834", "image_id": "5", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/466e03e1c9533b4dab1b9949dad393883f385d80", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "1433834", "image_id": "6", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/0aeaa90784f3a811704f2c3dd0c17137a1647df2", "width": "0", "height": "0", "credit": "", "caption": "G=∑i=1n∑j=1n|xi−xj|2∑i=1n∑j=1nxj=∑i=1n∑j=1n|xi−xj|2n∑j=1nxj=∑i=1n∑j=1n|xi−xj|2n2x¯{\\displaystyle G={\\frac {\\displaystyle {\\sum _{i=1}^{n}\\sum _{j=1}^{n}\\left|x_{i}-x_{j}\\right|}}{\\displaystyle {2\\sum _{i=1}^{n}\\sum _{j=1}^{n}x_{j}}}}={\\frac {\\displaystyle {\\sum _{i=1}^{n}\\sum _{j=1}^{n}\\left|x_{i}-x_{j}\\right|}}{\\displaystyle {2n\\sum _{j=1}^{n}x_{j}}}}={\\frac {\\displaystyle {\\sum _{i=1}^{n}\\sum _{j=1}^{n}\\left|x_{i}-x_{j}\\right|}}{\\displaystyle {2n^{2}{\\bar {x}}}}}}"}, "7": {"item_id": "1433834", "image_id": "7", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/1c5e8ef62b413bc7b9fcfb2b6b6f5220c14831b4", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "1433834", "image_id": "8", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/ebb8cf000dd875b6afcfa79c16e9d2b597f5bf86", "width": "0", "height": "0", "credit": "x", "caption": "μ=∫−∞∞xp"}, "9": {"item_id": "1433834", "image_id": "9", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Gini_coefficient_for_distribution_with_only_two_income_or_wealth_levels.svg/220px-Gini_coefficient_for_distribution_with_only_two_income_or_wealth_levels.svg.png", "width": "220", "height": "220", "credit": "red", "caption": "Richest u of population"}, "10": {"item_id": "1433834", "image_id": "10", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/f38b5319dcec4a3132ca87f3bed4cc94c0199014", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "1433834", "image_id": "11", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/07b36fef97fe40681932f30390081167d4e33944", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "1433834", "image_id": "12", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/99825fc6ea51986c29bc773afc3c5d7c1b7129cc", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "1433834", "image_id": "13", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/fbf97f4b64415eed31e8175a1ea45066ee995c5c", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "1433834", "image_id": "14", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/9e52668bec1411c5a2a122743fa66f8bbc986542", "width": "0", "height": "0", "credit": "yi", "caption": "f"}, "15": {"item_id": "1433834", "image_id": "15", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/a5726d00b79af1b4666a6319c45381579dc85a9a", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "1433834", "image_id": "16", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/bb0fb85a2b800d44c5f1a7ba8ef3b6140db96d1c", "width": "0", "height": "0", "credit": "yi", "caption": "f"}, "17": {"item_id": "1433834", "image_id": "17", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/2000cdffbebd85d221d8cc0cc18738a220caf555", "width": "0", "height": "0", "credit": "", "caption": "yi>0{\\displaystyle y_{i}>0}"}, "18": {"item_id": "1433834", "image_id": "18", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/23b546171e2026cd970edae7b151cf55d6336678", "width": "0", "height": "0", "credit": "", "caption": ""}, "19": {"item_id": "1433834", "image_id": "19", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/b1a0be20925fcef8806b41ced18f8398bfae50ee", "width": "0", "height": "0", "credit": "", "caption": ""}, "20": {"item_id": "1433834", "image_id": "20", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/53adfcbfb798110764d9ab2a29f9e026c28367f2", "width": "0", "height": "0", "credit": "", "caption": ""}, "21": {"item_id": "1433834", "image_id": "21", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/ec77cfcdb9a93ec2dcf86e2c6ec20976c1b70ac2", "width": "0", "height": "0", "credit": "", "caption": ""}, "22": {"item_id": "1433834", "image_id": "22", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/6f8cf1e1a259db25a88d44824f01c44c5ac23aaf", "width": "0", "height": "0", "credit": "", "caption": ""}, "23": {"item_id": "1433834", "image_id": "23", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/ba4044a82ec305366f34042b2efb5496a5bb597b", "width": "0", "height": "0", "credit": "", "caption": "S0=0.{\\displaystyle S_{0}=0.}"}, "24": {"item_id": "1433834", "image_id": "24", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/f496c96c26704d53a89d882bbb9d342d8f5a0b90", "width": "0", "height": "0", "credit": "", "caption": "n→∞.{\\displaystyle n\\rightarrow \\infty .}"}, "25": {"item_id": "1433834", "image_id": "25", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/e4815876380a1d4a5c6f042d449e0b3c180086c0", "width": "0", "height": "0", "credit": "", "caption": ""}, "26": {"item_id": "1433834", "image_id": "26", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/bb4f36cf9104ae1e7b1196be60fe0e179d30805f", "width": "0", "height": "0", "credit": "", "caption": ""}, "27": {"item_id": "1433834", "image_id": "27", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/cb2eaf9d9436626709a550941dad47f2df3e7525", "width": "0", "height": "0", "credit": "", "caption": ""}, "28": {"item_id": "1433834", "image_id": "28", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/6711ae78ab4dd6f8ed7b3fe5f68e7ba4f0939c02", "width": "0", "height": "0", "credit": "", "caption": ""}, "29": {"item_id": "1433834", "image_id": "29", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/f90633330824c7bc3d8a3278660eac2edf524099", "width": "0", "height": "0", "credit": "", "caption": ""}, "30": {"item_id": "1433834", "image_id": "30", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/050f58ceb781781fce54a6d12e8ea1389089e27f", "width": "0", "height": "0", "credit": "", "caption": ""}, "31": {"item_id": "1433834", "image_id": "31", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/1a8ee8591a5cc9057c9c646d1d07b848c1d68e81", "width": "0", "height": "0", "credit": "", "caption": ""}, "32": {"item_id": "1433834", "image_id": "32", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/0b2d9606af752a9a180d98dade729c0d7e2633df", "width": "0", "height": "0", "credit": "σ2", "caption": "G=2Φ"}, "33": {"item_id": "1433834", "image_id": "33", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/aed80a2011a3912b028ba32a52dfa57165455f24", "width": "0", "height": "0", "credit": "", "caption": ""}, "34": {"item_id": "1433834", "image_id": "34", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/8dc2d914c2df66bc0f7893bfb8da36766650fe47", "width": "0", "height": "0", "credit": "", "caption": ""}, "35": {"item_id": "1433834", "image_id": "35", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/db5b0aa87688e6070ee35465aee14f4de31b8a10", "width": "0", "height": "0", "credit": "", "caption": ""}, "36": {"item_id": "1433834", "image_id": "36", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/9747c28ca4f1f8ddbb3ad0f5c184120e2270b843", "width": "0", "height": "0", "credit": "", "caption": ""}, "37": {"item_id": "1433834", "image_id": "37", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/a4cd398aacaa08c0a49031e96c06ef6cd2b22d64", "width": "0", "height": "0", "credit": "", "caption": ""}, "38": {"item_id": "1433834", "image_id": "38", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/7d446981e5008b4f43bed33306548a985d0f0f61", "width": "0", "height": "0", "credit": "", "caption": ""}, "39": {"item_id": "1433834", "image_id": "39", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/e308a3a46b7fdce07cc09dcab9e8d8f73e37d935", "width": "0", "height": "0", "credit": "", "caption": ""}, "40": {"item_id": "1433834", "image_id": "40", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/52447c709e478dfc03063cfff61c56659d825ef0", "width": "0", "height": "0", "credit": "", "caption": "1xσ2πe−12(ln(x)−μσ)2{\\displaystyle {\\frac {1}{x\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {\\ln \\,(x)-\\mu }{\\sigma }}\\right)^{2}}}"}, "41": {"item_id": "1433834", "image_id": "41", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/7fa1b4489e2efdaefe84d3d9f427f34906da12bb", "width": "0", "height": "0", "credit": "σ/2", "caption": "erf"}, "42": {"item_id": "1433834", "image_id": "42", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/03e401ef333be0869460084ec9d062270cd19dd4", "width": "0", "height": "0", "credit": "", "caption": ""}, "43": {"item_id": "1433834", "image_id": "43", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/1b605be7b8ef0ca5d721690166f8c45cc99be739", "width": "0", "height": "0", "credit": "", "caption": ""}, "44": {"item_id": "1433834", "image_id": "44", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/719d8e1d90a6a3e6a020d8a047217991772da586", "width": "0", "height": "0", "credit": "", "caption": ""}, "45": {"item_id": "1433834", "image_id": "45", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/79808ff9ffb33e3e821a176c33c6fdc6a72cb6cc", "width": "0", "height": "0", "credit": "1+k2", "caption": "2Γ"}, "46": {"item_id": "1433834", "image_id": "46", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/46052c4c41cbbeb2b008a53ddb7c73d7df30564d", "width": "0", "height": "0", "credit": "", "caption": ""}, "47": {"item_id": "1433834", "image_id": "47", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/47de10626269bc2faca8c2f73372d2a733749660", "width": "0", "height": "0", "credit": "", "caption": ""}, "48": {"item_id": "1433834", "image_id": "48", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/e4fc4c72b4bafd2590e2fd43357d0c562351aef3", "width": "0", "height": "0", "credit": "xλ", "caption": "kλ"}, "49": {"item_id": "1433834", "image_id": "49", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/b8117307e37c52653dd5969021cf63c2c5b75475", "width": "0", "height": "0", "credit": "", "caption": ""}, "50": {"item_id": "1433834", "image_id": "50", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/fd843d3b4f8107294d472539c3e683e0f179f55f", "width": "0", "height": "0", "credit": "1−x", "caption": "xα−1"}, "51": {"item_id": "1433834", "image_id": "51", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/88be3deb7b13b3ff6ab2af9ce56629fed6c5980f", "width": "0", "height": "0", "credit": "2α", "caption": ""}, "52": {"item_id": "1433834", "image_id": "52", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/b1371be5f4752531881e8d39dc19bd32a5c4e7b3", "width": "0", "height": "0", "credit": "β/α", "caption": ""}, "53": {"item_id": "1433834", "image_id": "53", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/f6c99413f61c93d4b156a3db9d6f801af6e73a0f", "width": "0", "height": "0", "credit": "", "caption": ""}, "54": {"item_id": "1433834", "image_id": "54", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/61f86e18337c8c5101b9cddc69dfcafe9e52b2d3", "width": "0", "height": "0", "credit": "", "caption": ""}, "55": {"item_id": "1433834", "image_id": "55", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/c56b1af5476f3ace651234955d6776f293380143", "width": "0", "height": "0", "credit": "", "caption": ""}, "56": {"item_id": "1433834", "image_id": "56", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/039a108ec870ef90aeed3f0c2dac7585241894b1", "width": "0", "height": "0", "credit": "", "caption": ""}, "57": {"item_id": "1433834", "image_id": "57", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/9fd47b2a39f7a7856952afec1f1db72c67af6161", "width": "0", "height": "0", "credit": "", "caption": ""}, "58": {"item_id": "1433834", "image_id": "58", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/af4a0955af42beb5f85aa05fb8c07abedc13990d", "width": "0", "height": "0", "credit": "", "caption": ""}, "59": {"item_id": "1433834", "image_id": "59", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/e15efedea663fe578e737f2fb0c3eb317b41406a", "width": "0", "height": "0", "credit": "", "caption": ""}, "60": {"item_id": "1433834", "image_id": "60", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/5db47cb3d2f9496205a17a6856c91c1d3d363ccd", "width": "0", "height": "0", "credit": "", "caption": ""}, "61": {"item_id": "1433834", "image_id": "61", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/9fa4039bbc2a0048c3a3c02e5fd24390cab0dc97", "width": "0", "height": "0", "credit": "", "caption": ""}, "62": {"item_id": "1433834", "image_id": "62", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/3d0f2f3303ccc6221141c7055c724e500ba34b7b", "width": "0", "height": "0", "credit": "", "caption": ""}, "63": {"item_id": "1433834", "image_id": "63", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Lorenz_curve_global_income_2011.svg/250px-Lorenz_curve_global_income_2011.svg.png", "width": "250", "height": "156", "credit": "", "caption": "Derivation of the Lorenz curve and Gini coefficient for global income in 2011"}, "64": {"item_id": "1433834", "image_id": "64", "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/0/01/Gini_since_WWII.svg/720px-Gini_since_WWII.svg.png", "width": "720", "height": "521", "credit": "", "caption": ""}, "65": {"item_id": "1433834", "image_id": "65", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/e30686bb08576301cd2c30aade56036fc32ae2f9", "width": "0", "height": "0", "credit": "", "caption": ""}, "66": {"item_id": "1433834", "image_id": "66", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/1add931eabb9aec89275acb2f50d50428010b0fe", "width": "0", "height": "0", "credit": "", "caption": ""}, "67": {"item_id": "1433834", "image_id": "67", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a", "width": "0", "height": "0", "credit": "", "caption": ""}, "68": {"item_id": "1433834", "image_id": "68", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/da4535bdb18a0914505c66a316f858f601be0cf8", "width": "0", "height": "0", "credit": "", "caption": "1−λ{\\displaystyle 1-\\lambda }"}, "69": {"item_id": "1433834", "image_id": "69", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/e6ea4f4668b8334c8a7d3d284b0fd22131ef5f52", "width": "0", "height": "0", "credit": "", "caption": ""}, "70": {"item_id": "1433834", "image_id": "70", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/ef6bbb79090564e2f820ef45d55c9f9d9d9226fa", "width": "0", "height": "0", "credit": "1979", "caption": "Allison, Paul D."}, "71": {"item_id": "1433834", "image_id": "71", "src": "https://wikimedia.org/api/rest_v1/media/math/render/svg/2557531f95642458cca8c305124aba1784ab9b3f", "width": "0", "height": "0", "credit": "1979", "caption": "Jasso, Guillermina"}}, "domain_metadata": {"name": "Wikipedia", "logo": "https://logo.clearbit.com/wikipedia.org?size=800", "greyscale_logo": "https://logo.clearbit.com/wikipedia.org?size=800&greyscale=true"}, "listen_duration_estimate": 3496}, "2756882840": {"item_id": "2756882840", "resolved_id": "2756882840", "given_url": "https://machinelearningmastery.com/what-is-information-entropy/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1601117928", "time_updated": "1624293719", "time_read": "1604362105", "time_favorited": "0", "sort_id": 1, "resolved_title": "A Gentle Introduction to Information Entropy", "resolved_url": "https://machinelearningmastery.com/what-is-information-entropy/", "excerpt": "Information theory is a subfield of mathematics concerned with transmitting data across a noisy channel. A cornerstone of information theory is the idea of quantifying how much information there is in a message.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2369", "lang": "en", "time_to_read": 11, "top_image_url": "https://machinelearningmastery.com/wp-content/uploads/2019/10/Plot-of-Probability-Distribution-vs-Entropy.png", "tags": {"info-theory": {"item_id": "2756882840", "tag": "info-theory"}, "machine-learning": {"item_id": "2756882840", "tag": "machine-learning"}}, "authors": {"26997241": {"item_id": "2756882840", "author_id": "26997241", "name": "Jason Brownlee", "url": "https://machinelearningmastery.com/author/jasonb/"}}, "image": {"item_id": "2756882840", "src": "https://machinelearningmastery.com/wp-content/uploads/2019/10/A-Gentle-Introduction-to-Information-Entropy.jpg", "width": "640", "height": "424"}, "images": {"1": {"item_id": "2756882840", "image_id": "1", "src": "https://machinelearningmastery.com/wp-content/uploads/2019/10/A-Gentle-Introduction-to-Information-Entropy.jpg", "width": "640", "height": "424", "credit": "", "caption": "A Gentle Introduction to Information Entropy   \nPhoto by Cristiano Medeiros Dalbem, some rights reserved."}, "2": {"item_id": "2756882840", "image_id": "2", "src": "https://machinelearningmastery.com/wp-content/uploads/2019/09/Cover-220-1.png", "width": "220", "height": "311", "credit": "", "caption": ""}}, "listen_duration_estimate": 917}, "3366862219": {"item_id": "3366862219", "resolved_id": "3366862219", "given_url": "https://arxiv.org/pdf/1802.05968", "given_title": "", "favorite": "0", "status": "1", "time_added": "1624829669", "time_updated": "1624997421", "time_read": "1624997421", "time_favorited": "0", "sort_id": 2, "resolved_title": "", "resolved_url": "https://arxiv.org/pdf/1802.05968", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"info-theory": {"item_id": "3366862219", "tag": "info-theory"}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "2974602893": {"item_id": "2974602893", "resolved_id": "2974602893", "given_url": "https://notamonadtutorial.com/a-brief-introduction-to-the-beauty-of-information-theory-8357f5b6a355", "given_title": "A brief introduction to the beauty of Information Theory", "favorite": "0", "status": "1", "time_added": "1588844736", "time_updated": "1624293719", "time_read": "1589136674", "time_favorited": "0", "sort_id": 3, "resolved_title": "A brief introduction to the beauty of Information Theory", "resolved_url": "https://notamonadtutorial.com/a-brief-introduction-to-the-beauty-of-information-theory-8357f5b6a355", "excerpt": "Authors: Juan Pablo Amoroso, Javier Rodríguez Chatruc, Camilo Plata, and Federico Carrone. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. — Claude Shannon, 1948", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1371", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1200/1*TkW9quvg52IBgM06fM-7IA.jpeg", "tags": {"info-theory": {"item_id": "2974602893", "tag": "info-theory"}, "machine-learning": {"item_id": "2974602893", "tag": "machine-learning"}}, "authors": {"134054171": {"item_id": "2974602893", "author_id": "134054171", "name": "Federico Carrone", "url": "https://notamonadtutorial.com/@federicocarrone"}}, "image": {"item_id": "2974602893", "src": "https://miro.medium.com/max/1732/1*kCNAUFlhWJv1kUzKatqINA.png", "width": "866", "height": "170"}, "images": {"1": {"item_id": "2974602893", "image_id": "1", "src": "https://miro.medium.com/max/1732/1*kCNAUFlhWJv1kUzKatqINA.png", "width": "866", "height": "170", "credit": "", "caption": ""}, "2": {"item_id": "2974602893", "image_id": "2", "src": "https://miro.medium.com/max/3568/1*TgCse1znfuWYULYwXeo4Aw.png", "width": "1784", "height": "1162", "credit": "", "caption": ""}, "3": {"item_id": "2974602893", "image_id": "3", "src": "https://miro.medium.com/max/2036/1*fCyO-nZidJEiOqwqDGWJ3g.png", "width": "1018", "height": "162", "credit": "", "caption": ""}, "4": {"item_id": "2974602893", "image_id": "4", "src": "https://miro.medium.com/max/3496/1*s0z3MUCTtJvh_AsvxGg72g.png", "width": "1748", "height": "1194", "credit": "", "caption": ""}, "5": {"item_id": "2974602893", "image_id": "5", "src": "https://miro.medium.com/max/1336/1*ezweLprVK1INseQwDCN2yg.png", "width": "668", "height": "260", "credit": "", "caption": ""}, "6": {"item_id": "2974602893", "image_id": "6", "src": "https://miro.medium.com/max/4200/1*TkW9quvg52IBgM06fM-7IA.jpeg", "width": "2100", "height": "2100", "credit": "", "caption": "Hardcore Guess Who gamers apply Information Theory for optimal results"}, "7": {"item_id": "2974602893", "image_id": "7", "src": "https://miro.medium.com/max/1540/1*KZq1CO03SyEnsH0qLamzRQ.png", "width": "770", "height": "224", "credit": "", "caption": ""}, "8": {"item_id": "2974602893", "image_id": "8", "src": "https://miro.medium.com/max/1412/1*tIzlfBpMihRvpfZICpWZJA.png", "width": "706", "height": "156", "credit": "", "caption": ""}, "9": {"item_id": "2974602893", "image_id": "9", "src": "https://miro.medium.com/max/2810/1*dEesB-YyIVG81qhIDn_T_w.png", "width": "1405", "height": "1543", "credit": "", "caption": ""}, "10": {"item_id": "2974602893", "image_id": "10", "src": "https://miro.medium.com/max/2696/1*2dDOZS_8PGYonq3RZYoyAg.png", "width": "1348", "height": "614", "credit": "", "caption": ""}, "11": {"item_id": "2974602893", "image_id": "11", "src": "https://miro.medium.com/max/3396/1*VCQsujq_mEufMZi1X4DGdw.png", "width": "1698", "height": "1120", "credit": "", "caption": ""}}, "listen_duration_estimate": 531}, "2994386236": {"item_id": "2994386236", "resolved_id": "2994386250", "given_url": "https://towardsdatascience.com/entropy-and-information-gain-b738ca8abd2a?source=rss----7f60cf5620c9---4", "given_title": "Entropy and Information Gain", "favorite": "0", "status": "1", "time_added": "1590340588", "time_updated": "1624293719", "time_read": "1591029810", "time_favorited": "0", "sort_id": 4, "resolved_title": "Entropy and Information Gain", "resolved_url": "https://towardsdatascience.com/entropy-and-information-gain-b738ca8abd2a", "excerpt": "Information Gain is yet another method that can also be used to optimally choose which feature to split the data set on. Before we go on to learn about Information gain, we must first discuss Entropy, which was introduced by Shannon(1948).", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "625", "lang": "en", "time_to_read": 3, "top_image_url": "https://miro.medium.com/max/1200/0*i_v4XULShj9hU4nV", "tags": {"info-theory": {"item_id": "2994386236", "tag": "info-theory"}, "machine-learning": {"item_id": "2994386236", "tag": "machine-learning"}}, "authors": {"144628892": {"item_id": "2994386236", "author_id": "144628892", "name": "Steven Loaiza", "url": "https://stevenloaiza.medium.com"}}, "image": {"item_id": "2994386236", "src": "https://miro.medium.com/fit/c/56/56/1*RCIt7nUAdt3a6u_bxFRQcA.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2994386236", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*RCIt7nUAdt3a6u_bxFRQcA.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2994386236", "image_id": "2", "src": "https://miro.medium.com/max/1000/0*i_v4XULShj9hU4nV", "width": "500", "height": "750", "credit": "Jake Blucker on Unsplash", "caption": ""}, "3": {"item_id": "2994386236", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*ip-m43GK4I8n6NfbQqdnWQ.png", "width": "700", "height": "238", "credit": "", "caption": "x: The number of bits, n: set of values"}, "4": {"item_id": "2994386236", "image_id": "4", "src": "https://miro.medium.com/max/1400/0*0rmec8ULg69JyDJ4", "width": "700", "height": "467", "credit": "Markus Spiske on Unsplash", "caption": ""}, "5": {"item_id": "2994386236", "image_id": "5", "src": "https://miro.medium.com/max/542/1*wmU4yQtCzC67lVJy7gU9EA.png", "width": "271", "height": "48", "credit": "", "caption": ""}, "6": {"item_id": "2994386236", "image_id": "6", "src": "https://miro.medium.com/max/940/1*ixgJCZRgq9v0AOKJro-3Iw.png", "width": "470", "height": "53", "credit": "", "caption": ""}, "7": {"item_id": "2994386236", "image_id": "7", "src": "https://miro.medium.com/max/1400/0*HtDq6-3uWk9ZzIaf", "width": "700", "height": "466", "credit": "Alex on Unsplash", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 242}, "3190167007": {"item_id": "3190167007", "resolved_id": "3190167080", "given_url": "https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1?source=rss----7f60cf5620c9---4", "given_title": "Essential Math for Data Science: Information Theory", "favorite": "0", "status": "1", "time_added": "1606922045", "time_updated": "1624293719", "time_read": "1608290517", "time_favorited": "0", "sort_id": 5, "resolved_title": "Essential Math for Data Science: Information Theory", "resolved_url": "https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1", "excerpt": "The field of information theory studies the quantification of information in signals. In the context of machine learning, some of these concepts are used to characterize or compare probability distributions.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2352", "lang": "en", "time_to_read": 11, "top_image_url": "https://miro.medium.com/max/967/1*7BxImLe30p6sT_UqwcDgNw.jpeg", "tags": {"info-theory": {"item_id": "3190167007", "tag": "info-theory"}}, "authors": {"142507914": {"item_id": "3190167007", "author_id": "142507914", "name": "Hadrien Jean", "url": "https://medium.com/@hadrienj"}}, "image": {"item_id": "3190167007", "src": "https://miro.medium.com/fit/c/56/56/1*VdabcdRMZNGkHMHxuVpsBg.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3190167007", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*VdabcdRMZNGkHMHxuVpsBg.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3190167007", "image_id": "2", "src": "https://miro.medium.com/max/1934/1*7BxImLe30p6sT_UqwcDgNw.jpeg", "width": "967", "height": "647", "credit": "Image by author", "caption": ""}, "3": {"item_id": "3190167007", "image_id": "3", "src": "https://miro.medium.com/max/794/1*dxHEfafLI6MZCyME7PFJBA.png", "width": "397", "height": "39", "credit": "", "caption": ""}, "4": {"item_id": "3190167007", "image_id": "4", "src": "https://miro.medium.com/max/3676/1*U09CDz6Y9YD-KlF4-7P3CA.png", "width": "1838", "height": "83", "credit": "", "caption": ""}, "5": {"item_id": "3190167007", "image_id": "5", "src": "https://miro.medium.com/max/760/1*OalAV44ABOzG-odezggMTQ.png", "width": "380", "height": "265", "credit": "", "caption": ""}, "6": {"item_id": "3190167007", "image_id": "6", "src": "https://miro.medium.com/max/592/1*8-WFWxM7BAdGVKFCA1e4Ng.png", "width": "296", "height": "39", "credit": "", "caption": ""}, "7": {"item_id": "3190167007", "image_id": "7", "src": "https://miro.medium.com/max/728/1*KvbWke75kXBsRZm5Z4bomg.png", "width": "364", "height": "35", "credit": "", "caption": ""}, "8": {"item_id": "3190167007", "image_id": "8", "src": "https://miro.medium.com/max/940/1*I4yFngAwIPDLUinBD7OLWw.png", "width": "470", "height": "39", "credit": "", "caption": ""}, "9": {"item_id": "3190167007", "image_id": "9", "src": "https://miro.medium.com/max/940/1*G44c3dStHJnetToJS6b20Q.png", "width": "470", "height": "39", "credit": "", "caption": ""}, "10": {"item_id": "3190167007", "image_id": "10", "src": "https://miro.medium.com/max/2008/1*i1azlXD9WXnhldhQVQUMGw.png", "width": "1004", "height": "39", "credit": "", "caption": ""}, "11": {"item_id": "3190167007", "image_id": "11", "src": "https://miro.medium.com/max/804/1*JZATlWgKiHiDg5xnTmnRGA.png", "width": "402", "height": "103", "credit": "", "caption": ""}, "12": {"item_id": "3190167007", "image_id": "12", "src": "https://miro.medium.com/max/1468/1*NPkskH59N8QA3_RG_dLHUQ.png", "width": "734", "height": "80", "credit": "", "caption": ""}, "13": {"item_id": "3190167007", "image_id": "13", "src": "https://miro.medium.com/max/4068/1*mE4hbdBZv3l3yUF5hCmZ2g.png", "width": "2034", "height": "1972", "credit": "", "caption": ""}, "14": {"item_id": "3190167007", "image_id": "14", "src": "https://miro.medium.com/max/778/1*oM7C1gL0ZL8xEww5qS96_A.png", "width": "389", "height": "281", "credit": "", "caption": ""}, "15": {"item_id": "3190167007", "image_id": "15", "src": "https://miro.medium.com/max/1168/1*9dRFSVvkAKO2YhKnHouVwg.png", "width": "584", "height": "85", "credit": "", "caption": ""}, "16": {"item_id": "3190167007", "image_id": "16", "src": "https://miro.medium.com/max/1222/1*VcDtnBA9b9CJZYf6OyVhQw.png", "width": "611", "height": "80", "credit": "", "caption": ""}, "17": {"item_id": "3190167007", "image_id": "17", "src": "https://miro.medium.com/max/8054/1*Rgm0BB7BbBt7rZ9yQUbVvw.png", "width": "4027", "height": "1958", "credit": "", "caption": ""}, "18": {"item_id": "3190167007", "image_id": "18", "src": "https://miro.medium.com/max/14750/1*fJ949erEC0v5qOQoJUpVWw.png", "width": "7375", "height": "3493", "credit": "", "caption": ""}, "19": {"item_id": "3190167007", "image_id": "19", "src": "https://miro.medium.com/max/1848/1*c4zpFAMwxRxqLCZuExLAEA.png", "width": "924", "height": "295", "credit": "", "caption": ""}, "20": {"item_id": "3190167007", "image_id": "20", "src": "https://miro.medium.com/max/1798/1*GcB9h_ywXDcTT1Sifls8Nw.png", "width": "899", "height": "305", "credit": "", "caption": ""}, "21": {"item_id": "3190167007", "image_id": "21", "src": "https://miro.medium.com/max/1548/1*7ocTzDLAWHfe6cC1h5weDA.png", "width": "774", "height": "195", "credit": "", "caption": ""}, "22": {"item_id": "3190167007", "image_id": "22", "src": "https://miro.medium.com/max/1348/1*gxvPh0godh7gveeKQPBFKg.png", "width": "674", "height": "39", "credit": "", "caption": ""}, "23": {"item_id": "3190167007", "image_id": "23", "src": "https://miro.medium.com/max/2144/1*H9mVGlFIFQM6zMC0KIUBLw.png", "width": "1072", "height": "356", "credit": "", "caption": ""}, "24": {"item_id": "3190167007", "image_id": "24", "src": "https://miro.medium.com/max/400/1*9S5qA5aYzopRFt9kIu2l4w.jpeg", "width": "200", "height": "250", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 910}, "3817135498": {"item_id": "3817135498", "resolved_id": "3817135513", "given_url": "https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=rss----7f60cf5620c9---4", "given_title": "How to Understand and Use the Jensen-Shannon Divergence", "favorite": "0", "status": "1", "time_added": "1677759649", "time_updated": "1677776117", "time_read": "1677776049", "time_favorited": "0", "sort_id": 6, "resolved_title": "How to Understand and Use Jensen Shannon Divergence", "resolved_url": "https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6", "excerpt": "In machine learning systems, drift monitoring can be critical to delivering quality ML. Some common use cases for drift analysis in production ML systems include:", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "1905", "lang": "en", "time_to_read": 9, "top_image_url": "https://miro.medium.com/max/1020/1*iXqnpz9Rlxl2X83fAYxyLg.jpeg", "tags": {"info-theory": {"item_id": "3817135498", "tag": "info-theory"}, "machine-learning": {"item_id": "3817135498", "tag": "machine-learning"}, "prob-stats": {"item_id": "3817135498", "tag": "prob-stats"}}, "authors": {"150985940": {"item_id": "3817135498", "author_id": "150985940", "name": "Aparna Dhinakaran", "url": "https://aparnadhinak.medium.com"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 737}, "3353515542": {"item_id": "3353515542", "resolved_id": "3353509676", "given_url": "https://towardsdatascience.com/information-theory-a-gentle-introduction-6abaf99835ac?source=rss----7f60cf5620c9---4", "given_title": "Information Theory: A Gentle Introduction", "favorite": "0", "status": "1", "time_added": "1623381441", "time_updated": "1706233547", "time_read": "1623454898", "time_favorited": "0", "sort_id": 7, "resolved_title": "Information Theory: A Gentle Introduction", "resolved_url": "https://towardsdatascience.com/information-theory-a-gentle-introduction-6abaf99835ac", "excerpt": "This is the first in a series of articles about Information Theory and its relationship to data driven enterprises and strategy. While there will be some equations in each section, they can largely be ignored for those less interested in the details and more in the implications.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2667", "lang": "en", "time_to_read": 12, "tags": {"algorithms-math": {"item_id": "3353515542", "tag": "algorithms-math"}, "info-theory": {"item_id": "3353515542", "tag": "info-theory"}}, "authors": {"149985227": {"item_id": "3353515542", "author_id": "149985227", "name": "Douglas Hamilton", "url": "https://dougmh.medium.com"}}, "image": {"item_id": "3353515542", "src": "https://miro.medium.com/fit/c/56/56/1*dmbNkD5D-u45r44go_cf0g.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3353515542", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*dmbNkD5D-u45r44go_cf0g.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3353515542", "image_id": "2", "src": "https://miro.medium.com/max/1114/1*dBbkCNO96NSNWxWX9IeM9A.jpeg", "width": "557", "height": "118", "credit": "H", "caption": "Entropy"}, "3": {"item_id": "3353515542", "image_id": "3", "src": "https://miro.medium.com/max/278/1*sRhNlLMwOH28lq-mKLHuUw.jpeg", "width": "139", "height": "95", "credit": "", "caption": "Mapping a Bit to a Coin Toss"}, "4": {"item_id": "3353515542", "image_id": "4", "src": "https://miro.medium.com/max/490/1*gvyLAlWsWJiOx99Qa4ojng.jpeg", "width": "245", "height": "350", "credit": "", "caption": "A 3 bit dice roll has additional capacity; we null out 2 possible states to account for this"}, "5": {"item_id": "3353515542", "image_id": "5", "src": "https://miro.medium.com/max/306/1*6agrqn6kfdb5qKswyZbutA.jpeg", "width": "153", "height": "259", "credit": "", "caption": "A 2 bit dice roll destroys information"}, "6": {"item_id": "3353515542", "image_id": "6", "src": "https://miro.medium.com/max/1344/1*kDJpFOk_HHTjlHNQmH8qbA.jpeg", "width": "672", "height": "149", "credit": "", "caption": "Conditional Entropy (H(X|Y) is entropy with more information; all else being equal it is always less than uninformed entropy"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1032}, "3231525362": {"item_id": "3231525362", "resolved_id": "3231525381", "given_url": "https://towardsdatascience.com/link-prediction-and-information-theory-a-tutorial-a67ecc73e7f9?source=rss----7f60cf5620c9---4", "given_title": "Link Prediction and Information Theory: A Tutorial", "favorite": "0", "status": "1", "time_added": "1610851689", "time_updated": "1624293719", "time_read": "1610897584", "time_favorited": "0", "sort_id": 8, "resolved_title": "Link Prediction and Information Theory: A Tutorial", "resolved_url": "https://towardsdatascience.com/link-prediction-and-information-theory-a-tutorial-a67ecc73e7f9", "excerpt": "During my literature review, I stumbled upon an information-theoretic framework to analyse the link prediction problem (Tan et al. (2014), Kumar and Sharma (2020)). For an overview of what link prediction is, read my previous article here. The basic idea is to predict unseen edges in a graph.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "932", "lang": "en", "time_to_read": 4, "top_image_url": "https://miro.medium.com/max/1200/1*VK7jOqx6Vogs_-liMWe5cg.png", "tags": {"graphs": {"item_id": "3231525362", "tag": "graphs"}, "info-theory": {"item_id": "3231525362", "tag": "info-theory"}, "machine-learning": {"item_id": "3231525362", "tag": "machine-learning"}}, "authors": {"145559445": {"item_id": "3231525362", "author_id": "145559445", "name": "Edward Elson Kosasih", "url": "https://eekosasih.medium.com"}}, "image": {"item_id": "3231525362", "src": "https://miro.medium.com/fit/c/56/56/1*RKuCzHUp7Rzo9t38zFSZkA.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3231525362", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*RKuCzHUp7Rzo9t38zFSZkA.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3231525362", "image_id": "2", "src": "https://miro.medium.com/max/3840/1*VK7jOqx6Vogs_-liMWe5cg.png", "width": "1920", "height": "639", "credit": "", "caption": "Image by Gerd Altmann from Pixabay"}, "3": {"item_id": "3231525362", "image_id": "3", "src": "https://miro.medium.com/max/566/1*QBUgUvMfTphz8U3iPHSRTw.png", "width": "283", "height": "61", "credit": "", "caption": ""}, "4": {"item_id": "3231525362", "image_id": "4", "src": "https://miro.medium.com/max/2092/1*ksVua17a8GkBmo1q6lV08w.png", "width": "1046", "height": "106", "credit": "", "caption": ""}, "5": {"item_id": "3231525362", "image_id": "5", "src": "https://miro.medium.com/max/570/1*osV4dnbK-OjgmE4NH4GW8Q.png", "width": "285", "height": "102", "credit": "", "caption": ""}, "6": {"item_id": "3231525362", "image_id": "6", "src": "https://miro.medium.com/max/660/1*7nd7cOiGXZVSrX14AeMUQQ.png", "width": "330", "height": "93", "credit": "", "caption": ""}, "7": {"item_id": "3231525362", "image_id": "7", "src": "https://miro.medium.com/max/2398/1*5XGPPIODLnhy9ncYn8x-LA.png", "width": "1199", "height": "105", "credit": "", "caption": ""}, "8": {"item_id": "3231525362", "image_id": "8", "src": "https://miro.medium.com/max/930/1*xJrKjGBr6LQpe3ssURkhYw.png", "width": "465", "height": "305", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 361}, "3123126610": {"item_id": "3123126610", "resolved_id": "3123126630", "given_url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873?source=rss----7f60cf5620c9---4", "given_title": "Why and How to use Cross Entropy", "favorite": "0", "status": "1", "time_added": "1601113895", "time_updated": "1624293719", "time_read": "1604362161", "time_favorited": "0", "sort_id": 9, "resolved_title": "Why and How to use Cross Entropy", "resolved_url": "https://towardsdatascience.com/why-and-how-to-use-cross-entropy-4e983cbdd873", "excerpt": "This post discusses why logistic regression necessarily uses a different loss function than linear regression. First, the simple yet inefficient way to solve logistic regression will be presented, then the slightly less simple but much more efficient way will be explained and compared.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1290", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/844/1*a4rjEj993DCAAlGHW732xg.png", "tags": {"info-theory": {"item_id": "3123126610", "tag": "info-theory"}, "machine-learning": {"item_id": "3123126610", "tag": "machine-learning"}}, "authors": {"141744102": {"item_id": "3123126610", "author_id": "141744102", "name": "Will Arliss", "url": "https://willarliss.medium.com"}}, "image": {"item_id": "3123126610", "src": "https://miro.medium.com/max/520/1*AYSlinHaVBk9BrCHRikcpw.png", "width": "260", "height": "66"}, "images": {"1": {"item_id": "3123126610", "image_id": "1", "src": "https://miro.medium.com/max/520/1*AYSlinHaVBk9BrCHRikcpw.png", "width": "260", "height": "66", "credit": "", "caption": "Equation 1: Probability of class membership using the sigmoid function"}, "2": {"item_id": "3123126610", "image_id": "2", "src": "https://miro.medium.com/max/560/1*wZfUupzcjrSCXDnQCOlazA.png", "width": "280", "height": "86", "credit": "", "caption": "Equation 2: Mean squared error loss function"}, "3": {"item_id": "3123126610", "image_id": "3", "src": "https://miro.medium.com/max/1100/1*npPdqj1VcQpV4ee0rfgntg.png", "width": "550", "height": "82", "credit": "", "caption": "Equation 3: Binary cross entropy / log loss"}, "4": {"item_id": "3123126610", "image_id": "4", "src": "https://miro.medium.com/max/338/1*y0rjA0O0_F9w0fLN5wIhrA.png", "width": "169", "height": "66", "credit": "", "caption": "Equation 4: The sigmoid / logit function"}, "5": {"item_id": "3123126610", "image_id": "5", "src": "https://miro.medium.com/max/1330/1*trLN-NWDvkUOJRinWLKoEg.png", "width": "665", "height": "405", "credit": "", "caption": ""}, "6": {"item_id": "3123126610", "image_id": "6", "src": "https://miro.medium.com/max/1328/1*FllvYbZkqe91ogUWYFz-Rg.png", "width": "664", "height": "405", "credit": "", "caption": "Figure 1: Log loss is worked out for two incorrectly identified observations. The log loss equation has been simplified from equation 3"}, "7": {"item_id": "3123126610", "image_id": "7", "src": "https://miro.medium.com/max/890/1*haNwCLTR18ANhgmTHkdrwQ.png", "width": "445", "height": "324", "credit": "", "caption": "Figure 2: Synthetic data for classification"}, "8": {"item_id": "3123126610", "image_id": "8", "src": "https://miro.medium.com/max/926/1*MAdslNS1pQ_G9soqk-X4OA.png", "width": "463", "height": "45", "credit": "", "caption": ""}, "9": {"item_id": "3123126610", "image_id": "9", "src": "https://miro.medium.com/max/868/1*8kzkgxPL335riLFkEPlVuQ.png", "width": "434", "height": "333", "credit": "", "caption": "Figure 3: Log loss plotted during gradient descent on the binary cross entropy loss function"}, "10": {"item_id": "3123126610", "image_id": "10", "src": "https://miro.medium.com/max/856/1*0TZMEwbhCBIFRODzRYyV6w.png", "width": "428", "height": "333", "credit": "", "caption": "Figure 4: Log loss and mean squared error loss plotted during gradient descent on the mean squared error loss function"}, "11": {"item_id": "3123126610", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*a4rjEj993DCAAlGHW732xg.png", "width": "700", "height": "265", "credit": "", "caption": "Figure 5: Decision boundaries for the correct loss function and the incorrect loss function"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 499}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709934689}