{"status": 1, "complete": 1, "list": {"3866468993": {"item_id": "3866468993", "resolved_id": "3866431202", "given_url": "https://thesequence.substack.com/p/edge-291-reinforcement-learning-with?utm_medium=email", "given_title": "", "favorite": "0", "status": "1", "time_added": "1684240747", "time_updated": "1706821793", "time_read": "1684404949", "time_favorited": "0", "sort_id": 0, "resolved_title": "Edge 291: Reinforcement Learning with Human Feedback", "resolved_url": "https://thesequence.substack.com/p/edge-291-reinforcement-learning-with", "excerpt": "The RLHF paper. The transformer reinforcement learning framework.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "100", "lang": "en", "top_image_url": "https://substackcdn.com/image/fetch/w_1200,h_600,c_limit,f_jpg,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1aaa4cc-10d6-4ada-bba0-f1f2f0793427_1024x1024.png", "tags": {"chatgpt": {"item_id": "3866468993", "tag": "chatgpt"}, "llms": {"item_id": "3866468993", "tag": "llms"}, "reinforcement-learning": {"item_id": "3866468993", "tag": "reinforcement-learning"}, "transformers": {"item_id": "3866468993", "tag": "transformers"}}, "authors": {"86252": {"item_id": "3866468993", "author_id": "86252", "name": "Jesus Rodriguez", "url": ""}}, "listen_duration_estimate": 39}, "3840872915": {"item_id": "3840872915", "resolved_id": "3840872915", "given_url": "https://huggingface.co/blog/stackllama", "given_title": "", "favorite": "0", "status": "1", "time_added": "1680921782", "time_updated": "1681092987", "time_read": "1680964607", "time_favorited": "0", "sort_id": 1, "resolved_title": "StackLLaMA: A hands-on guide to train LLaMA with RLHF", "resolved_url": "https://huggingface.co/blog/stackllama", "excerpt": "Models such as ChatGPT, GPT-4, and Claude are powerful language models that have been fine-tuned using a method called Reinforcement Learning from Human Feedback (RLHF) to be better aligned with how we expect them to behave and would like to use them. From InstructGPT paper: Ouyang, Long, et al.", "is_article": "1", "is_index": "1", "has_video": "0", "has_image": "1", "word_count": "3112", "lang": "en", "time_to_read": 14, "top_image_url": "https://huggingface.co/blog/assets/138_stackllama/thumbnail.png", "tags": {"reinforcement-learning": {"item_id": "3840872915", "tag": "reinforcement-learning"}}, "image": {"item_id": "3840872915", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/instructGPT.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3840872915", "image_id": "1", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/instructGPT.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3840872915", "image_id": "2", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_ddp.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3840872915", "image_id": "3", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_preprocessing-clm.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3840872915", "image_id": "4", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3840872915", "image_id": "5", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/wandb_reward.png", "width": "0", "height": "0", "credit": "", "caption": "Per batch reward at each step during training. The model’s performance plateaus after around 1000 steps."}, "6": {"item_id": "3840872915", "image_id": "6", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/llama_prompt.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "3840872915", "image_id": "7", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_high_reward.png", "width": "0", "height": "0", "credit": "", "caption": "Wow this run must be great, look at that sweet, sweet, reward!"}, "8": {"item_id": "3840872915", "image_id": "8", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_neg_kl.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "3840872915", "image_id": "9", "src": "https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_loss_spikes.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1205}, "3839421215": {"item_id": "3839421215", "resolved_id": "3839421215", "given_url": "https://spectrum.ieee.org/chip-design-controversy", "given_title": "", "favorite": "0", "status": "1", "time_added": "1680782863", "time_updated": "1680786098", "time_read": "1680786098", "time_favorited": "0", "sort_id": 2, "resolved_title": "Ending an Ugly Chapter in Chip Design", "resolved_url": "https://spectrum.ieee.org/chip-design-controversy", "excerpt": "Discussions at chip design conferences rarely get heated. But a year ago at the International Symposium on Physical Design, things got out of hand. It was described by observers as a “trainwreck” and an “ambush”.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "2228", "lang": "en", "time_to_read": 10, "amp_url": "https://spectrum.ieee.org/amp/chip-design-controversy-2659708634", "top_image_url": "https://spectrum.ieee.org/media-library/six-squares-contain-variously-sized-rectangles-of-four-colors-with-differently-colored-blobs-filling-in-gaps-between-the-rectang.jpg?id=33402790&width=1200&height=600&coordinates=0%2C155%2C0%2C155", "tags": {"chip-design": {"item_id": "3839421215", "tag": "chip-design"}, "reinforcement-learning": {"item_id": "3839421215", "tag": "reinforcement-learning"}, "semiconductors": {"item_id": "3839421215", "tag": "semiconductors"}}, "domain_metadata": {"name": "IEEE", "logo": "https://logo.clearbit.com/ieee.org?size=800", "greyscale_logo": "https://logo.clearbit.com/ieee.org?size=800&greyscale=true"}, "listen_duration_estimate": 862}, "2863292128": {"item_id": "2863292128", "resolved_id": "2723338441", "given_url": "https://www.datasciencecentral.com/profiles/blogs/introduction-to-various-reinforcement-learning-algorithms-part-i?fbclid=IwAR2zO-0Mox6vBQFaLtYgDyT1aVKqlIAQWiv92_bFYbvIfWzU0Rql0N7fTOI", "given_title": "", "favorite": "0", "status": "1", "time_added": "1579896212", "time_updated": "1612385367", "time_read": "1582142665", "time_favorited": "0", "sort_id": 3, "resolved_title": "Introduction to Various Reinforcement Learning Algorithms", "resolved_url": "https://www.datasciencecentral.com/profiles/blogs/introduction-to-various-reinforcement-learning-algorithms-part-i", "excerpt": "This article was written by Steeve Huang. Reinforcement Learning (RL) refers to a kind of Machine Learning method in which the agent receives a delayed reward in the next time step to evaluate its previous action. It was mostly used in games (e.g.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "622", "lang": "en", "time_to_read": 3, "top_image_url": "https://miro.medium.com/max/1552/1*PkQDd3IdcDXI4vUVwMAqKA.png?profile=RESIZE_710x", "tags": {"reinforcement-learning": {"item_id": "2863292128", "tag": "reinforcement-learning"}}, "authors": {"97841554": {"item_id": "2863292128", "author_id": "97841554", "name": "Andrea Manero-Bastin", "url": "https://www.datasciencecentral.com/profile/AndreaManeroBastin"}}, "image": {"item_id": "2863292128", "src": "https://miro.medium.com/max/1552/1*PkQDd3IdcDXI4vUVwMAqKA.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2863292128", "image_id": "1", "src": "https://miro.medium.com/max/1552/1*PkQDd3IdcDXI4vUVwMAqKA.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Data Science Central", "logo": "https://logo.clearbit.com/datasciencecentral.com?size=800", "greyscale_logo": "https://logo.clearbit.com/datasciencecentral.com?size=800&greyscale=true"}, "listen_duration_estimate": 241}, "3170977506": {"item_id": "3170977506", "resolved_id": "3170977506", "given_url": "https://rltheorybook.github.io/rltheorybook_AJKS.pdf", "given_title": "", "favorite": "0", "status": "1", "time_added": "1624215057", "time_updated": "1638708525", "time_read": "1624292161", "time_favorited": "0", "sort_id": 4, "resolved_title": "", "resolved_url": "https://rltheorybook.github.io/rltheorybook_AJKS.pdf", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"deep-learning": {"item_id": "3170977506", "tag": "deep-learning"}, "reinforcement-learning": {"item_id": "3170977506", "tag": "reinforcement-learning"}}, "listen_duration_estimate": 0}, "3800647893": {"item_id": "3800647893", "resolved_id": "3797920240", "given_url": "https://arxiv.org/abs/2301.03881?utm_source=substack&utm_medium=email", "given_title": "", "favorite": "0", "status": "1", "time_added": "1675591867", "time_updated": "1679781251", "time_read": "1675948854", "time_favorited": "0", "sort_id": 5, "resolved_title": "Title:Why People Skip Music? On Predicting Music Skips using Deep Reinforcement Learning", "resolved_url": "https://arxiv.org/abs/2301.03881v1", "excerpt": "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "77", "lang": "en", "top_image_url": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png", "tags": {"arxiv": {"item_id": "3800647893", "tag": "arxiv"}, "music": {"item_id": "3800647893", "tag": "music"}, "reinforcement-learning": {"item_id": "3800647893", "tag": "reinforcement-learning"}}, "authors": {"177703994": {"item_id": "3800647893", "author_id": "177703994", "name": "cs", "url": "https://arxiv.org/abs/2301.03881?context=cs"}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 30}, "3753103267": {"item_id": "3753103267", "resolved_id": "3753103267", "given_url": "https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e", "given_title": "", "favorite": "0", "status": "1", "time_added": "1669552844", "time_updated": "1669667175", "time_read": "1669667174", "time_favorited": "0", "sort_id": 6, "resolved_title": "6 Reinforcement Learning Algorithms Explained", "resolved_url": "https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e", "excerpt": "Machine Learning (ML) is split into three branches: Supervised Learning, Unsupervised Learning, and Reinforcement Learning.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "3280", "lang": "en", "time_to_read": 15, "top_image_url": "https://miro.medium.com/max/1200/0*bo5XH0yQcOvHpKWh", "tags": {"deep-learning": {"item_id": "3753103267", "tag": "deep-learning"}, "reinforcement-learning": {"item_id": "3753103267", "tag": "reinforcement-learning"}}, "authors": {"162188001": {"item_id": "3753103267", "author_id": "162188001", "name": "Kay Jan Wong", "url": "https://kayjanwong.medium.com"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1270}, "3661604060": {"item_id": "3661604060", "resolved_id": "3661604060", "given_url": "https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88", "given_title": "", "favorite": "0", "status": "1", "time_added": "1657997156", "time_updated": "1658180459", "time_read": "1658180458", "time_favorited": "0", "sort_id": 7, "resolved_title": "Hands on introduction to reinforcement learning in Python", "resolved_url": "https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88", "excerpt": "One of the biggest barriers to traditional machine learning is that most supervised and unsupervised machine learning algorithms need huge amounts of data to be useful in real world use cases. Even then, the AI is unable to learn as it goes without human supervision and feedback.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2523", "lang": "en", "time_to_read": 11, "top_image_url": "https://miro.medium.com/max/1200/0*gMgU8hLuZL_6MSHs", "tags": {"python": {"item_id": "3661604060", "tag": "python"}, "reinforcement-learning": {"item_id": "3661604060", "tag": "reinforcement-learning"}}, "authors": {"169663130": {"item_id": "3661604060", "author_id": "169663130", "name": "Neha Desaraju", "url": "https://medium.com/@nehadesaraju"}}, "image": {"item_id": "3661604060", "src": "https://miro.medium.com/max/1400/0*gMgU8hLuZL_6MSHs", "width": "700", "height": "525"}, "images": {"1": {"item_id": "3661604060", "image_id": "1", "src": "https://miro.medium.com/max/1400/0*gMgU8hLuZL_6MSHs", "width": "700", "height": "525", "credit": "Brett Jordan on Unsplash", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 977}, "3493082102": {"item_id": "3493082102", "resolved_id": "3493082102", "given_url": "https://github.com/bjpcjp/scikit-and-tensorflow-workbooks/blob/master/ch16-reinforcement-learning.ipynb", "given_title": "", "favorite": "0", "status": "1", "time_added": "1638239964", "time_updated": "1642376784", "time_read": "1642373444", "time_favorited": "0", "sort_id": 8, "resolved_title": "", "resolved_url": "https://github.com/bjpcjp/scikit-and-tensorflow-workbooks/blob/master/ch16-reinforcement-learning.ipynb", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"reinforcement-learning": {"item_id": "3493082102", "tag": "reinforcement-learning"}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "3651527930": {"item_id": "3651527930", "resolved_id": "3651521154", "given_url": "https://arxiv.org/abs/2206.15378", "given_title": "", "favorite": "0", "status": "1", "time_added": "1657566601", "time_updated": "1657571886", "time_read": "1657571886", "time_favorited": "0", "sort_id": 9, "resolved_title": "Title:Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning", "resolved_url": "https://arxiv.org/abs/2206.15378v1", "excerpt": "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "77", "lang": "en", "top_image_url": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png", "tags": {"deep-learning": {"item_id": "3651527930", "tag": "deep-learning"}, "game-theory": {"item_id": "3651527930", "tag": "game-theory"}, "games": {"item_id": "3651527930", "tag": "games"}, "reinforcement-learning": {"item_id": "3651527930", "tag": "reinforcement-learning"}, "stratego": {"item_id": "3651527930", "tag": "stratego"}}, "authors": {"65395181": {"item_id": "3651527930", "author_id": "65395181", "name": "cs cs.GT cs.MA", "url": ""}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 30}, "1482524177": {"item_id": "1482524177", "resolved_id": "1482524177", "given_url": "https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning", "given_title": "Deep Learning Research Review Week 2: Reinforcement Learning – Adit Deshpan", "favorite": "0", "status": "1", "time_added": "1518132859", "time_updated": "1612385367", "time_read": "1528501310", "time_favorited": "0", "sort_id": 10, "resolved_title": "Deep Learning Research Review Week 2: Reinforcement Learning", "resolved_url": "https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning", "excerpt": "This is the 2nd installment of a new series called Deep Learning Research Review. Every couple weeks or so, I’ll be summarizing and explaining research papers in specific subfields of deep learning. This week focuses on Reinforcement Learning. Last time was Generative Adversarial Networks ICYMI", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3549", "lang": "en", "time_to_read": 16, "top_image_url": "https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning?utm_content=buffer36669&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer", "tags": {"reinforcement-learning": {"item_id": "1482524177", "tag": "reinforcement-learning"}}, "authors": {"53325162": {"item_id": "1482524177", "author_id": "53325162", "name": "Adit Deshpande", "url": ""}}, "image": {"item_id": "1482524177", "src": "https://adeshpande3.github.io/assets/Cover6th.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "1482524177", "image_id": "1", "src": "https://adeshpande3.github.io/assets/Cover6th.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "1482524177", "image_id": "2", "src": "https://adeshpande3.github.io/assets/IRL1.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "1482524177", "image_id": "3", "src": "https://adeshpande3.github.io/assets/IRL2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "1482524177", "image_id": "4", "src": "https://adeshpande3.github.io/assets/IRL3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "1482524177", "image_id": "5", "src": "https://adeshpande3.github.io/assets/IRL4.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "1482524177", "image_id": "6", "src": "https://adeshpande3.github.io/assets/IRL5.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "1482524177", "image_id": "7", "src": "https://adeshpande3.github.io/assets/IRL6.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "1482524177", "image_id": "8", "src": "https://adeshpande3.github.io/assets/IRL7.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "1482524177", "image_id": "9", "src": "https://adeshpande3.github.io/assets/IRL8.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "1482524177", "image_id": "10", "src": "https://adeshpande3.github.io/assets/IRL9.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "1482524177", "image_id": "11", "src": "https://adeshpande3.github.io/assets/IRL10.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "1482524177", "image_id": "12", "src": "https://adeshpande3.github.io/assets/IRL11.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "1482524177", "image_id": "13", "src": "https://adeshpande3.github.io/assets/IRL12.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "1482524177", "image_id": "14", "src": "https://adeshpande3.github.io/assets/IRL13.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "15": {"item_id": "1482524177", "image_id": "15", "src": "https://adeshpande3.github.io/assets/IRL14.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "1482524177", "image_id": "16", "src": "https://adeshpande3.github.io/assets/IRL15.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "17": {"item_id": "1482524177", "image_id": "17", "src": "https://adeshpande3.github.io/assets/IRL16.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "18": {"item_id": "1482524177", "image_id": "18", "src": "https://adeshpande3.github.io/assets/IRL17.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1374}, "1717821340": {"item_id": "1717821340", "resolved_id": "1717821340", "given_url": "https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124", "given_title": "Deep Reinforcement Learning Demystified (Episode 0)", "favorite": "0", "status": "1", "time_added": "1497040711", "time_updated": "1612385367", "time_read": "1514398169", "time_favorited": "0", "sort_id": 11, "resolved_title": "Deep Reinforcement Learning Demystified (Episode 0)", "resolved_url": "https://medium.com/@m.alzantot/deep-reinforcement-learning-demystified-episode-0-2198c05a6124", "excerpt": "I recently became interested in learning more about deep reinforcement learning. Recently, big news headlines were made as deep reinforcement learning was used to build a compter program that mastered different Atari games, and the AlphaGo program could beat the a top human of the Go game.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1842", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/max/843/1*kPHMuxV2jvEdCrDoTtEqBg.jpeg", "tags": {"reinforcement-learning": {"item_id": "1717821340", "tag": "reinforcement-learning"}}, "authors": {"67532774": {"item_id": "1717821340", "author_id": "67532774", "name": "Moustafa Alzantot", "url": "https://medium.com/@m.alzantot"}}, "image": {"item_id": "1717821340", "src": "https://miro.medium.com/fit/c/56/56/0*vbsTKQhLiZPMFy9f.", "width": "28", "height": "28"}, "images": {"1": {"item_id": "1717821340", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/0*vbsTKQhLiZPMFy9f.", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "1717821340", "image_id": "2", "src": "https://miro.medium.com/max/1600/1*kPHMuxV2jvEdCrDoTtEqBg.jpeg", "width": "700", "height": "395", "credit": "", "caption": ""}, "3": {"item_id": "1717821340", "image_id": "3", "src": "https://miro.medium.com/max/1600/1*Z2yMvuQ1-t5Ol1ac_W4dOQ.png", "width": "700", "height": "391", "credit": "", "caption": "Source: Nervana Deep Reinforcement learning with OpenAI gym."}}, "domain_metadata": {"name": "Medium", "logo": "https://logo.clearbit.com/medium.com?size=800", "greyscale_logo": "https://logo.clearbit.com/medium.com?size=800&greyscale=true"}, "listen_duration_estimate": 713}, "3852138745": {"item_id": "3852138745", "resolved_id": "3852138745", "given_url": "https://venturebeat.com/ai/how-reinforcement-learning-with-human-feedback-is-unlocking-the-power-of-generative-ai/", "given_title": "How reinforcement learning with human feedback is unlocking the power of ge", "favorite": "0", "status": "1", "time_added": "1682267298", "time_updated": "1682439483", "time_read": "1682439482", "time_favorited": "0", "sort_id": 12, "resolved_title": "How reinforcement learning with human feedback is unlocking the power of generative AI", "resolved_url": "https://venturebeat.com/ai/how-reinforcement-learning-with-human-feedback-is-unlocking-the-power-of-generative-ai/", "excerpt": "Join top executives in San Francisco on July 11-12, to hear how leaders are integrating and optimizing AI investments for success. Learn More", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1224", "lang": "en", "time_to_read": 6, "amp_url": "https://venturebeat.com/ai/how-reinforcement-learning-with-human-feedback-is-unlocking-the-power-of-generative-ai/amp/", "top_image_url": "https://venturebeat.com/wp-content/uploads/2023/04/annevb_human_and_artificial_intelligence_working_together_to_un_5b2cbb9e-e380-4bb1-812b-366943b59568.png?w=1200&strip=all", "tags": {"generative": {"item_id": "3852138745", "tag": "generative"}, "reinforcement-learning": {"item_id": "3852138745", "tag": "reinforcement-learning"}}, "authors": {"167233379": {"item_id": "3852138745", "author_id": "167233379", "name": "Sujatha Sagiraju", "url": ""}}, "image": {"item_id": "3852138745", "src": "https://venturebeat.com/wp-content/uploads/2023/04/annevb_human_and_artificial_intelligence_working_together_to_un_5b2cbb9e-e380-4bb1-812b-366943b59568.png?fit=750%2C375&strip=all", "width": "750", "height": "375"}, "images": {"1": {"item_id": "3852138745", "image_id": "1", "src": "https://venturebeat.com/wp-content/uploads/2023/04/annevb_human_and_artificial_intelligence_working_together_to_un_5b2cbb9e-e380-4bb1-812b-366943b59568.png?fit=750%2C375&strip=all", "width": "750", "height": "375", "credit": "", "caption": ""}}, "domain_metadata": {"name": "VentureBeat", "logo": "https://logo.clearbit.com/venturebeat.com?size=800", "greyscale_logo": "https://logo.clearbit.com/venturebeat.com?size=800&greyscale=true"}, "listen_duration_estimate": 474}, "2025952315": {"item_id": "2025952315", "resolved_id": "2025952315", "given_url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287", "given_title": "Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learni", "favorite": "0", "status": "1", "time_added": "1580591910", "time_updated": "1612385367", "time_read": "1582142575", "time_favorited": "0", "sort_id": 13, "resolved_title": "Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)", "resolved_url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287", "excerpt": "Reinforcement Learning (RL) refers to a kind of Machine Learning method in which the agent receives a delayed reward in the next time step to evaluate its previous action. It was mostly used in games (e.g. Atari, Mario), with performance on par with or even exceeding humans.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1743", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/max/1200/1*MiN803ThUoqdCKwklZ8wwA.png", "tags": {"reinforcement-learning": {"item_id": "2025952315", "tag": "reinforcement-learning"}}, "authors": {"124909917": {"item_id": "2025952315", "author_id": "124909917", "name": "Kung-Hsiang, Huang (Steeve)", "url": "https://medium.com/@huangkh19951228"}}, "image": {"item_id": "2025952315", "src": "https://miro.medium.com/fit/c/56/56/1*D36_E45qZHNBD7TvFgiutQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2025952315", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*D36_E45qZHNBD7TvFgiutQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2025952315", "image_id": "2", "src": "https://miro.medium.com/max/8104/1*MiN803ThUoqdCKwklZ8wwA.png", "width": "4052", "height": "2701", "credit": "", "caption": ""}, "3": {"item_id": "2025952315", "image_id": "3", "src": "https://miro.medium.com/max/1816/1*c3pEt4pFk0Mx684DDVsW-w.png", "width": "908", "height": "350", "credit": "https://i.stack.imgur.com/eoeSq.png", "caption": "Reinforcement Learning Illustration"}, "4": {"item_id": "2025952315", "image_id": "4", "src": "https://miro.medium.com/max/466/1*JPn8KZr7yxbQdPcr90qheA.png", "width": "233", "height": "20", "credit": "https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit", "caption": "Bellman Equation"}, "5": {"item_id": "2025952315", "image_id": "5", "src": "https://miro.medium.com/max/632/1*kwLmPgagp0o31nD8PmRjmg.png", "width": "316", "height": "64", "credit": "https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit", "caption": "Bellman Equation In Q-value Form"}, "6": {"item_id": "2025952315", "image_id": "6", "src": "https://miro.medium.com/max/550/1*vA87bBl9ZKfsEa3W--1L6Q.png", "width": "275", "height": "32", "credit": "https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit", "caption": "Optimal Q-value"}, "7": {"item_id": "2025952315", "image_id": "7", "src": "https://miro.medium.com/max/3104/1*PkQDd3IdcDXI4vUVwMAqKA.png", "width": "1552", "height": "910", "credit": "http://blog.csdn.net/songrotek/article/details/51378582", "caption": "Policy Iteration"}, "8": {"item_id": "2025952315", "image_id": "8", "src": "https://miro.medium.com/max/2620/1*XuyjK3QfRqV04y--hYK87w.png", "width": "1310", "height": "1116", "credit": "http://blog.csdn.net/songrotek/article/details/51378582", "caption": "Pseudo Code For Policy Iteration"}, "9": {"item_id": "2025952315", "image_id": "9", "src": "https://miro.medium.com/max/2360/1*rWapNlIa0C1bXV8RgaTEmA.png", "width": "1180", "height": "290", "credit": "http://blog.csdn.net/songrotek/article/details/51378582", "caption": "Optimal Bellman Equation"}, "10": {"item_id": "2025952315", "image_id": "10", "src": "https://miro.medium.com/max/2484/1*S-a_T-k5hXYhinq9758xCQ.png", "width": "1242", "height": "716", "credit": "http://blog.csdn.net/songrotek/article/details/51378582", "caption": "Pseudo Code For Value Iteration"}, "11": {"item_id": "2025952315", "image_id": "11", "src": "https://miro.medium.com/max/1928/1*n9yjEWqBVZ0jw2bff9hRBw.png", "width": "964", "height": "74", "credit": "https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning", "caption": "Q-Learning Update Equation"}, "12": {"item_id": "2025952315", "image_id": "12", "src": "https://miro.medium.com/max/1024/1*B8tGarFYboV9maL93sF45Q.png", "width": "512", "height": "379", "credit": "https://martin-thoma.com/images/2016/07/q-learning.png", "caption": "Q-learning Pseudo Code"}, "13": {"item_id": "2025952315", "image_id": "13", "src": "https://miro.medium.com/max/1852/1*DVtlBC0pNsW6LbDM25y7qw.png", "width": "926", "height": "48", "credit": "https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning", "caption": "SARSA Update Equation"}, "14": {"item_id": "2025952315", "image_id": "14", "src": "https://miro.medium.com/max/1024/1*NdEQk3LeJfkzImOiQij_NA.png", "width": "512", "height": "487", "credit": "https://martin-thoma.com/images/2016/07/sarsa-lambda.png", "caption": "SARSA Pseudo Code"}, "15": {"item_id": "2025952315", "image_id": "15", "src": "https://miro.medium.com/max/2484/1*4antxYinbORGPNUElrzOUA.png", "width": "1242", "height": "728", "credit": "https://zhuanlan.zhihu.com/p/25239682", "caption": "DQN Atari Example"}, "16": {"item_id": "2025952315", "image_id": "16", "src": "https://miro.medium.com/max/1012/1*VcgBin7pa2eERUxjVwvg1Q.png", "width": "506", "height": "64", "credit": "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf", "caption": "Target Q-value"}, "17": {"item_id": "2025952315", "image_id": "17", "src": "https://miro.medium.com/max/2412/1*nb61CxDTTAWR1EJnbCl1cA.png", "width": "1206", "height": "964", "credit": "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf", "caption": "DQN Pseudo Code"}, "18": {"item_id": "2025952315", "image_id": "18", "src": "https://miro.medium.com/max/3248/1*py-aIXySIL28u_1_cRrHqg.png", "width": "1624", "height": "158", "credit": "https://zhuanlan.zhihu.com/p/25239682", "caption": "Policy Function"}, "19": {"item_id": "2025952315", "image_id": "19", "src": "https://miro.medium.com/max/856/1*-LcAiv5h_LEVdqIwkPNaUA.png", "width": "428", "height": "50", "credit": "http://proceedings.mlr.press/v32/silver14.pdf", "caption": "Temporal Difference Error"}, "20": {"item_id": "2025952315", "image_id": "20", "src": "https://miro.medium.com/max/2456/1*IgGdMLe12MeWoQNkDhm0mg.png", "width": "1228", "height": "744", "credit": "https://arxiv.org/pdf/1509.02971.pdf", "caption": "Actor-critic Architecture"}, "21": {"item_id": "2025952315", "image_id": "21", "src": "https://miro.medium.com/max/1202/1*kQOZZfjgTMg7fiqNOXTsOg.png", "width": "601", "height": "441", "credit": "left", "caption": "Action Noise"}, "22": {"item_id": "2025952315", "image_id": "22", "src": "https://miro.medium.com/max/3108/1*qV8STzz6mEYIKjOXyibtrQ.png", "width": "1554", "height": "1130", "credit": "https://arxiv.org/pdf/1509.02971.pdf", "caption": "DDPG Pseudo Code"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 675}, "1888642190": {"item_id": "1888642190", "resolved_id": "1888642190", "given_url": "http://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/", "given_title": "Learning to Optimize with Reinforcement Learning", "favorite": "0", "status": "1", "time_added": "1505234984", "time_updated": "1612385367", "time_read": "1528501405", "time_favorited": "0", "sort_id": 14, "resolved_title": "Learning to Optimize with Reinforcement Learning", "resolved_url": "http://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/", "excerpt": "Since we posted our paper on “Learning to Optimize” last year, the area of optimizer learning has received growing attention. In this article, we provide an introduction to this line of work and share our perspective on the opportunities and challenges in this area.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3887", "lang": "en", "time_to_read": 18, "top_image_url": "http://bair.berkeley.edu/blog/assets/lto/teaser.png", "tags": {"reinforcement-learning": {"item_id": "1888642190", "tag": "reinforcement-learning"}}, "authors": {"70602785": {"item_id": "1888642190", "author_id": "70602785", "name": "Daniel Seita", "url": ""}}, "image": {"item_id": "1888642190", "src": "http://bair.berkeley.edu/static/blog/lto/teaser.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "1888642190", "image_id": "1", "src": "http://bair.berkeley.edu/static/blog/lto/teaser.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "1888642190", "image_id": "2", "src": "http://bair.berkeley.edu/static/blog/lto/alg_structure.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "1888642190", "image_id": "3", "src": "http://bair.berkeley.edu/static/blog/lto/memorization.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "1888642190", "image_id": "4", "src": "http://bair.berkeley.edu/static/blog/lto/impossibility.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "1888642190", "image_id": "5", "src": "http://bair.berkeley.edu/static/blog/lto/sl_performance.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "1888642190", "image_id": "6", "src": "http://bair.berkeley.edu/static/blog/lto/rl_performance.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "1888642190", "image_id": "7", "src": "http://bair.berkeley.edu/static/blog/lto/rl_formulation.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "1888642190", "image_id": "8", "src": "http://bair.berkeley.edu/static/blog/lto/results.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "1888642190", "image_id": "9", "src": "http://bair.berkeley.edu/static/blog/lto/traj_visualizations.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1505}, "2781404601": {"item_id": "2781404601", "resolved_id": "2781404601", "given_url": "https://jfpettit.github.io/blog/2019/11/03/fundamentals-of-reinforcement-learning", "given_title": "Looking at the Fundamentals of Reinforcement Learning", "favorite": "0", "status": "1", "time_added": "1573151327", "time_updated": "1638708525", "time_read": "1573151337", "time_favorited": "0", "sort_id": 15, "resolved_title": "Looking at the Fundamentals of Reinforcement Learning", "resolved_url": "https://jfpettit.github.io/blog/2019/11/03/fundamentals-of-reinforcement-learning", "excerpt": "In this post, we’ll get into the weeds with some of the fundamentals of reinforcement learning.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3500", "lang": "en", "time_to_read": 16, "tags": {"deep-learning": {"item_id": "2781404601", "tag": "deep-learning"}, "reinforcement-learning": {"item_id": "2781404601", "tag": "reinforcement-learning"}}, "authors": {"74984196": {"item_id": "2781404601", "author_id": "74984196", "name": "Where  is", "url": ""}}, "image": {"item_id": "2781404601", "src": "https://jfpettit.github.io/assets/imgs/github_logo.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2781404601", "image_id": "1", "src": "https://jfpettit.github.io/assets/imgs/github_logo.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2781404601", "image_id": "2", "src": "https://jfpettit.github.io/assets/imgs/twitter_logo.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2781404601", "image_id": "3", "src": "https://jfpettit.github.io/assets/imgs/svbtle_logo.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2781404601", "image_id": "4", "src": "https://jfpettit.github.io/assets/imgs/email_logo.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2781404601", "image_id": "5", "src": "https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2781404601", "image_id": "6", "src": "https://miro.medium.com/max/1454/1*G3q-q9gEiDc2fD8sPXHBpQ.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2781404601", "image_id": "7", "src": "https://lilianweng.github.io/lil-log/assets/images/MC_control.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1355}, "3179898048": {"item_id": "3179898048", "resolved_id": "3179898048", "given_url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-7-n-step-bootstrapping-6c3006a13265", "given_title": "N-step Bootstrapping. This is part 7 of the RL tutorial… | by Sagi Shaier |", "favorite": "0", "status": "1", "time_added": "1605987312", "time_updated": "1612385367", "time_read": "1606612329", "time_favorited": "0", "sort_id": 16, "resolved_title": "N-step Bootstrapping", "resolved_url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-7-n-step-bootstrapping-6c3006a13265", "excerpt": "This is part 7 of the RL tutorial series that will provide an overview of the book “Reinforcement Learning: An Introduction. Second edition.” by Richard S. Sutton and Andrew G. Barto. This book is available for free here This article is part of a series.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "718", "lang": "en", "time_to_read": 3, "top_image_url": "https://miro.medium.com/max/1125/0*vtGAsZqDz7Uib6UG", "tags": {"reinforcement-learning": {"item_id": "3179898048", "tag": "reinforcement-learning"}}, "authors": {"103781816": {"item_id": "3179898048", "author_id": "103781816", "name": "Sagi Shaier", "url": "https://medium.com/@Shaier"}}, "image": {"item_id": "3179898048", "src": "https://miro.medium.com/fit/c/56/56/1*ztUz_XsPmas8Q5_2DUbXDQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3179898048", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*ztUz_XsPmas8Q5_2DUbXDQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3179898048", "image_id": "2", "src": "https://miro.medium.com/max/2250/0*vtGAsZqDz7Uib6UG", "width": "1125", "height": "750", "credit": "ref", "caption": "Image from Pexels"}, "3": {"item_id": "3179898048", "image_id": "3", "src": "https://miro.medium.com/max/2776/1*zm6-Vq1NgUpvo8zCkzFvdg.png", "width": "1388", "height": "1250", "credit": "ref", "caption": "Backup diagram for n step methods"}, "4": {"item_id": "3179898048", "image_id": "4", "src": "https://miro.medium.com/max/2292/1*3D6DrNNwkTXCNAb59n27mg.png", "width": "1146", "height": "112", "credit": "ref", "caption": "MC return"}, "5": {"item_id": "3179898048", "image_id": "5", "src": "https://miro.medium.com/max/1280/1*4WHm0jfXC6C6Aywyma84AA.png", "width": "640", "height": "112", "credit": "ref", "caption": "1 step TD return"}, "6": {"item_id": "3179898048", "image_id": "6", "src": "https://miro.medium.com/max/1896/1*iTXLxmMYbXsbrEJkhfy7rA.png", "width": "948", "height": "116", "credit": "ref", "caption": "2 step TD return"}, "7": {"item_id": "3179898048", "image_id": "7", "src": "https://miro.medium.com/max/2908/1*Wpr3WJgwyh1v5K2KL9xXVA.png", "width": "1454", "height": "128", "credit": "ref", "caption": "n step TD return"}, "8": {"item_id": "3179898048", "image_id": "8", "src": "https://miro.medium.com/max/4204/1*8omwhV8YaJ6jHnvOQvEMmw.png", "width": "2102", "height": "1416", "credit": "ref", "caption": "n step TD algorithm"}, "9": {"item_id": "3179898048", "image_id": "9", "src": "https://miro.medium.com/max/3956/1*sQ5SWCh6JMVQBaL02_62wg.png", "width": "1978", "height": "1244", "credit": "ref", "caption": "Backup diagram for n step Sarsa"}, "10": {"item_id": "3179898048", "image_id": "10", "src": "https://miro.medium.com/max/3040/1*R0vhY8hFS1KmTrEX6B_O4w.png", "width": "1520", "height": "88", "credit": "ref", "caption": "Return"}, "11": {"item_id": "3179898048", "image_id": "11", "src": "https://miro.medium.com/max/2924/1*L8d9cbmw4VAH-Q5kvghL7w.png", "width": "1462", "height": "120", "credit": "ref", "caption": "Action-value pair"}, "12": {"item_id": "3179898048", "image_id": "12", "src": "https://miro.medium.com/max/4224/1*_hIZSw_UVqNHNxj-kZoZgw.png", "width": "2112", "height": "1738", "credit": "ref", "caption": "n step Sarsa algorithm"}, "13": {"item_id": "3179898048", "image_id": "13", "src": "https://miro.medium.com/max/3996/1*s9N9VKcHYDA-Rns3hr7apw.png", "width": "1998", "height": "598", "credit": "ref", "caption": "Path selection example"}, "14": {"item_id": "3179898048", "image_id": "14", "src": "https://miro.medium.com/max/2448/1*Luuy9zlWp-1XrNZoCIvBAQ.png", "width": "1224", "height": "106", "credit": "ref", "caption": "The returns"}, "15": {"item_id": "3179898048", "image_id": "15", "src": "https://miro.medium.com/max/1272/1*qZe5beid8s71AiiSbN1FqA.png", "width": "636", "height": "156", "credit": "ref", "caption": "In the last step we weight the probabilities of the actions"}, "16": {"item_id": "3179898048", "image_id": "16", "src": "https://miro.medium.com/max/1328/1*qoCtJhiZf2Wllji5RjWkbg.png", "width": "664", "height": "246", "credit": "ref", "caption": "importance sampling ratio"}, "17": {"item_id": "3179898048", "image_id": "17", "src": "https://miro.medium.com/max/2736/1*QoK2cmUkU9Cql7K2wo6a1g.png", "width": "1368", "height": "110", "credit": "ref", "caption": "Note that we weight the returns with the importance sampling ratio"}, "18": {"item_id": "3179898048", "image_id": "18", "src": "https://miro.medium.com/max/3316/1*j_8suleL7GX-OdRNlgC4TQ.png", "width": "1658", "height": "102", "credit": "ref", "caption": "Sarsa update"}, "19": {"item_id": "3179898048", "image_id": "19", "src": "https://miro.medium.com/max/4200/1*NIFrgo764ZRUWOMIWcbVLg.png", "width": "2100", "height": "1726", "credit": "ref", "caption": "Off policy n step Sarsa"}, "20": {"item_id": "3179898048", "image_id": "20", "src": "https://miro.medium.com/max/556/1*7VEuMJDilfmtFSzXiMmBMA.png", "width": "278", "height": "960", "credit": "ref", "caption": "3 step tree backup diagram"}, "21": {"item_id": "3179898048", "image_id": "21", "src": "https://miro.medium.com/max/1928/1*siYpSWbcFLQbper3QXSQ1A.png", "width": "964", "height": "146", "credit": "ref", "caption": "which is similar to expected Sarsa"}, "22": {"item_id": "3179898048", "image_id": "22", "src": "https://miro.medium.com/max/3404/1*lqoKAUTrK3WRKfzbq5RIKg.png", "width": "1702", "height": "432", "credit": "ref", "caption": "2 step tree backup"}, "23": {"item_id": "3179898048", "image_id": "23", "src": "https://miro.medium.com/max/2752/1*ap31WsJGtDzAVgNEjlciLA.png", "width": "1376", "height": "104", "credit": "ref", "caption": ""}, "24": {"item_id": "3179898048", "image_id": "24", "src": "https://miro.medium.com/max/3664/1*k3EpHoUxRogHmWh9mYup2A.png", "width": "1832", "height": "1676", "credit": "ref", "caption": "n step tree backup"}, "25": {"item_id": "3179898048", "image_id": "25", "src": "https://miro.medium.com/max/2940/1*ckGXD9e2eTvqpkViBPS_jA.png", "width": "1470", "height": "1092", "credit": "ref", "caption": ""}, "26": {"item_id": "3179898048", "image_id": "26", "src": "https://miro.medium.com/max/3428/1*iy3QW92qzE4b6gg8nLbImA.png", "width": "1714", "height": "1792", "credit": "ref", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 278}, "3224867213": {"item_id": "3224867213", "resolved_id": "3224867235", "given_url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-6-policy-gradients-step-by-step-f9f448e73754?source=rss----7f60cf5620c9---4", "given_title": "Reinforcement Learning Explained Visually (Part 6): Policy Gradients, step-", "favorite": "0", "status": "1", "time_added": "1610215533", "time_updated": "1638708525", "time_read": "1610731441", "time_favorited": "0", "sort_id": 17, "resolved_title": "Reinforcement Learning Explained Visually (Part 6): Policy Gradients, step-by-step", "resolved_url": "https://towardsdatascience.com/reinforcement-learning-explained-visually-part-6-policy-gradients-step-by-step-f9f448e73754", "excerpt": "This is the sixth article in my series on Reinforcement Learning (RL). We now have a good understanding of the concepts that form the building blocks of an RL problem, and the techniques used to solve them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2062", "lang": "en", "time_to_read": 9, "top_image_url": "https://miro.medium.com/max/737/1*hGNwjytdKtWFY69SFSMwwg.png", "tags": {"deep-learning": {"item_id": "3224867213", "tag": "deep-learning"}, "reinforcement-learning": {"item_id": "3224867213", "tag": "reinforcement-learning"}}, "authors": {"142118215": {"item_id": "3224867213", "author_id": "142118215", "name": "Ketan Doshi", "url": "https://ketanhdoshi.medium.com"}}, "image": {"item_id": "3224867213", "src": "https://miro.medium.com/fit/c/56/56/1*ZpFBF1tIJfdXWqSjIk1DKg.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3224867213", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*ZpFBF1tIJfdXWqSjIk1DKg.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3224867213", "image_id": "2", "src": "https://miro.medium.com/max/2000/0*DiqznIUhtrhHOq-J", "width": "1000", "height": "786", "credit": "Michael Dziedzic on Unsplash", "caption": ""}, "3": {"item_id": "3224867213", "image_id": "3", "src": "https://miro.medium.com/max/1338/1*_qQ5Sf39P4ZusDspO6WGpQ.png", "width": "669", "height": "201", "credit": "Image by Author", "caption": ""}, "4": {"item_id": "3224867213", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*ekQLwlQzVIeWJujJTB59Wg.png", "width": "700", "height": "286", "credit": "Image by Author", "caption": ""}, "5": {"item_id": "3224867213", "image_id": "5", "src": "https://miro.medium.com/max/2000/1*C4hMehBY9u0i02hom9hF1g.png", "width": "1000", "height": "341", "credit": "Image by Author", "caption": ""}, "6": {"item_id": "3224867213", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*37xQ9X8M2DDRfAJ-WaELaw.png", "width": "700", "height": "240", "credit": "Image by Author", "caption": ""}, "7": {"item_id": "3224867213", "image_id": "7", "src": "https://miro.medium.com/max/1258/1*ADZ_txGODUd0suwrWRmnKA.png", "width": "629", "height": "394", "credit": "Image by Author", "caption": ""}, "8": {"item_id": "3224867213", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*vGx4z8W3ToJnE6PBGK1L_w.png", "width": "700", "height": "248", "credit": "Image by Author", "caption": ""}, "9": {"item_id": "3224867213", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*JDpofIMGWCOOMjTCcd7kpg.png", "width": "700", "height": "322", "credit": "Image by Author", "caption": ""}, "10": {"item_id": "3224867213", "image_id": "10", "src": "https://miro.medium.com/max/1400/1*DeDoQtoiUBqiXrlcOU7kzw.png", "width": "700", "height": "255", "credit": "Image by Author", "caption": ""}, "11": {"item_id": "3224867213", "image_id": "11", "src": "https://miro.medium.com/max/1082/1*e_Lo_JBc6YbKRZw6Z3m-kw.png", "width": "541", "height": "246", "credit": "Image by Author", "caption": ""}, "12": {"item_id": "3224867213", "image_id": "12", "src": "https://miro.medium.com/max/1400/1*fr7h0lRmfK8oKz1z96_KDw.png", "width": "700", "height": "349", "credit": "Image by Author", "caption": ""}, "13": {"item_id": "3224867213", "image_id": "13", "src": "https://miro.medium.com/max/1324/1*tzEfqfMVxjNKUYdkV6fYVg.png", "width": "662", "height": "408", "credit": "Image by Author", "caption": ""}, "14": {"item_id": "3224867213", "image_id": "14", "src": "https://miro.medium.com/max/1400/1*hGNwjytdKtWFY69SFSMwwg.png", "width": "700", "height": "385", "credit": "Image by Author", "caption": ""}, "15": {"item_id": "3224867213", "image_id": "15", "src": "https://miro.medium.com/max/1400/1*5FLTkAHoTcj4ZJ-KBXVnSA.png", "width": "700", "height": "312", "credit": "Image by Author", "caption": ""}, "16": {"item_id": "3224867213", "image_id": "16", "src": "https://miro.medium.com/max/1368/1*Jb2mIgmxIyFeASIWJvyXmQ.png", "width": "684", "height": "448", "credit": "Image by Author", "caption": ""}, "17": {"item_id": "3224867213", "image_id": "17", "src": "https://miro.medium.com/max/1220/1*kFydYqArwGgvnHn3CGckhQ.png", "width": "610", "height": "320", "credit": "Image by Author", "caption": ""}, "18": {"item_id": "3224867213", "image_id": "18", "src": "https://miro.medium.com/max/2000/1*CW3LpQFtwLJUakp6m1YTgw.png", "width": "1000", "height": "335", "credit": "Image by Author", "caption": ""}, "19": {"item_id": "3224867213", "image_id": "19", "src": "https://miro.medium.com/max/2000/1*Rj0A1QGDECrnexDeduXtaQ.png", "width": "1000", "height": "284", "credit": "Image by Author", "caption": ""}, "20": {"item_id": "3224867213", "image_id": "20", "src": "https://miro.medium.com/max/1400/1*BeAmOjXP4PeJ0vTJmaGmmA.png", "width": "700", "height": "373", "credit": "Image by Author", "caption": ""}, "21": {"item_id": "3224867213", "image_id": "21", "src": "https://miro.medium.com/max/1400/1*oXhrjtq2GWxhD9x4OKSLgg.png", "width": "700", "height": "354", "credit": "Image by Author", "caption": ""}, "22": {"item_id": "3224867213", "image_id": "22", "src": "https://miro.medium.com/max/2000/1*1zG8cmkKZ4EBjyPhXcAn0g.png", "width": "1000", "height": "507", "credit": "Image by Author", "caption": ""}, "23": {"item_id": "3224867213", "image_id": "23", "src": "https://miro.medium.com/max/956/1*pb9a5NDAdyhbJBDyoBiG-Q.png", "width": "478", "height": "288", "credit": "Image by Author", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 798}, "3292486832": {"item_id": "3292486832", "resolved_id": "3292486832", "given_url": "https://venturebeat.com/2021/03/28/reinforcement-learning-the-next-great-ai-tech-moving-from-the-lab-to-the-real-world/", "given_title": "Reinforcement learning: The next great AI tech moving from the lab to the r", "favorite": "0", "status": "1", "time_added": "1616954171", "time_updated": "1638708525", "time_read": "1617138532", "time_favorited": "0", "sort_id": 18, "resolved_title": "Reinforcement learning: The next great AI tech moving from the lab to the real world", "resolved_url": "https://venturebeat.com/2021/03/28/reinforcement-learning-the-next-great-ai-tech-moving-from-the-lab-to-the-real-world/", "excerpt": "Elevate your enterprise data technology and strategy at Transform 2021.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1440", "lang": "en", "time_to_read": 7, "amp_url": "https://venturebeat.com/2021/03/28/reinforcement-learning-the-next-great-ai-tech-moving-from-the-lab-to-the-real-world/amp/", "top_image_url": "https://venturebeat.com/wp-content/uploads/2021/03/ai-4.jpg?w=1200&strip=all", "tags": {"deep-learning": {"item_id": "3292486832", "tag": "deep-learning"}, "reinforcement-learning": {"item_id": "3292486832", "tag": "reinforcement-learning"}}, "image": {"item_id": "3292486832", "src": "https://venturebeat.com/wp-content/uploads/2021/03/ai-4.jpg?resize=1200%2C600&strip=all", "width": "1200", "height": "600"}, "images": {"1": {"item_id": "3292486832", "image_id": "1", "src": "https://venturebeat.com/wp-content/uploads/2021/03/ai-4.jpg?resize=1200%2C600&strip=all", "width": "1200", "height": "600", "credit": "", "caption": "Image Credit: gremlin/Getty Images"}}, "domain_metadata": {"name": "VentureBeat", "logo": "https://logo.clearbit.com/venturebeat.com?size=800", "greyscale_logo": "https://logo.clearbit.com/venturebeat.com?size=800&greyscale=true"}, "listen_duration_estimate": 557}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709934866}