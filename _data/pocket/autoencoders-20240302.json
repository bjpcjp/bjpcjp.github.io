{"status": 1, "complete": 1, "list": {"3372760393": {"item_id": "3372760393", "resolved_id": "3372760393", "given_url": "https://duckduckgo.com/?q=variational+autoencoders&t=canonical&ia=web", "given_title": "", "favorite": "0", "status": "1", "time_added": "1625354076", "time_updated": "1673901753", "time_read": "1625354152", "time_favorited": "0", "sort_id": 0, "resolved_title": "variational autoencoders at DuckDuckGo", "resolved_url": "https://duckduckgo.com/?q=variational+autoencoders&t=canonical&ia=web", "excerpt": "DuckDuckGo. Privacy, Simplified.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "en", "tags": {"autoencoders": {"item_id": "3372760393", "tag": "autoencoders"}, "deep-learning": {"item_id": "3372760393", "tag": "deep-learning"}, "variational": {"item_id": "3372760393", "tag": "variational"}}, "domain_metadata": {"name": "DuckDuckGo", "logo": "https://logo.clearbit.com/duckduckgo.com?size=800", "greyscale_logo": "https://logo.clearbit.com/duckduckgo.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "3493082100": {"item_id": "3493082100", "resolved_id": "3493082100", "given_url": "https://github.com/bjpcjp/scikit-and-tensorflow-workbooks/blob/master/ch15-autoencoders.ipynb", "given_title": "", "favorite": "0", "status": "1", "time_added": "1638239964", "time_updated": "1673901753", "time_read": "1642374645", "time_favorited": "0", "sort_id": 1, "resolved_title": "", "resolved_url": "https://github.com/bjpcjp/scikit-and-tensorflow-workbooks/blob/master/ch15-autoencoders.ipynb", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"autoencoders": {"item_id": "3493082100", "tag": "autoencoders"}, "deep-learning": {"item_id": "3493082100", "tag": "deep-learning"}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "3521891293": {"item_id": "3521891293", "resolved_id": "3521891293", "given_url": "https://heartbeat.comet.ml/essential-guide-to-auto-encoders-in-data-science-part-2-431c3d069f90", "given_title": "", "favorite": "0", "status": "1", "time_added": "1642022194", "time_updated": "1673901753", "time_read": "1642028817", "time_favorited": "0", "sort_id": 2, "resolved_title": "Essential Guide to Auto Encoders in Data Science (Part 2)", "resolved_url": "https://heartbeat.comet.ml/essential-guide-to-auto-encoders-in-data-science-part-2-431c3d069f90", "excerpt": "Auto-encoders learn from data examples automatically. In other words, the algorithm can be tailored to be effective on specific input types without needing any new engineering: only appropriate training data is required.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1601", "lang": "en", "time_to_read": 7, "top_image_url": "https://miro.medium.com/freeze/max/624/1*wA8dPd7OrhiUnH9KuheiWQ.gif", "tags": {"autoencoders": {"item_id": "3521891293", "tag": "autoencoders"}, "machine-learning": {"item_id": "3521891293", "tag": "machine-learning"}}, "authors": {"159518789": {"item_id": "3521891293", "author_id": "159518789", "name": "Sharmila", "url": "https://medium.com/@godhasharmila"}}, "image": {"item_id": "3521891293", "src": "https://miro.medium.com/fit/c/56/56/0*pijl5a85SqGsU697", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3521891293", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/0*pijl5a85SqGsU697", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3521891293", "image_id": "2", "src": "https://miro.medium.com/max/1240/1*FPrGauU7poVCbzCGcvo6rA.jpeg", "width": "620", "height": "434", "credit": "", "caption": ""}, "3": {"item_id": "3521891293", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*7YZ4Uim-Y3GmoPvXsR-zTw.jpeg", "width": "700", "height": "116", "credit": "", "caption": ""}, "4": {"item_id": "3521891293", "image_id": "4", "src": "https://miro.medium.com/max/1248/1*wA8dPd7OrhiUnH9KuheiWQ.gif", "width": "624", "height": "387", "credit": "", "caption": "Image Source"}, "5": {"item_id": "3521891293", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*BEuwWbuAe0clP60O3rzoOA.jpeg", "width": "700", "height": "160", "credit": "", "caption": "L1 Loss"}, "6": {"item_id": "3521891293", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*P6rXG3WTKI1awzwAR413MQ.jpeg", "width": "700", "height": "180", "credit": "", "caption": "KL-Divergence"}, "7": {"item_id": "3521891293", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*9RR_wfJdKkSN5RodPAnIxw.jpeg", "width": "700", "height": "143", "credit": "", "caption": ""}, "8": {"item_id": "3521891293", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*yuYJ-cUslBPXCetNGCCWhQ.jpeg", "width": "700", "height": "130", "credit": "", "caption": ""}, "9": {"item_id": "3521891293", "image_id": "9", "src": "https://miro.medium.com/max/726/1*Fepj4zSLveVIYPHA86cyFw.png", "width": "363", "height": "521", "credit": "", "caption": "Image Source"}, "10": {"item_id": "3521891293", "image_id": "10", "src": "https://miro.medium.com/max/1400/1*Y_IT-5BroSMm_aS7MVc06A.jpeg", "width": "700", "height": "347", "credit": "", "caption": "Image Source"}, "11": {"item_id": "3521891293", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*v1p2mqL4ua2XPaCa3NCqCA.jpeg", "width": "700", "height": "263", "credit": "", "caption": "Image Source"}, "12": {"item_id": "3521891293", "image_id": "12", "src": "https://miro.medium.com/max/1400/1*oqfw2WDQRdhOMGKejnP3iA.jpeg", "width": "700", "height": "305", "credit": "", "caption": "Image Source"}, "13": {"item_id": "3521891293", "image_id": "13", "src": "https://miro.medium.com/max/1400/1*K_EjGtVY3xiZ_-1K5l-mMA.jpeg", "width": "700", "height": "313", "credit": "", "caption": "Image Source"}, "14": {"item_id": "3521891293", "image_id": "14", "src": "https://miro.medium.com/max/1400/1*nQE1BYWdp3LZivLk95Q6DA.jpeg", "width": "700", "height": "291", "credit": "", "caption": "Image Source"}}, "listen_duration_estimate": 620}, "3587663927": {"item_id": "3587663927", "resolved_id": "3578791287", "given_url": "https://link.medium.com/Mv5y1iN5Tob", "given_title": "", "favorite": "0", "status": "1", "time_added": "1648918614", "time_updated": "1673901753", "time_read": "1648943586", "time_favorited": "0", "sort_id": 3, "resolved_title": "Autoencoders (AE) — A Smart Way to Process Your Data Using Unsupervised Neural Networks", "resolved_url": "https://towardsdatascience.com/autoencoders-ae-a-smart-way-to-process-your-data-using-unsupervised-neural-networks-9661f93a8509", "excerpt": "Autoencoders present an efficient way to learn a representation of your data that focuses on the signal, not the noise. You can use them for a variety of tasks such as:", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1132", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1200/1*JgMGbqdPXCpDjfbQWzyAzQ.png", "tags": {"autoencoders": {"item_id": "3587663927", "tag": "autoencoders"}, "deep-learning": {"item_id": "3587663927", "tag": "deep-learning"}}, "authors": {"148379686": {"item_id": "3587663927", "author_id": "148379686", "name": "Saul Dobilas", "url": "https://solclover.com"}}, "image": {"item_id": "3587663927", "src": "https://miro.medium.com/max/1400/1*JgMGbqdPXCpDjfbQWzyAzQ.png", "width": "700", "height": "586"}, "images": {"1": {"item_id": "3587663927", "image_id": "1", "src": "https://miro.medium.com/max/1400/1*JgMGbqdPXCpDjfbQWzyAzQ.png", "width": "700", "height": "586", "credit": "", "caption": "Undercomplete Autoencoders. Image by author, created using AlexNail’s NN-SVG tool."}, "2": {"item_id": "3587663927", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*1JwnlfIm4MmlFcPH5QaUIg.png", "width": "700", "height": "427", "credit": "", "caption": "A high-level illustration of layers within an Autoencoder Neural Network. Image by author."}, "3": {"item_id": "3587663927", "image_id": "3", "src": "https://miro.medium.com/max/2708/1*qkXay39OnVc2IosW6rkxtw.png", "width": "1354", "height": "460", "credit": "", "caption": ""}, "4": {"item_id": "3587663927", "image_id": "4", "src": "https://miro.medium.com/max/2708/1*vabxOXtQ4T034N_mscHSmQ.png", "width": "1354", "height": "460", "credit": "", "caption": ""}, "5": {"item_id": "3587663927", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*Cc-HFI93fc3mc56YWPn95A.png", "width": "700", "height": "638", "credit": "", "caption": "Undercomplete Autoencoder. Image by author, created using AlexNail’s NN-SVG tool."}, "6": {"item_id": "3587663927", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*C7cJ7SUyZ0TL5OW-H5EdAg.png", "width": "700", "height": "357", "credit": "", "caption": "A snippet of Kaggle’s Australian weather data with some modifications. Image by author."}, "7": {"item_id": "3587663927", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*P3m2edKPTzoX-JJetMkYsA.png", "width": "700", "height": "643", "credit": "", "caption": "Autoencoder model summary. Image by author."}, "8": {"item_id": "3587663927", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*EgSCTvKKxh1-bMCUaKkpgw.png", "width": "700", "height": "1391", "credit": "", "caption": "Autoencoder model diagram. Image by author."}, "9": {"item_id": "3587663927", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*TVXOmG9-0k6T_IXB6mth-A.png", "width": "700", "height": "401", "credit": "", "caption": "Autoencoder model loss by epoch. Image by author."}, "10": {"item_id": "3587663927", "image_id": "10", "src": "https://miro.medium.com/max/1400/1*6VBiWFiTu0ex1FW-pILKEg.png", "width": "700", "height": "749", "credit": "", "caption": "Encoder model diagram. Image by author."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 438}, "3835064556": {"item_id": "3835064556", "resolved_id": "3830004280", "given_url": "https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc", "given_title": "", "favorite": "0", "status": "1", "time_added": "1680049787", "time_updated": "1680284008", "time_read": "1680284008", "time_favorited": "0", "sort_id": 4, "resolved_title": "Hands-on Generative AI with GANs using Python: Autoencoders", "resolved_url": "https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-autoencoders-c77232b402fc", "excerpt": "In recent years, generative models have gained popularity due to Artificial Intelligent’s ability to produce synthetic instances that are almost indistinguishable from real data.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "1433", "lang": "en", "time_to_read": 7, "top_image_url": "https://miro.medium.com/v2/resize:fit:1200/0*1Sd8PSZCmbPTizb8", "tags": {"autoencoders": {"item_id": "3835064556", "tag": "autoencoders"}, "deep-learning": {"item_id": "3835064556", "tag": "deep-learning"}}, "authors": {"156659042": {"item_id": "3835064556", "author_id": "156659042", "name": "Marcello Politi", "url": "https://medium.com/@marcellopoliti"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 555}, "3886679405": {"item_id": "3886679405", "resolved_id": "3886679405", "given_url": "https://towardsdatascience.com/a-deep-dive-into-autoencoders-and-their-relationship-to-pca-and-svd-97e37c81898a", "given_title": "A Deep Dive into Autoencoders and Their Relationship to PCA and SVD", "favorite": "0", "status": "1", "time_added": "1686697193", "time_updated": "1690158945", "time_read": "1690158945", "time_favorited": "0", "sort_id": 5, "resolved_title": "A Deep Dive into Autoencoders and Their Relationship to PCA and SVD", "resolved_url": "https://towardsdatascience.com/a-deep-dive-into-autoencoders-and-their-relationship-to-pca-and-svd-97e37c81898a", "excerpt": "An autoencoder is a type of neural network that learns to reconstruct its input. It consists of an encoder network that compresses the input data into a low-dimensional space and a decoder network that reconstructs the input data from that space.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "9353", "lang": "en", "time_to_read": 43, "top_image_url": "https://miro.medium.com/v2/resize:fit:1079/1*kjJq1sntQLluYvrn7bXFUA.jpeg", "tags": {"autoencoders": {"item_id": "3886679405", "tag": "autoencoders"}, "machine-learning": {"item_id": "3886679405", "tag": "machine-learning"}, "pca": {"item_id": "3886679405", "tag": "pca"}, "svd": {"item_id": "3886679405", "tag": "svd"}}, "authors": {"146502413": {"item_id": "3886679405", "author_id": "146502413", "name": "Reza Bagheri", "url": "https://reza-bagheri79.medium.com"}}, "image": {"item_id": "3886679405", "src": "https://miro.medium.com/v2/resize:fill:88:88/2*IK2HiNXwVbxjMxsRX2B7WQ.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "3886679405", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/2*IK2HiNXwVbxjMxsRX2B7WQ.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "3886679405", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 3621}, "3574417713": {"item_id": "3574417713", "resolved_id": "3574417741", "given_url": "https://towardsdatascience.com/autoencoders-from-vanilla-to-variational-6f5bb5537e4a?source=rss----7f60cf5620c9---4", "given_title": "Autoencoders: From Vanilla to Variational", "favorite": "0", "status": "1", "time_added": "1647369502", "time_updated": "1673901753", "time_read": "1647383214", "time_favorited": "0", "sort_id": 6, "resolved_title": "Autoencoders: From Vanilla to Variational", "resolved_url": "https://towardsdatascience.com/autoencoders-from-vanilla-to-variational-6f5bb5537e4a", "excerpt": "When you hear about computer-generated images, you probably think about deep fakes, the cats that don’t exist, or a horse turned zebra. And this, quite reasonably, brings up GANs to your mind.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "4035", "lang": "en", "time_to_read": 18, "top_image_url": "https://miro.medium.com/max/1200/1*2n7b1cjwcLzLS12dVf_HuA.png", "tags": {"autoencoders": {"item_id": "3574417713", "tag": "autoencoders"}, "deep-learning": {"item_id": "3574417713", "tag": "deep-learning"}, "gans": {"item_id": "3574417713", "tag": "gans"}}, "authors": {"144032084": {"item_id": "3574417713", "author_id": "144032084", "name": "Michał Oleszak", "url": "https://michaloleszak.medium.com"}}, "image": {"item_id": "3574417713", "src": "https://miro.medium.com/max/1400/1*2n7b1cjwcLzLS12dVf_HuA.png", "width": "700", "height": "394"}, "images": {"1": {"item_id": "3574417713", "image_id": "1", "src": "https://miro.medium.com/max/1400/1*2n7b1cjwcLzLS12dVf_HuA.png", "width": "700", "height": "394", "credit": "", "caption": ""}, "2": {"item_id": "3574417713", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*32Dtu22nD4latACr.png", "width": "700", "height": "63", "credit": "", "caption": ""}, "3": {"item_id": "3574417713", "image_id": "3", "src": "https://miro.medium.com/max/1148/1*3YcQBrhJya463vwjS3iopg.png", "width": "574", "height": "329", "credit": "link: arXiv:1906.00446", "caption": "Part of Figure 5 from “Generating Diverse High-Fidelity Images with VQ-VAE-2”"}, "4": {"item_id": "3574417713", "image_id": "4", "src": "https://miro.medium.com/max/1206/1*ksT8Zxb3HWZdgJI6UMuLsA.png", "width": "603", "height": "197", "credit": "link:   arXiv:1512.09300", "caption": "Part of Figure 4 from “Autoencoding beyond pixels using a learned similarity metric”"}, "5": {"item_id": "3574417713", "image_id": "5", "src": "https://miro.medium.com/max/1130/1*mUozcPnJPQ40O_yjoYducw.png", "width": "565", "height": "212", "credit": "", "caption": "Autoencoder learns a low-dim representation of its input. Image by the author."}, "6": {"item_id": "3574417713", "image_id": "6", "src": "https://miro.medium.com/max/710/1*7vKPRYsMODGI9lX29-N9Gw.png", "width": "355", "height": "409", "credit": "", "caption": "Data sample. Source: https://github.com/googlecreativelab/quickdraw-dataset."}, "7": {"item_id": "3574417713", "image_id": "7", "src": "https://miro.medium.com/max/874/1*ndeC_rPoaHYfzGDLwc5w8Q.png", "width": "437", "height": "175", "credit": "top row", "caption": "Original images from the test set"}, "8": {"item_id": "3574417713", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*9BKvSG-q9KsWm__R_4ogGg.png", "width": "700", "height": "548", "credit": "", "caption": "Test images’ latent representations mapped to 2D by t-SNE. Image by the author."}, "9": {"item_id": "3574417713", "image_id": "9", "src": "https://miro.medium.com/max/784/1*C44zmbM3rBzvokUu78lmbw.png", "width": "392", "height": "262", "credit": "", "caption": "Probability density over the latent space values. Image by the author."}, "10": {"item_id": "3574417713", "image_id": "10", "src": "https://miro.medium.com/max/1114/1*njylZDPlcE8WnV7Z-MU6vw.png", "width": "557", "height": "558", "credit": "", "caption": "A 3D subset of test data embedding. Image by the author."}, "11": {"item_id": "3574417713", "image_id": "11", "src": "https://miro.medium.com/max/588/1*w0UVgLuVG5HFezVwa8a5rA.png", "width": "294", "height": "306", "credit": "", "caption": "Generated cats, dogs, and trees — kind of. Image by the author."}, "12": {"item_id": "3574417713", "image_id": "12", "src": "https://miro.medium.com/max/1400/0*rynNcRuNo2LvTP_9.png", "width": "700", "height": "63", "credit": "", "caption": ""}, "13": {"item_id": "3574417713", "image_id": "13", "src": "https://miro.medium.com/max/870/1*5T4Cm4SOxJhFl4JzZMW-Hg.png", "width": "435", "height": "185", "credit": "top row", "caption": "Original images from the test set"}, "14": {"item_id": "3574417713", "image_id": "14", "src": "https://miro.medium.com/max/700/1*XbdfgWmJ2xDilhVHEoJwfQ.png", "width": "350", "height": "171", "credit": "", "caption": "Sample images of a cat and a tree. Image by the author."}, "15": {"item_id": "3574417713", "image_id": "15", "src": "https://miro.medium.com/max/1400/1*ABdywfw-DggzULlvbg-Ljw.png", "width": "700", "height": "65", "credit": "", "caption": "A cat morphing into a tree. Image by the author."}, "16": {"item_id": "3574417713", "image_id": "16", "src": "https://miro.medium.com/max/1400/1*_3NS1lNQ49fkRYEGJVw7Pg.png", "width": "700", "height": "67", "credit": "", "caption": "A cat morphing into a dog. Image by the author."}, "17": {"item_id": "3574417713", "image_id": "17", "src": "https://miro.medium.com/max/720/1*auvUaaVg5YgSAmPTD8mXuw.png", "width": "360", "height": "173", "credit": "", "caption": "Cat reconstructions. Image by the author."}, "18": {"item_id": "3574417713", "image_id": "18", "src": "https://miro.medium.com/max/934/1*b4fB9GLl5-OJHsy_wiaPOA.png", "width": "467", "height": "117", "credit": "", "caption": "Arithmetic on images."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1562}, "3322333081": {"item_id": "3322333081", "resolved_id": "3322333107", "given_url": "https://towardsdatascience.com/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-part-2-6b79ad697c79?source=rss----7f60cf5620c9---4", "given_title": "", "favorite": "0", "status": "1", "time_added": "1620047400", "time_updated": "1673901753", "time_read": "1620052748", "time_favorited": "0", "sort_id": 7, "resolved_title": "Beginner guide to Variational Autoencoders (VAE) with PyTorch Lightning (Part 2)", "resolved_url": "https://towardsdatascience.com/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-part-2-6b79ad697c79", "excerpt": "In Part 1, we looked at the variational autoencoder, a model based on the autoencoder but allows for data generation. We learned about the overall architecture and the implementation details that allow it to learn successfully.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1289", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1200/0*eFzoUZatvbK4m3zK", "tags": {"autoencoders": {"item_id": "3322333081", "tag": "autoencoders"}, "deep-learning": {"item_id": "3322333081", "tag": "deep-learning"}, "pytorch": {"item_id": "3322333081", "tag": "pytorch"}}, "authors": {"126262743": {"item_id": "3322333081", "author_id": "126262743", "name": "reo neo", "url": "https://medium.com/@reoneo97"}}, "image": {"item_id": "3322333081", "src": "https://miro.medium.com/fit/c/56/56/1*HD8pJNnA7NpdzOc2JxgcMQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3322333081", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*HD8pJNnA7NpdzOc2JxgcMQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3322333081", "image_id": "2", "src": "https://miro.medium.com/max/11910/0*eFzoUZatvbK4m3zK", "width": "5955", "height": "3350", "credit": "Marc-Olivier Jodoin on Unsplash", "caption": ""}, "3": {"item_id": "3322333081", "image_id": "3", "src": "https://miro.medium.com/max/1052/1*IDWKLfpuiZ6cYCJmPy97Bg.png", "width": "526", "height": "436", "credit": "", "caption": "Tracking loss function using TensorBoard"}, "4": {"item_id": "3322333081", "image_id": "4", "src": "https://miro.medium.com/max/484/1*TwErmNaLLFdRHWcwtamgAg.png", "width": "242", "height": "242", "credit": "", "caption": ""}, "5": {"item_id": "3322333081", "image_id": "5", "src": "https://miro.medium.com/max/484/1*0yLwZQEs6b8Ni40SqfQqzA.png", "width": "242", "height": "242", "credit": "Left", "caption": "Comparing image sample during first epoch"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 499}, "3371668950": {"item_id": "3371668950", "resolved_id": "3371668970", "given_url": "https://towardsdatascience.com/building-a-vae-playground-with-streamlit-aa88a3394c04?source=rss----7f60cf5620c9---4", "given_title": "Building a VAE Playground with Streamlit", "favorite": "0", "status": "1", "time_added": "1625232887", "time_updated": "1673901753", "time_read": "1625351193", "time_favorited": "0", "sort_id": 8, "resolved_title": "Building a VAE Playground with Streamlit", "resolved_url": "https://towardsdatascience.com/building-a-vae-playground-with-streamlit-aa88a3394c04", "excerpt": "This blog post is part of a mini-series that talks about the different aspects of building a PyTorch Deep Learning project using Variational Autoencoders. In previous sections we learnt how to train two different types of Variational Autoencoders using PyTorch Lightning.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1694", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/freeze/max/928/1*bf-CUFfd2qoyOcw_XDeaRA.gif", "tags": {"autoencoders": {"item_id": "3371668950", "tag": "autoencoders"}, "machine-learning": {"item_id": "3371668950", "tag": "machine-learning"}, "streamlit": {"item_id": "3371668950", "tag": "streamlit"}}, "authors": {"151872092": {"item_id": "3371668950", "author_id": "151872092", "name": "Reo Neo", "url": "https://reoneo.medium.com"}}, "image": {"item_id": "3371668950", "src": "https://miro.medium.com/fit/c/56/56/1*HD8pJNnA7NpdzOc2JxgcMQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3371668950", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*HD8pJNnA7NpdzOc2JxgcMQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3371668950", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*bf-CUFfd2qoyOcw_XDeaRA.gif", "width": "700", "height": "541", "credit": "", "caption": ""}, "3": {"item_id": "3371668950", "image_id": "3", "src": "https://miro.medium.com/max/1356/1*aYoE0SyzT2zx4M547s41Tg.png", "width": "678", "height": "323", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 656}, "3289249214": {"item_id": "3289249214", "resolved_id": "3289249289", "given_url": "https://towardsdatascience.com/curious-about-variational-autoencoders-vaes-start-here-ab212fc2dd54?source=rss----7f60cf5620c9---4", "given_title": "Curious about Variational Autoencoders (VAEs)? Start Here.", "favorite": "0", "status": "1", "time_added": "1616630092", "time_updated": "1673901753", "time_read": "1616661987", "time_favorited": "0", "sort_id": 9, "resolved_title": "Curious about Variational Autoencoders (VAEs)? Start Here.", "resolved_url": "https://towardsdatascience.com/curious-about-variational-autoencoders-vaes-start-here-ab212fc2dd54", "excerpt": "In recent years, GANs (generative adversarial networks) have been all the rage in the field of deep-learning generative models, leaving VAEs in relative obscurity.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "133", "lang": "en", "tags": {"autoencoders": {"item_id": "3289249214", "tag": "autoencoders"}, "deep-learning": {"item_id": "3289249214", "tag": "deep-learning"}}, "authors": {"146923948": {"item_id": "3289249214", "author_id": "146923948", "name": "Ben Huberman", "url": "https://benzbox.medium.com"}}, "image": {"item_id": "3289249214", "src": "https://miro.medium.com/fit/c/56/56/0*d9SeKmDV1vNzwzz1.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3289249214", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/0*d9SeKmDV1vNzwzz1.png", "width": "28", "height": "28", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 51}, "3712137297": {"item_id": "3712137297", "resolved_id": "3712137297", "given_url": "https://towardsdatascience.com/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce", "given_title": "", "favorite": "0", "status": "1", "time_added": "1665661436", "time_updated": "1673901753", "time_read": "1665670109", "time_favorited": "0", "sort_id": 10, "resolved_title": "NSVQ: Improved Vector Quantization technique for Neural Networks Training", "resolved_url": "https://towardsdatascience.com/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce", "excerpt": "Vector quantization (VQ) is a data compression technique which models the probability density function of the data by some representative vectors called codebooks.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1347", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1200/0*mQbE21i6fsfqpU3L", "tags": {"autoencoders": {"item_id": "3712137297", "tag": "autoencoders"}, "compression-encoding": {"item_id": "3712137297", "tag": "compression-encoding"}, "deep-learning": {"item_id": "3712137297", "tag": "deep-learning"}, "machine-learning": {"item_id": "3712137297", "tag": "machine-learning"}, "search": {"item_id": "3712137297", "tag": "search"}}, "authors": {"172754577": {"item_id": "3712137297", "author_id": "172754577", "name": "Mohammad Vali", "url": "https://medium.com/@mohammad.vali"}}, "image": {"item_id": "3712137297", "src": "https://miro.medium.com/max/1400/0*mQbE21i6fsfqpU3L", "width": "700", "height": "467"}, "images": {"1": {"item_id": "3712137297", "image_id": "1", "src": "https://miro.medium.com/max/1400/0*mQbE21i6fsfqpU3L", "width": "700", "height": "467", "credit": "vackground.com on Unsplash", "caption": ""}, "2": {"item_id": "3712137297", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*_j4iFnsv7Xt5CoeZ40rZoQ.png", "width": "700", "height": "558", "credit": "image from here", "caption": "Vector Quantization operation"}, "3": {"item_id": "3712137297", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*pTECMRydvo-e3j58W75Hnw.png", "width": "700", "height": "390", "credit": "Image from here", "caption": "Straight Through Estimator"}, "4": {"item_id": "3712137297", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*xvwZLWlnBY41-4dlyEZE-Q.png", "width": "700", "height": "310", "credit": "Image by author", "caption": "Block diagram of NSVQ: Noise Substitution in Vector Quantization"}, "5": {"item_id": "3712137297", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*fobBGu61-2QmsNsvuIwktA.png", "width": "700", "height": "357", "credit": "Image by author", "caption": "Performance of the proposed NSVQ and STE in terms of PESQ and pSNR metrics for 12 bit VQ at overall bitrates of 8, 9.6, 13.2, 16.4, 24.4 and 32 kbit/s in the speech coding scenario; solid lines refer to the mean values of PESQ and pSNR, and the corresponding filled areas refer to their 95% quantiles."}, "6": {"item_id": "3712137297", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*P0LWep6XHRHZT-e3nnfr5A.png", "width": "700", "height": "362", "credit": "Image by author", "caption": "SSIM and Peak SNR values of STE and the proposed NSVQ for different VQ bitrates after 15 k training updates and over 20 individual experiments in the image compression scenario; solid lines refer to the mean values of SSIM and Peak SNR, and the corresponding filled areas refer to their 95% quantiles."}, "7": {"item_id": "3712137297", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*xZS1wtQHkWXTCXBo3OvdxA.png", "width": "700", "height": "352", "credit": "MSE", "caption": "Smoothed training error"}, "8": {"item_id": "3712137297", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*ZYByHhJjFqfzRtEIzX7npA.png", "width": "700", "height": "440", "credit": "Image by author", "caption": "Final optimized codebooks for 8 bit vector quantization using NSVQ method"}, "9": {"item_id": "3712137297", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*6diWUUxradTGbc7ZnAtA_g.png", "width": "700", "height": "441", "credit": "Image by author", "caption": "Final optimized codebooks for 8 bit vector quantization using STE method"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 521}, "3703769052": {"item_id": "3703769052", "resolved_id": "3703769052", "given_url": "https://towardsdatascience.com/introduction-to-embedding-clustering-and-similarity-11dd80b00061", "given_title": "Introduction to Embedding, Clustering, and Similarity", "favorite": "0", "status": "1", "time_added": "1663269182", "time_updated": "1673901753", "time_read": "1663289992", "time_favorited": "0", "sort_id": 11, "resolved_title": "Introduction to Embedding, Clustering, and Similarity", "resolved_url": "https://towardsdatascience.com/introduction-to-embedding-clustering-and-similarity-11dd80b00061", "excerpt": "In this post, we give a general introduction to embedding, similarity, and clustering, which are the basics to most ML and essential to understanding the Latent Space.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3024", "lang": "en", "time_to_read": 14, "top_image_url": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*aKGHxGmlyyA1kO6y", "tags": {"autoencoders": {"item_id": "3703769052", "tag": "autoencoders"}, "clustering": {"item_id": "3703769052", "tag": "clustering"}, "machine-learning": {"item_id": "3703769052", "tag": "machine-learning"}, "model-compression": {"item_id": "3703769052", "tag": "model-compression"}}, "authors": {"172289546": {"item_id": "3703769052", "author_id": "172289546", "name": "Mathias Grønne", "url": "https://medium.com/@mathiasgronne"}}, "image": {"item_id": "3703769052", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*0Ju5M1MDffvgQOdLkL-yLA.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "3703769052", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*0Ju5M1MDffvgQOdLkL-yLA.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "3703769052", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1171}, "3610344525": {"item_id": "3610344525", "resolved_id": "3610344525", "given_url": "https://towardsdatascience.com/sparse-autoencoder-neural-networks-how-to-utilise-sparsity-for-robust-information-encoding-6aa9ff542bc9", "given_title": "Sparse Autoencoder Neural Networks — How to Utilise Sparsity for Robust Inf", "favorite": "0", "status": "1", "time_added": "1651619509", "time_updated": "1673901753", "time_read": "1651678041", "time_favorited": "0", "sort_id": 12, "resolved_title": "Sparse Autoencoder Neural Networks — How to Utilise Sparsity for Robust Information Encoding", "resolved_url": "https://towardsdatascience.com/sparse-autoencoder-neural-networks-how-to-utilise-sparsity-for-robust-information-encoding-6aa9ff542bc9", "excerpt": "Autoencoders enable us to distil information by utilising a neural network architecture composed of an encoder and decoder. There are multiple types of autoencoders that vary based on their structure or the problems they are designed to solve. The four most commons ones are:", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1158", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/940/1*1-gz-jCEEnE4XALIUUKVEQ.png", "tags": {"autoencoders": {"item_id": "3610344525", "tag": "autoencoders"}, "deep-learning": {"item_id": "3610344525", "tag": "deep-learning"}, "python": {"item_id": "3610344525", "tag": "python"}}, "authors": {"148379686": {"item_id": "3610344525", "author_id": "148379686", "name": "Saul Dobilas", "url": "https://solclover.com"}}, "image": {"item_id": "3610344525", "src": "https://miro.medium.com/max/1400/1*1-gz-jCEEnE4XALIUUKVEQ.png", "width": "700", "height": "587"}, "images": {"1": {"item_id": "3610344525", "image_id": "1", "src": "https://miro.medium.com/max/1400/1*1-gz-jCEEnE4XALIUUKVEQ.png", "width": "700", "height": "587", "credit": "SAE", "caption": "Sparse Autoencoder"}, "2": {"item_id": "3610344525", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*0gDmxAoBF6k3hltZvS76fg.png", "width": "700", "height": "571", "credit": "", "caption": "Undercomplete Autoencoder architecture. Image by author, created using AlexNail’s NN-SVG tool."}, "3": {"item_id": "3610344525", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*cs1t0PLZF4tsnRcY9GTFKA.png", "width": "700", "height": "618", "credit": "SAE", "caption": "Sparse Autoencoder"}, "4": {"item_id": "3610344525", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*5gEa1BA4Ce864MxMOKHtnA.png", "width": "700", "height": "387", "credit": "", "caption": "The first ten digits of the MNIST dataset. Image by author."}, "5": {"item_id": "3610344525", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*ZoC6_q8_25E22L5xQ4ZIPg.png", "width": "700", "height": "435", "credit": "", "caption": "Undercomplete AE model summary. Image by author."}, "6": {"item_id": "3610344525", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*1qLGWSwmIOqAJJRRUOW1dw.png", "width": "700", "height": "402", "credit": "", "caption": "Undercomplete AE model loss chart. Image by author."}, "7": {"item_id": "3610344525", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*ZaYCahTJ8zLNpftBVesSLw.png", "width": "700", "height": "475", "credit": "", "caption": "Sparse AE model summary. Image by author."}, "8": {"item_id": "3610344525", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*iRl-pL0L6FKFNzL9Pa4kLw.png", "width": "700", "height": "400", "credit": "", "caption": "Sparse AE model loss chart. Image by author."}, "9": {"item_id": "3610344525", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*M64UApv47JmN7mXlsG4lbg.png", "width": "700", "height": "319", "credit": "", "caption": "The selected ten digits for model comparison. Image by author."}, "10": {"item_id": "3610344525", "image_id": "10", "src": "https://miro.medium.com/max/1400/1*EBv3Ykqwa42RxELQ8DBf4w.png", "width": "700", "height": "349", "credit": "", "caption": "Neuron activations in the two AE models. Image by author."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 448}, "3788268337": {"item_id": "3788268337", "resolved_id": "3788268356", "given_url": "https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9?source=rss----7f60cf5620c9---4", "given_title": "Uncovering Anomalies with Variational Autoencoders (VAE): A Deep Dive into ", "favorite": "0", "status": "1", "time_added": "1673946278", "time_updated": "1673966479", "time_read": "1673966479", "time_favorited": "0", "sort_id": 13, "resolved_title": "Uncovering Anomalies with Variational Autoencoders (VAE): A Deep Dive into the World of Unsupervised Learning", "resolved_url": "https://towardsdatascience.com/uncovering-anomalies-with-variational-autoencoders-vae-a-deep-dive-into-the-world-of-1b2bce47e2e9", "excerpt": "In an earlier post, I explained what autoencoders are, what they are used for and how to leverage them in training an anomaly detection model. As a reminder, autoencoders are a type of neural network that are commonly used for dimensionality reduction and learning features.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "2067", "lang": "en", "time_to_read": 9, "top_image_url": "https://miro.medium.com/max/1200/0*M_N58YhhzOwbzxJz", "tags": {"autoencoders": {"item_id": "3788268337", "tag": "autoencoders"}, "deep-learning": {"item_id": "3788268337", "tag": "deep-learning"}, "variational": {"item_id": "3788268337", "tag": "variational"}}, "authors": {"103200589": {"item_id": "3788268337", "author_id": "103200589", "name": "Will Badr", "url": "https://medium.com/@will.badr"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 800}, "3917153167": {"item_id": "3917153167", "resolved_id": "3917134072", "given_url": "https://towardsdatascience.com/variational-autoencoder-vae-with-discrete-distribution-using-gumbel-softmax-b3f749b3417e?source=rss----7f60cf5620c9---4", "given_title": "Variational Autoencoder (VAE) with Discrete Distribution using Gumbel Softm", "favorite": "0", "status": "1", "time_added": "1691621740", "time_updated": "1691682640", "time_read": "1691682640", "time_favorited": "0", "sort_id": 14, "resolved_title": "Variational Autoencoder (VAE) with Discrete Distribution using Gumbel Softmax", "resolved_url": "https://towardsdatascience.com/variational-autoencoder-vae-with-discrete-distribution-using-gumbel-softmax-b3f749b3417e", "excerpt": "Generative models have become very popular nowadays thanks to their ability to generate novel samples with inherent variability by learning and capturing the underlying probability distribution of the training data.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "292", "lang": "en", "top_image_url": "https://miro.medium.com/v2/resize:fit:640/1*IB22-8qoNpIITYqKA034lQ.png", "tags": {"autoencoders": {"item_id": "3917153167", "tag": "autoencoders"}, "deep-learning": {"item_id": "3917153167", "tag": "deep-learning"}, "generative": {"item_id": "3917153167", "tag": "generative"}, "pytorch": {"item_id": "3917153167", "tag": "pytorch"}}, "authors": {"160244866": {"item_id": "3917153167", "author_id": "160244866", "name": "Alexey Kravets", "url": "https://medium.com/@alexml0123"}}, "image": {"item_id": "3917153167", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*Sk0BsQAVvq8-jQhT9iVrwQ.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "3917153167", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*Sk0BsQAVvq8-jQhT9iVrwQ.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "3917153167", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 113}, "2799205818": {"item_id": "2799205818", "resolved_id": "2799205861", "given_url": "https://www.kdnuggets.com/2019/11/all-about-autoencoders.html", "given_title": "Neural Networks 201: All About Autoencoders", "favorite": "0", "status": "1", "time_added": "1574431023", "time_updated": "1673901753", "time_read": "1576355687", "time_favorited": "0", "sort_id": 15, "resolved_title": "Neural Networks 201: All About Autoencoders", "resolved_url": "https://www.kdnuggets.com/neural-networks-201-all-about-autoencoders.html/", "excerpt": "Autoencoders can be a very powerful tool for leveraging unlabeled data to solve a variety of problems, such as learning a \"feature extractor\" that helps build powerful classifiers, finding anomalies, or doing a Missing Value Imputation. By Zak Jost, Research Scientist at Amazon Web Services.", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "334", "lang": "en", "tags": {"autoencoders": {"item_id": "2799205818", "tag": "autoencoders"}, "deep-learning": {"item_id": "2799205818", "tag": "deep-learning"}}, "image": {"item_id": "2799205818", "src": "http://img.youtube.com/vi/3jmcHZq3A5s/0.jpg", "width": "480", "height": "360"}, "images": {"1": {"item_id": "2799205818", "image_id": "1", "src": "http://img.youtube.com/vi/3jmcHZq3A5s/0.jpg", "width": "480", "height": "360", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "2799205818", "video_id": "1", "src": "https://www.youtube.com/embed/3jmcHZq3A5s?feature=oembed", "width": "600", "height": "338", "type": "1", "vid": "3jmcHZq3A5s", "length": "0"}}, "domain_metadata": {"name": "KDnuggets", "logo": "https://logo.clearbit.com/kdnuggets.com?size=800", "greyscale_logo": "https://logo.clearbit.com/kdnuggets.com?size=800&greyscale=true"}, "listen_duration_estimate": 129}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709419163}