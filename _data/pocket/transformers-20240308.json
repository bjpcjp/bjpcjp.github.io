{"status": 1, "complete": 1, "list": {"3131820715": {"item_id": "3131820715", "resolved_id": "3131820715", "given_url": "https://github.com/lucidrains/vit-pytorch", "given_title": "", "favorite": "0", "status": "1", "time_added": "1671369674", "time_updated": "1671403223", "time_read": "1671403222", "time_favorited": "0", "sort_id": 0, "resolved_title": "lucidrains/vit-pytorch", "resolved_url": "https://github.com/lucidrains/vit-pytorch", "excerpt": "Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch. Significance is further explained in Yannic Kilcher's video.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2899", "lang": "en", "time_to_read": 13, "top_image_url": "https://opengraph.githubassets.com/e225c022a1f8272134515bd1488ec8fb876695df81a4b08ea2057d7bc9cc877e/lucidrains/vit-pytorch", "tags": {"deep-learning": {"item_id": "3131820715", "tag": "deep-learning"}, "machine-vision": {"item_id": "3131820715", "tag": "machine-vision"}, "pytorch": {"item_id": "3131820715", "tag": "pytorch"}, "transformers": {"item_id": "3131820715", "tag": "transformers"}}, "image": {"item_id": "3131820715", "src": "https://github.com/lucidrains/vit-pytorch/raw/main/images/vit.gif", "width": "500", "height": "0"}, "images": {"1": {"item_id": "3131820715", "image_id": "1", "src": "https://github.com/lucidrains/vit-pytorch/raw/main/images/vit.gif", "width": "500", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3131820715", "image_id": "2", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/distill.png", "width": "300", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3131820715", "image_id": "3", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/t2t.png", "width": "400", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3131820715", "image_id": "4", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/cross_vit.png", "width": "400", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3131820715", "image_id": "5", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/pit.png", "width": "400", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "3131820715", "image_id": "6", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/levit.png", "width": "300", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "3131820715", "image_id": "7", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/cvt.png", "width": "400", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "3131820715", "image_id": "8", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/twins_svt.png", "width": "400", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "3131820715", "image_id": "9", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/nest.png", "width": "400", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "3131820715", "image_id": "10", "src": "https://github.com/lucidrains/vit-pytorch/blob/main/images/dino.png", "width": "350", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 1122}, "3866468993": {"item_id": "3866468993", "resolved_id": "3866431202", "given_url": "https://thesequence.substack.com/p/edge-291-reinforcement-learning-with?utm_medium=email", "given_title": "", "favorite": "0", "status": "1", "time_added": "1684240747", "time_updated": "1706821793", "time_read": "1684404949", "time_favorited": "0", "sort_id": 1, "resolved_title": "Edge 291: Reinforcement Learning with Human Feedback", "resolved_url": "https://thesequence.substack.com/p/edge-291-reinforcement-learning-with", "excerpt": "The RLHF paper. The transformer reinforcement learning framework.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "100", "lang": "en", "top_image_url": "https://substackcdn.com/image/fetch/w_1200,h_600,c_limit,f_jpg,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1aaa4cc-10d6-4ada-bba0-f1f2f0793427_1024x1024.png", "tags": {"chatgpt": {"item_id": "3866468993", "tag": "chatgpt"}, "llms": {"item_id": "3866468993", "tag": "llms"}, "reinforcement-learning": {"item_id": "3866468993", "tag": "reinforcement-learning"}, "transformers": {"item_id": "3866468993", "tag": "transformers"}}, "authors": {"86252": {"item_id": "3866468993", "author_id": "86252", "name": "Jesus Rodriguez", "url": ""}}, "listen_duration_estimate": 39}, "3844888061": {"item_id": "3844888061", "resolved_id": "3844888061", "given_url": "https://txt.cohere.ai/what-are-transformer-models/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1681585847", "time_updated": "1681924877", "time_read": "1681924877", "time_favorited": "0", "sort_id": 2, "resolved_title": "What Are Transformer Models and How Do They Work?", "resolved_url": "https://txt.cohere.ai/what-are-transformer-models/", "excerpt": "Transformers are a new development in machine learning that have been making a lot of noise lately. They are incredibly good at keeping track of context, and this is why the text that they write makes sense. In this blog post, we will go over their architecture and how they work.", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "2979", "lang": "en", "time_to_read": 14, "top_image_url": "https://txt.cohere.ai/content/images/2023/04/Fueling-Generative-Content-with-Keyword-Research--2-.jpg", "tags": {"transformers": {"item_id": "3844888061", "tag": "transformers"}}, "authors": {"2017534": {"item_id": "3844888061", "author_id": "2017534", "name": "Luis Serrano", "url": ""}}, "image": {"item_id": "3844888061", "src": "https://lh4.googleusercontent.com/YMc1E2RFxD4ooeQTDZNWQWjptSLB-ZyFlQC6i5WkUcAA7Sb2BXNIzGqxiaXWGcCfernmocOJBaoxcxWFiWGePiJ__jW9d68VE1saGOrAODwjT7UFIay98vcjsyX8nzxio8wJ04sav2wEm9nVBKycT2U", "width": "624", "height": "249"}, "images": {"1": {"item_id": "3844888061", "image_id": "1", "src": "https://lh4.googleusercontent.com/YMc1E2RFxD4ooeQTDZNWQWjptSLB-ZyFlQC6i5WkUcAA7Sb2BXNIzGqxiaXWGcCfernmocOJBaoxcxWFiWGePiJ__jW9d68VE1saGOrAODwjT7UFIay98vcjsyX8nzxio8wJ04sav2wEm9nVBKycT2U", "width": "624", "height": "249", "credit": "", "caption": ""}, "2": {"item_id": "3844888061", "image_id": "2", "src": "https://lh6.googleusercontent.com/NRTq1BnxHIBl2geH9pyvnrg8pZpeBYH293iBSu7Nt5fLsIOQP5JqFgvvXaLIHlHIlm-18JmgN_KvygzRHj5oWFSBQ-YfzZF5gHH6nNEmDczG2HPPV1YFNm5IF34nTkJM-N1_Q_mcZaAmKd71ECXpCxU", "width": "370", "height": "362", "credit": "", "caption": ""}, "3": {"item_id": "3844888061", "image_id": "3", "src": "https://lh4.googleusercontent.com/7PeFWif-WmUK6bi-3ocaoUpNYwEGzo-KCxMjU9wKS0D3qRpsgxek6fiYTo8HUhqgmnF7cijWpCT8Vhamb6aEkVbg7o9PadR9AWv0EDeS35_srSBgjM71vRnlm7kVky9uob7Y9tSmGsRWtkwHGUZuwSk", "width": "624", "height": "297", "credit": "", "caption": ""}, "4": {"item_id": "3844888061", "image_id": "4", "src": "https://txt.cohere.ai/content/images/2023/04/image-2.png", "width": "1224", "height": "340", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "3844888061", "video_id": "1", "src": "https://www.youtube.com/embed/-QH8fRhqFHM?feature=oembed", "width": "200", "height": "113", "type": "1", "vid": "-QH8fRhqFHM", "length": "0"}}, "listen_duration_estimate": 1153}, "3812165598": {"item_id": "3812165598", "resolved_id": "3812165607", "given_url": "https://arxiv.org/abs/2302.10360", "given_title": "", "favorite": "0", "status": "1", "time_added": "1677153397", "time_updated": "1685475303", "time_read": "1677372105", "time_favorited": "0", "sort_id": 3, "resolved_title": "Title:Optical Transformers", "resolved_url": "https://arxiv.org/abs/2302.10360v1", "excerpt": "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "77", "lang": "en", "top_image_url": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png", "tags": {"transformers": {"item_id": "3812165598", "tag": "transformers"}}, "authors": {"63240129": {"item_id": "3812165598", "author_id": "63240129", "name": "cs cs.LG cs.NE", "url": ""}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 30}, "3469204119": {"item_id": "3469204119", "resolved_id": "3469204119", "given_url": "https://e2eml.school/transformers.html", "given_title": "", "favorite": "0", "status": "1", "time_added": "1637869636", "time_updated": "1638708525", "time_read": "1638202072", "time_favorited": "0", "sort_id": 4, "resolved_title": "", "resolved_url": "https://e2eml.school/transformers.html", "excerpt": "I procrastinated a deep dive into transformers for a few years. Finally the discomfort of not knowing what makes them tick grew too great for me. Here is that dive. Transformers were introduced in this 2017 paper as a tool for sequence transduction—converting one sequence of symbols to another.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "11242", "lang": "", "tags": {"deep-learning": {"item_id": "3469204119", "tag": "deep-learning"}, "transformers": {"item_id": "3469204119", "tag": "transformers"}}, "image": {"item_id": "3469204119", "src": "https://e2eml.school/images/transformers/one_hot_vocabulary.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3469204119", "image_id": "1", "src": "https://e2eml.school/images/transformers/one_hot_vocabulary.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3469204119", "image_id": "2", "src": "https://e2eml.school/images/transformers/one_hot_sentence.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3469204119", "image_id": "3", "src": "https://e2eml.school/images/transformers/dot_product.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3469204119", "image_id": "4", "src": "https://e2eml.school/images/transformers/match.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3469204119", "image_id": "5", "src": "https://e2eml.school/images/transformers/non_match.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "3469204119", "image_id": "6", "src": "https://e2eml.school/images/transformers/similarity.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "3469204119", "image_id": "7", "src": "https://e2eml.school/images/transformers/matrix_mult_one_row_one_col.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "3469204119", "image_id": "8", "src": "https://e2eml.school/images/transformers/matrix_mult_two_row_one_col.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "3469204119", "image_id": "9", "src": "https://e2eml.school/images/transformers/matrix_mult_one_row_two_col.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "3469204119", "image_id": "10", "src": "https://e2eml.school/images/transformers/matrix_mult_three_row_two_col.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "3469204119", "image_id": "11", "src": "https://e2eml.school/images/transformers/markov_chain.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "3469204119", "image_id": "12", "src": "https://e2eml.school/images/transformers/transition_matrix.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "3469204119", "image_id": "13", "src": "https://e2eml.school/images/transformers/transition_lookups.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "3469204119", "image_id": "14", "src": "https://e2eml.school/images/transformers/markov_chain_2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "15": {"item_id": "3469204119", "image_id": "15", "src": "https://e2eml.school/images/transformers/markov_chain_second_order.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "3469204119", "image_id": "16", "src": "https://e2eml.school/images/transformers/transition_matrix_first_order_2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "17": {"item_id": "3469204119", "image_id": "17", "src": "https://e2eml.school/images/transformers/transition_matrix_second_order.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "18": {"item_id": "3469204119", "image_id": "18", "src": "https://e2eml.school/images/transformers/feature_voting.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "19": {"item_id": "3469204119", "image_id": "19", "src": "https://e2eml.school/images/transformers/transition_matrix_second_order_skips.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "20": {"item_id": "3469204119", "image_id": "20", "src": "https://e2eml.school/images/transformers/feature_selection.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "21": {"item_id": "3469204119", "image_id": "21", "src": "https://e2eml.school/images/transformers/masked_feature_activities.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "22": {"item_id": "3469204119", "image_id": "22", "src": "https://e2eml.school/images/transformers/masked_transition_matrix.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "23": {"item_id": "3469204119", "image_id": "23", "src": "https://e2eml.school/images/transformers/mask_matrix_lookup.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "24": {"item_id": "3469204119", "image_id": "24", "src": "https://e2eml.school/images/transformers/attention_equation_QKT.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "25": {"item_id": "3469204119", "image_id": "25", "src": "https://e2eml.school/images/transformers/feature_creation_layer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "26": {"item_id": "3469204119", "image_id": "26", "src": "https://e2eml.school/images/transformers/feature_creation_matrix.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "27": {"item_id": "3469204119", "image_id": "27", "src": "https://e2eml.school/images/transformers/second_order_feature_battery.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "28": {"item_id": "3469204119", "image_id": "28", "src": "https://e2eml.school/images/transformers/second_order_feature_program.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "29": {"item_id": "3469204119", "image_id": "29", "src": "https://e2eml.school/images/transformers/feedforward_equations.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "30": {"item_id": "3469204119", "image_id": "30", "src": "https://e2eml.school/images/transformers/architecture_feedforward.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "31": {"item_id": "3469204119", "image_id": "31", "src": "https://e2eml.school/images/transformers/one_hot_vs_embedding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "32": {"item_id": "3469204119", "image_id": "32", "src": "https://e2eml.school/images/transformers/embedded_words.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "33": {"item_id": "3469204119", "image_id": "33", "src": "https://e2eml.school/images/transformers/embedding_projection.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "34": {"item_id": "3469204119", "image_id": "34", "src": "https://e2eml.school/images/transformers/architecture_embedding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "35": {"item_id": "3469204119", "image_id": "35", "src": "https://e2eml.school/images/transformers/positional_encoding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "36": {"item_id": "3469204119", "image_id": "36", "src": "https://e2eml.school/images/transformers/architecture_positional.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "37": {"item_id": "3469204119", "image_id": "37", "src": "https://e2eml.school/images/transformers/de_embedding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "38": {"item_id": "3469204119", "image_id": "38", "src": "https://e2eml.school/images/transformers/de_embedded_results.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "39": {"item_id": "3469204119", "image_id": "39", "src": "https://e2eml.school/images/transformers/architecture_de_embedding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "40": {"item_id": "3469204119", "image_id": "40", "src": "https://e2eml.school/images/transformers/matrix_multiply_shape.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "41": {"item_id": "3469204119", "image_id": "41", "src": "https://e2eml.school/images/transformers/matrix_shapes.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "42": {"item_id": "3469204119", "image_id": "42", "src": "https://e2eml.school/images/transformers/architecture_multihead.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "43": {"item_id": "3469204119", "image_id": "43", "src": "https://e2eml.school/images/transformers/multihead_attention_equation.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "44": {"item_id": "3469204119", "image_id": "44", "src": "https://e2eml.school/images/transformers/architecture_single_head.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "45": {"item_id": "3469204119", "image_id": "45", "src": "https://e2eml.school/images/transformers/attention_equation.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "46": {"item_id": "3469204119", "image_id": "46", "src": "https://e2eml.school/images/transformers/mask.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "47": {"item_id": "3469204119", "image_id": "47", "src": "https://e2eml.school/images/transformers/architecture_add_norm.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "48": {"item_id": "3469204119", "image_id": "48", "src": "https://e2eml.school/images/transformers/skip_connection_gradients.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "49": {"item_id": "3469204119", "image_id": "49", "src": "https://e2eml.school/images/transformers/normalization.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "50": {"item_id": "3469204119", "image_id": "50", "src": "https://e2eml.school/images/transformers/layer_conveyer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "51": {"item_id": "3469204119", "image_id": "51", "src": "https://e2eml.school/images/transformers/gpt_architecture.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "52": {"item_id": "3469204119", "image_id": "52", "src": "https://e2eml.school/images/transformers/architecture_cross_attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 4352}, "3352505292": {"item_id": "3352505292", "resolved_id": "3352505292", "given_url": "https://arxiv.org/pdf/2106.04554.pdf", "given_title": "", "favorite": "0", "status": "1", "time_added": "1623669909", "time_updated": "1638708525", "time_read": "1623716076", "time_favorited": "0", "sort_id": 5, "resolved_title": "", "resolved_url": "https://arxiv.org/pdf/2106.04554.pdf", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"deep-learning": {"item_id": "3352505292", "tag": "deep-learning"}, "transformers": {"item_id": "3352505292", "tag": "transformers"}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "3347876702": {"item_id": "3347876702", "resolved_id": "3239387150", "given_url": "https://email.mg2.substack.com/c/eJwlkE1vwyAMhn9NuS3iIwFy4LDLjjtMvUd8uJSVQARkVf79SCtZtmzLev0-VjfwuRxqy7WhMy3t2EAleNYIrUFBe4WyBKcIYdMoZ-TU6IicJAp1uRWAVYeoWtkBbbuJweoWcjoPqJwZFeiuRiLsbI1xYJnUTFvDRi2oBYuJJOP81tW7C5AsKPiDcuQEKKp7a1u9sM8L_erhQ7vvZrB57c13gFh_svfQm2vRqd5yWaHUj-vecgk6VhQUxZRgjll_fqR8IAOfhaZMymniBBNiwFCwhhOjRw5YssuIV0-HupvatH2cYqgo87vZ363v_On2Nexml17XPYV2LJC0ieDeHNqb5ovM4iFB6ZTdopsinFJBMRZYdjIv2x0UE1hwwQjqqi73q6QcwBZBlxSSfwI84vEPRvWPwA", "given_title": "", "favorite": "0", "status": "1", "time_added": "1622726820", "time_updated": "1638708525", "time_read": "1622759063", "time_favorited": "0", "sort_id": 6, "resolved_title": "NielsRogge/Transformers-Tutorials", "resolved_url": "https://github.com/NielsRogge/Transformers-Tutorials", "excerpt": "Hi there! This repository contains demos I made with the Transformers library by 🤗 HuggingFace. Currently, all of them are implemented in PyTorch.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "1807", "lang": "en", "time_to_read": 8, "top_image_url": "https://opengraph.githubassets.com/3b208c07dc4ee4522d96eda369ab3bcf27f59e4bd6de79b344609cc919b3d371/NielsRogge/Transformers-Tutorials", "tags": {"deep-learning": {"item_id": "3347876702", "tag": "deep-learning"}, "transformers": {"item_id": "3347876702", "tag": "transformers"}}, "authors": {"172835540": {"item_id": "3347876702", "author_id": "172835540", "name": "NielsRogge", "url": "https://github.com/NielsRogge/Transformers-Tutorials/commits?author=NielsRogge"}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 699}, "3210738206": {"item_id": "3210738206", "resolved_id": "3210738206", "given_url": "https://theaisummer.com/transformer/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1621036985", "time_updated": "1638708525", "time_read": "1621356967", "time_favorited": "0", "sort_id": 7, "resolved_title": "How Transformers work in deep learning and NLP: an intuitive introduction", "resolved_url": "https://theaisummer.com/transformer/", "excerpt": "The famous paper “Attention is all you need” in 2017 changed the way we were thinking about attention. With enough data, matrix multiplications, linear layers, and layer normalization we can perform state-of-the-art-machine-translation.", "is_article": "1", "is_index": "1", "has_video": "0", "has_image": "1", "word_count": "3778", "lang": "en", "time_to_read": 17, "top_image_url": "https://theaisummer.com/static/6122618d7e1466853e88473ba375cdc7/ee604/transformer.png", "tags": {"deep-learning": {"item_id": "3210738206", "tag": "deep-learning"}, "nlp": {"item_id": "3210738206", "tag": "nlp"}, "transformers": {"item_id": "3210738206", "tag": "transformers"}}, "authors": {"136374887": {"item_id": "3210738206", "author_id": "136374887", "name": "Nikolas Adaloglou", "url": "https://theaisummer.com/author/Nikolas-Adaloglou/"}}, "image": {"item_id": "3210738206", "src": "https://theaisummer.com/static/c9a851690a62f1faaf054430ca35ab20/c7dcc/tokenization.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3210738206", "image_id": "1", "src": "https://theaisummer.com/static/c9a851690a62f1faaf054430ca35ab20/c7dcc/tokenization.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3210738206", "image_id": "2", "src": "https://theaisummer.com/static/257848131da90edbf099aa8c4bf392c4/27524/input-processing-tokenization-embedding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3210738206", "image_id": "3", "src": "https://theaisummer.com/static/a662e9c10a5401d1bd1ccdce52dfdbd6/eb645/positional-encoding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3210738206", "image_id": "4", "src": "https://theaisummer.com/static/2e000851b686eb35c6c3c06522437715/26a94/attention-as-database-query.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3210738206", "image_id": "5", "src": "https://theaisummer.com/static/ebfe1b1dbab018e608a77f85457e52db/16caa/vector-similarity.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "3210738206", "image_id": "6", "src": "https://theaisummer.com/static/4022cf02281d234e0e85fa44ad08b4e2/9f933/self-attention-probability-score-matrix.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "3210738206", "image_id": "7", "src": "https://theaisummer.com/static/56773616d30b9dcb31aa792f2d701276/3096d/key-query-value.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "3210738206", "image_id": "8", "src": "https://theaisummer.com/static/3ed7199184645f3e632d17ab6441244f/63a68/layer-norm.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "3210738206", "image_id": "9", "src": "https://theaisummer.com/static/f6068bcb3559a017af003c2bde071bcf/e3b18/encoders-attention-with-normalizarion.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "3210738206", "image_id": "10", "src": "https://theaisummer.com/static/dc71435f329458ee5cc09cb2ea09ebf8/7bc0b/encoder-without-multi-head.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "3210738206", "image_id": "11", "src": "https://theaisummer.com/static/9dc2e417714211a5166ece483b862d75/442cb/parallel-multi-head-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "3210738206", "image_id": "12", "src": "https://theaisummer.com/static/bba48bd14e38ede88ac1cacd8a638d6d/a4078/multi-head-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "3210738206", "image_id": "13", "src": "https://theaisummer.com/static/18072c01858310b080b3b6d9b4950175/e45a9/encoder.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "3210738206", "image_id": "14", "src": "https://theaisummer.com/static/7d6c2aa7af90f14cf44d533cbf88726e/8ff13/decoder.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1462}, "3807601744": {"item_id": "3807601744", "resolved_id": "3807601748", "given_url": "https://arxiv.org/abs/2302.07730", "given_title": "[2302.07730] Transformer models: an introduction and catalog", "favorite": "0", "status": "1", "time_added": "1696342638", "time_updated": "1696545966", "time_read": "1696545966", "time_favorited": "0", "sort_id": 8, "resolved_title": "Title:Transformer models: an introduction and catalog", "resolved_url": "https://arxiv.org/abs/2302.07730v1", "excerpt": "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "77", "lang": "en", "top_image_url": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png", "tags": {"arxiv": {"item_id": "3807601744", "tag": "arxiv"}, "llms": {"item_id": "3807601744", "tag": "llms"}, "transformers": {"item_id": "3807601744", "tag": "transformers"}}, "authors": {"178290250": {"item_id": "3807601744", "author_id": "178290250", "name": "cs", "url": "https://arxiv.org/abs/2302.07730?context=cs"}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 30}, "3552977122": {"item_id": "3552977122", "resolved_id": "3552977122", "given_url": "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021", "given_title": "All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Unders", "favorite": "0", "status": "1", "time_added": "1663290084", "time_updated": "1663713072", "time_read": "1663713071", "time_favorited": "0", "sort_id": 9, "resolved_title": "All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1", "resolved_url": "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021", "excerpt": "This is a long article that talks about almost everything one needs to know about the Attention mechanism including Self-Attention, Query, Keys, Values, Multi-Head Attention, Masked-Multi Head Attention, and Transformers including some details on BERT and GPT.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "2709", "lang": "en", "time_to_read": 12, "top_image_url": "https://miro.medium.com/v2/resize:fit:925/1*Rv_pntt-N2WL7LMbIptHxQ.png", "tags": {"deep-learning": {"item_id": "3552977122", "tag": "deep-learning"}, "transformers": {"item_id": "3552977122", "tag": "transformers"}}, "authors": {"141711448": {"item_id": "3552977122", "author_id": "141711448", "name": "Arjun Sarkar", "url": "https://arjun-sarkar786.medium.com"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1049}, "3702309716": {"item_id": "3702309716", "resolved_id": "3702309716", "given_url": "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada", "given_title": "All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Unders", "favorite": "0", "status": "1", "time_added": "1663112626", "time_updated": "1663713087", "time_read": "1663713087", "time_favorited": "0", "sort_id": 10, "resolved_title": "All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2", "resolved_url": "https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada", "excerpt": "In the previous story, I have explained what is the Attention mechanism, and some important keywords and blocks associated with Transformers, such as Self Attention, Query, Keys and Values, and Multi-head Attention.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1826", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/max/1200/0*Wvg_pNDViACfg-IK.png", "tags": {"deep-learning": {"item_id": "3702309716", "tag": "deep-learning"}, "transformers": {"item_id": "3702309716", "tag": "transformers"}}, "authors": {"141711448": {"item_id": "3702309716", "author_id": "141711448", "name": "Arjun Sarkar", "url": "https://arjun-sarkar786.medium.com"}}, "image": {"item_id": "3702309716", "src": "https://miro.medium.com/max/1090/1*9XuOogviDS6hkWGL2qIKQA.png", "width": "545", "height": "0"}, "images": {"1": {"item_id": "3702309716", "image_id": "1", "src": "https://miro.medium.com/max/1090/1*9XuOogviDS6hkWGL2qIKQA.png", "width": "545", "height": "0", "credit": "Source: Image from the original paper", "caption": "Figure 1. The Transformer Network"}, "2": {"item_id": "3702309716", "image_id": "2", "src": "https://miro.medium.com/max/552/1*y12gX71mhbtT96lwM0Zw0A.png", "width": "276", "height": "0", "credit": "Source: image from the original paper", "caption": "Figure 2. The Encoder part of the transformer network"}, "3": {"item_id": "3702309716", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*9-g1EJnHWF5lqFZTYtY68w.png", "width": "700", "height": "0", "credit": "Source: image created by author", "caption": "Figure 3. Input embedding"}, "4": {"item_id": "3702309716", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*xMcS_KlxzTo_X5zsKTpPdg.png", "width": "700", "height": "0", "credit": "Source: image created by author", "caption": "Figure 4. Intuitive understanding of positional embedding"}, "5": {"item_id": "3702309716", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*07Vf8VOpmj7J0wQB3rejSQ.png", "width": "700", "height": "0", "credit": "Source: image from the original paper", "caption": "Figure 5. Positional embedding used in the original paper"}, "6": {"item_id": "3702309716", "image_id": "6", "src": "https://miro.medium.com/max/550/1*Bo8Y3oW_nMqrgkk3ttsb-w.png", "width": "275", "height": "0", "credit": "Source: image from the original paper", "caption": "Figure 6. Scaled Dot-product Attention"}, "7": {"item_id": "3702309716", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*WP24tnEeiPNMHgTRCr-x2w.png", "width": "700", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "3702309716", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*iBtLFJu7eiGy5vhmOw56-w.png", "width": "700", "height": "0", "credit": "Source: image created by author", "caption": "Figure 7. The Attention block"}, "9": {"item_id": "3702309716", "image_id": "9", "src": "https://miro.medium.com/max/1400/0*Wvg_pNDViACfg-IK.png", "width": "700", "height": "0", "credit": "source: image created by author", "caption": "Figure 8. Multi-Head Attention"}, "10": {"item_id": "3702309716", "image_id": "10", "src": "https://miro.medium.com/max/552/1*PoM8vzFdcf6AEOMys3DtlA.png", "width": "276", "height": "0", "credit": "Souce: Image from the original paper", "caption": "Figure 9. The Decoder part of the Transformer network"}, "11": {"item_id": "3702309716", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*ag-93N1KFg67-qOjBo9Unw.png", "width": "700", "height": "0", "credit": "Source: image created by author", "caption": "Figure 10. The function of different decoder blocks in sentence translation"}, "12": {"item_id": "3702309716", "image_id": "12", "src": "https://miro.medium.com/max/1400/1*l9bfa_RbYzyTZBcYj9Vkcg.png", "width": "700", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "3702309716", "image_id": "13", "src": "https://miro.medium.com/max/1400/1*jxZqxY1sLjx_4i8O6Jt5QA.png", "width": "700", "height": "0", "credit": "source: image from the original paper", "caption": "Figure 11. Results"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 707}, "4014156439": {"item_id": "4014156439", "resolved_id": "4014156448", "given_url": "https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=rss----7f60cf5620c9---4", "given_title": "Attention for Vision Transformers, Explained", "favorite": "0", "status": "1", "time_added": "1708995339", "time_updated": "1709244943", "time_read": "1709244943", "time_favorited": "0", "sort_id": 11, "resolved_title": "Attention for Vision Transformers, Explained", "resolved_url": "https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673", "excerpt": "Since their introduction in 2017 with Attention is All You Need¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, An Image is Worth 16x16 Words² successfully adapted transformers for computer vision tasks.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2869", "lang": "en", "time_to_read": 13, "top_image_url": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*XLYx5OALyd914wu7", "tags": {"attention": {"item_id": "4014156439", "tag": "attention"}, "transformers": {"item_id": "4014156439", "tag": "transformers"}}, "authors": {"188474637": {"item_id": "4014156439", "author_id": "188474637", "name": "Skylar Jean Callis", "url": "https://medium.com/@sjcallis"}}, "image": {"item_id": "4014156439", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "4014156439", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "4014156439", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1111}, "3925060102": {"item_id": "3925060102", "resolved_id": "3914681247", "given_url": "https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161?utm_source=pocket_reader", "given_title": "Cracking Open the Hugging Face Transformers Library", "favorite": "0", "status": "1", "time_added": "1692962164", "time_updated": "1695684141", "time_read": "1695684141", "time_favorited": "0", "sort_id": 12, "resolved_title": "Cracking Open the Hugging Face Transformers Library", "resolved_url": "https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161", "excerpt": "This is the 3rd article in a series on using large language models (LLMs) in practice. Here I will give a beginner-friendly guide to the Hugging Face Transformers library, which provides an easy and cost-free way to work with a wide variety of open-source language models.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2295", "lang": "en", "time_to_read": 10, "top_image_url": "https://miro.medium.com/v2/resize:fit:1200/0*Rkoquyw55K6qbFWF", "tags": {"generative": {"item_id": "3925060102", "tag": "generative"}, "llms": {"item_id": "3925060102", "tag": "llms"}, "transformers": {"item_id": "3925060102", "tag": "transformers"}}, "authors": {"143628755": {"item_id": "3925060102", "author_id": "143628755", "name": "Shawhin Talebi", "url": "https://shawhin.medium.com"}}, "image": {"item_id": "3925060102", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*mhVX2L2LGQM4XZNwvU7H5A.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "3925060102", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*mhVX2L2LGQM4XZNwvU7H5A.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "3925060102", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 888}, "3352282942": {"item_id": "3352282942", "resolved_id": "3352282942", "given_url": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/", "given_title": "GPT-J-6B: 6B JAX-Based Transformer – Aran Komatsuzaki", "favorite": "0", "status": "1", "time_added": "1625445660", "time_updated": "1638708525", "time_read": "1625486738", "time_favorited": "0", "sort_id": 13, "resolved_title": "GPT-J-6B: 6B JAX-Based Transformer", "resolved_url": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/", "excerpt": "Summary: We have released GPT-J-6B, 6B JAX-based (Mesh) Transformer LM (Github). GPT-J-6B performs nearly on par with 6.7B GPT-3 (or Curie) on various zero-shot down-streaming tasks. You can try out this Colab notebook or free web demo.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1215", "lang": "en", "time_to_read": 6, "amp_url": "https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/amp/", "top_image_url": "https://arankomatsuzaki.files.wordpress.com/2021/05/jax_logo.png?w=1200", "tags": {"deep-learning": {"item_id": "3352282942", "tag": "deep-learning"}, "transformers": {"item_id": "3352282942", "tag": "transformers"}}, "authors": {"93456081": {"item_id": "3352282942", "author_id": "93456081", "name": "Aran Komatsuzaki", "url": ""}}, "image": {"item_id": "3352282942", "src": "https://arankomatsuzaki.files.wordpress.com/2021/05/jax_logo.png", "width": "490", "height": "283"}, "images": {"1": {"item_id": "3352282942", "image_id": "1", "src": "https://arankomatsuzaki.files.wordpress.com/2021/05/jax_logo.png", "width": "490", "height": "283", "credit": "", "caption": ""}}, "listen_duration_estimate": 470}, "3795585300": {"item_id": "3795585300", "resolved_id": "3795585300", "given_url": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/", "given_title": "Hacker News", "favorite": "0", "status": "1", "time_added": "1675691754", "time_updated": "1675807784", "time_read": "1675807784", "time_favorited": "0", "sort_id": 14, "resolved_title": "The Transformer Family Version 2.0", "resolved_url": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/", "excerpt": "Many new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post — restructure the hierarchy of sections and improve many sections with more recent papers.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "9152", "lang": "en", "time_to_read": 42, "tags": {"deep-learning": {"item_id": "3795585300", "tag": "deep-learning"}, "transformers": {"item_id": "3795585300", "tag": "transformers"}}, "authors": {"76470090": {"item_id": "3795585300", "author_id": "76470090", "name": "Lilian Weng", "url": ""}}, "image": {"item_id": "3795585300", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/multi-head-attention.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3795585300", "image_id": "1", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/multi-head-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3795585300", "image_id": "2", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/transformer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3795585300", "image_id": "3", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/sinoidual-positional-encoding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3795585300", "image_id": "4", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/RoPE.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3795585300", "image_id": "5", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/transformer-XL-training.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "3795585300", "image_id": "6", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/compressive-transformer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "3795585300", "image_id": "7", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/compressive-transformer-memory.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "3795585300", "image_id": "8", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/SPALM2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "3795585300", "image_id": "9", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/memorizing-transformer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "3795585300", "image_id": "10", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ALiBi-bias.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "3795585300", "image_id": "11", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/ALiBi-exp.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "3795585300", "image_id": "12", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/universal-transformer-loop.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "3795585300", "image_id": "13", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/universal-transformer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "3795585300", "image_id": "14", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/attention-per-head.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "15": {"item_id": "3795585300", "image_id": "15", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/soft-masking-function.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "3795585300", "image_id": "16", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/depth-adaptive-classifier.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "17": {"item_id": "3795585300", "image_id": "17", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/image-transformer-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "18": {"item_id": "3795585300", "image_id": "18", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/sparse-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "19": {"item_id": "3795585300", "image_id": "19", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/combined-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "20": {"item_id": "3795585300", "image_id": "20", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/LSH-attention-matrix.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "21": {"item_id": "3795585300", "image_id": "21", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/LSH-attention.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "22": {"item_id": "3795585300", "image_id": "22", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/linformer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "23": {"item_id": "3795585300", "image_id": "23", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/RFA.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "24": {"item_id": "3795585300", "image_id": "24", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/performer.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "25": {"item_id": "3795585300", "image_id": "25", "src": "https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/gated-transformer-XL.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 3543}, "3847407095": {"item_id": "3847407095", "resolved_id": "3847407095", "given_url": "https://magazine.sebastianraschka.com/p/understanding-large-language-models", "given_title": "Hacker News", "favorite": "0", "status": "1", "time_added": "1681682667", "time_updated": "1681865163", "time_read": "1681865163", "time_favorited": "0", "sort_id": 15, "resolved_title": "Understanding Large Language Models", "resolved_url": "https://magazine.sebastianraschka.com/p/understanding-large-language-models", "excerpt": "Note: Next to the monthly Ahead of AI series that discusses the latest research and trends, I plan to post some additional articles related to machine learning and AI once in a while.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3551", "lang": "en", "time_to_read": 16, "top_image_url": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png", "tags": {"llms": {"item_id": "3847407095", "tag": "llms"}, "transformers": {"item_id": "3847407095", "tag": "transformers"}}, "authors": {"10017863": {"item_id": "3847407095", "author_id": "10017863", "name": "Sebastian Raschka", "url": ""}}, "image": {"item_id": "3847407095", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png", "width": "595", "height": "261"}, "images": {"1": {"item_id": "3847407095", "image_id": "1", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png", "width": "595", "height": "261", "credit": "", "caption": "Source: https://arxiv.org/abs/1409.0473"}, "2": {"item_id": "3847407095", "image_id": "2", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png", "width": "515", "height": "431", "credit": "", "caption": "Source: https://arxiv.org/abs/1706.03762"}, "3": {"item_id": "3847407095", "image_id": "3", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png", "width": "266", "height": "269", "credit": "", "caption": "Source: Annotated figure based on https://people.idsia.ch//~juergen/fast-weight-programmer-1991-transformer.html#sec2"}, "4": {"item_id": "3847407095", "image_id": "4", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png", "width": "543", "height": "329", "credit": "", "caption": "Source: https://arxiv.org/abs/1810.04805"}, "5": {"item_id": "3847407095", "image_id": "5", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png", "width": "565", "height": "283", "credit": "", "caption": "Source: https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"}, "6": {"item_id": "3847407095", "image_id": "6", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png", "width": "553", "height": "318", "credit": "", "caption": "Source: https://arxiv.org/abs/2205.14135"}, "7": {"item_id": "3847407095", "image_id": "7", "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5b3254c-9b09-43a9-a975-68d41e3f12ae_1242x826.png", "width": "245", "height": "163", "credit": "", "caption": "https://sebastianraschka.com/books"}}, "listen_duration_estimate": 1375}, "3832449628": {"item_id": "3832449628", "resolved_id": "3832449628", "given_url": "https://johanwind.github.io/2023/03/23/rwkv_overview.html", "given_title": "Hacker News", "favorite": "1", "status": "1", "time_added": "1680186586", "time_updated": "1680276982", "time_read": "1680276982", "time_favorited": "1680276968", "sort_id": 16, "resolved_title": "The RWKV language model: An RNN with the advantages of a transformer", "resolved_url": "https://johanwind.github.io/2023/03/23/rwkv_overview.html", "excerpt": "For a while, I’ve been following and contributing to the RWKV language model, an open source large language model with great potential. As ChatGPT and large language models in general have gotten a lot of attention recently, I think it’s a good time to write about RWKV.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "867", "lang": "en", "time_to_read": 4, "tags": {"deep-learning": {"item_id": "3832449628", "tag": "deep-learning"}, "nlp": {"item_id": "3832449628", "tag": "nlp"}, "rnns": {"item_id": "3832449628", "tag": "rnns"}, "transformers": {"item_id": "3832449628", "tag": "transformers"}}, "image": {"item_id": "3832449628", "src": "https://johanwind.github.io/images/rwkv_benchmark.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3832449628", "image_id": "1", "src": "https://johanwind.github.io/images/rwkv_benchmark.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 336}, "3934190263": {"item_id": "3934190263", "resolved_id": "3934190263", "given_url": "https://dev.to/pavanbelagatti/hugging-face-101-a-tutorial-for-absolute-beginners-3b0l", "given_title": "Hugging Face 101: A Tutorial for Absolute Beginners!", "favorite": "0", "status": "1", "time_added": "1694540260", "time_updated": "1695684235", "time_read": "1695684235", "time_favorited": "0", "sort_id": 17, "resolved_title": "Hugging Face 101: A Tutorial for Absolute Beginners!", "resolved_url": "https://dev.to/pavanbelagatti/hugging-face-101-a-tutorial-for-absolute-beginners-3b0l", "excerpt": "Welcome to this beginner-friendly tutorial on sentiment analysis using Hugging Face's transformers library! Sentiment analysis is a Natural Language Processing (NLP) technique used to determine the emotional tone or attitude expressed in a piece of text.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "734", "lang": "en", "time_to_read": 3, "top_image_url": "https://res.cloudinary.com/practicaldev/image/fetch/s--y374RyTj--/c_imagga_scale,f_auto,fl_progressive,h_500,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ak3xsk6wk4hv3f6rhf6e.png", "tags": {"transformers": {"item_id": "3934190263", "tag": "transformers"}}, "image": {"item_id": "3934190263", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--HYau7Uf4--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ak3xsk6wk4hv3f6rhf6e.png", "width": "1000", "height": "420"}, "images": {"1": {"item_id": "3934190263", "image_id": "1", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--HYau7Uf4--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ak3xsk6wk4hv3f6rhf6e.png", "width": "1000", "height": "420", "credit": "", "caption": ""}, "2": {"item_id": "3934190263", "image_id": "2", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--6wbT5am---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zxfq7dnrvbmmzebq27xp.png", "width": "800", "height": "213", "credit": "", "caption": ""}, "3": {"item_id": "3934190263", "image_id": "3", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--1CVbOzi_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/uskchv84vfgvyeknnoht.png", "width": "550", "height": "774", "credit": "", "caption": ""}, "4": {"item_id": "3934190263", "image_id": "4", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--XS436P5U--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1b764lw2oq47fs22fxrr.png", "width": "800", "height": "961", "credit": "", "caption": ""}, "5": {"item_id": "3934190263", "image_id": "5", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--dbZ6Qr0u--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t7iuccys89tzt3dlqp9o.png", "width": "800", "height": "339", "credit": "", "caption": ""}, "6": {"item_id": "3934190263", "image_id": "6", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--MGtM9yBk--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ab0s284dhkvgu9u0fr0y.png", "width": "800", "height": "543", "credit": "", "caption": ""}, "7": {"item_id": "3934190263", "image_id": "7", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--JPGn0tO_--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/zqpbd180v07ilozdbqug.png", "width": "800", "height": "579", "credit": "", "caption": ""}, "8": {"item_id": "3934190263", "image_id": "8", "src": "https://res.cloudinary.com/practicaldev/image/fetch/s--YRQevSo1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/65sme5o4o9gzahh156m8.png", "width": "800", "height": "488", "credit": "", "caption": ""}}, "domain_metadata": {"name": "The Practical Dev", "logo": "https://logo.clearbit.com/dev.to?size=800", "greyscale_logo": "https://logo.clearbit.com/dev.to?size=800&greyscale=true"}, "listen_duration_estimate": 284}, "3610149901": {"item_id": "3610149901", "resolved_id": "3610149901", "given_url": "https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/", "given_title": "Meta has built a massive new language AI—and it’s giving it away for free", "favorite": "0", "status": "1", "time_added": "1681069729", "time_updated": "1682120651", "time_read": "1682120651", "time_favorited": "0", "sort_id": 18, "resolved_title": "Meta has built a massive new language AI—and it’s giving it away for free", "resolved_url": "https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/", "excerpt": "Meta’s AI lab has created a massive new language model that shares both the remarkable abilities and the harmful flaws of OpenAI’s pioneering neural network GPT-3.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1198", "lang": "en", "time_to_read": 5, "amp_url": "https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/amp/", "top_image_url": "https://wp.technologyreview.com/wp-content/uploads/2022/05/tiles2-1.jpeg?resize=1200,600", "tags": {"llms": {"item_id": "3610149901", "tag": "llms"}, "transformers": {"item_id": "3610149901", "tag": "transformers"}}, "authors": {"25561932": {"item_id": "3610149901", "author_id": "25561932", "name": "Will Douglas Heaven", "url": ""}}, "image": {"item_id": "3610149901", "src": "https://wp.technologyreview.com/wp-content/uploads/2022/05/tiles2-1.jpeg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3610149901", "image_id": "1", "src": "https://wp.technologyreview.com/wp-content/uploads/2022/05/tiles2-1.jpeg", "width": "0", "height": "0", "credit": "Ms Tech | Unsplash", "caption": ""}, "2": {"item_id": "3610149901", "image_id": "2", "src": "https://wp.technologyreview.com/wp-content/uploads/2020/11/newsletter-preferences-sm.png", "width": "0", "height": "0", "credit": "Illustration  Rose Wong", "caption": "Illustration by Rose Wong"}}, "domain_metadata": {"name": "MIT Technology Review", "logo": "https://logo.clearbit.com/technologyreview.com?size=800", "greyscale_logo": "https://logo.clearbit.com/technologyreview.com?size=800&greyscale=true"}, "listen_duration_estimate": 464}, "3897712130": {"item_id": "3897712130", "resolved_id": "3897712130", "given_url": "https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/", "given_title": "Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorc", "favorite": "0", "status": "1", "time_added": "1688429038", "time_updated": "1690156786", "time_read": "1690156786", "time_favorited": "0", "sort_id": 19, "resolved_title": "Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorch", "resolved_url": "https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/", "excerpt": "Key takeaway Peak memory consumption is a common bottleneck when training deep learning models such as vision transformers and LLMs.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2795", "lang": "en", "time_to_read": 13, "top_image_url": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/pytorch-memory-hero.png", "tags": {"llms": {"item_id": "3897712130", "tag": "llms"}, "pytorch": {"item_id": "3897712130", "tag": "pytorch"}, "transformers": {"item_id": "3897712130", "tag": "transformers"}}, "authors": {"179934693": {"item_id": "3897712130", "author_id": "179934693", "name": "Sebastian Raschka", "url": "https://lightning.ai/pages/author/sebastian-raschka/"}}, "image": {"item_id": "3897712130", "src": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/8_mixed-training-1024x339.png", "width": "622", "height": "206"}, "images": {"1": {"item_id": "3897712130", "image_id": "1", "src": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/8_mixed-training-1024x339.png", "width": "622", "height": "206", "credit": "", "caption": ""}, "2": {"item_id": "3897712130", "image_id": "2", "src": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/bfloat16.png", "width": "634", "height": "493", "credit": "", "caption": ""}, "3": {"item_id": "3897712130", "image_id": "3", "src": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/data-para-new.png", "width": "674", "height": "239", "credit": "", "caption": ""}, "4": {"item_id": "3897712130", "image_id": "4", "src": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/tensor-para-1-new.png", "width": "647", "height": "262", "credit": "", "caption": ""}, "5": {"item_id": "3897712130", "image_id": "5", "src": "https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/tensor-para-2-new.png", "width": "687", "height": "295", "credit": "", "caption": ""}}, "listen_duration_estimate": 1082}, "4014156402": {"item_id": "4014156402", "resolved_id": "4014156411", "given_url": "https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=rss----7f60cf5620c9---4", "given_title": "Position Embeddings for Vision Transformers, Explained", "favorite": "0", "status": "1", "time_added": "1708995333", "time_updated": "1709244954", "time_read": "1709244954", "time_favorited": "0", "sort_id": 20, "resolved_title": "Position Embeddings for Vision Transformers, Explained", "resolved_url": "https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5", "excerpt": "Since their introduction in 2017 with Attention is All You Need¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, An Image is Worth 16x16 Words² successfully adapted transformers for computer vision tasks.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2788", "lang": "en", "time_to_read": 13, "top_image_url": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*4SXrUaGxOcNmKG6e", "tags": {"transformers": {"item_id": "4014156402", "tag": "transformers"}}, "authors": {"188474637": {"item_id": "4014156402", "author_id": "188474637", "name": "Skylar Jean Callis", "url": "https://medium.com/@sjcallis"}}, "image": {"item_id": "4014156402", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "4014156402", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "4014156402", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1079}, "2239506919": {"item_id": "2239506919", "resolved_id": "2239506919", "given_url": "https://jalammar.github.io/illustrated-transformer/", "given_title": "The Illustrated Transformer – Jay Alammar – Visualizing machine learning on", "favorite": "0", "status": "1", "time_added": "1622248175", "time_updated": "1638708525", "time_read": "1622248847", "time_favorited": "0", "sort_id": 21, "resolved_title": "The Illustrated Transformer", "resolved_url": "http://jalammar.github.io/illustrated-transformer/", "excerpt": "Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese Watch: MIT’s Deep Learning State of the Art lecture referencing this post Featured in courses at Stanford, Harvard, MIT, Princeton", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "3954", "lang": "", "tags": {"deep-learning": {"item_id": "2239506919", "tag": "deep-learning"}, "transformers": {"item_id": "2239506919", "tag": "transformers"}}, "authors": {"91291457": {"item_id": "2239506919", "author_id": "91291457", "name": "Lukasz Kaiser", "url": "https://ai.google/research/people/LukaszKaiser"}}, "image": {"item_id": "2239506919", "src": "https://jalammar.github.io/images/t/the_transformer_3.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2239506919", "image_id": "1", "src": "https://jalammar.github.io/images/t/the_transformer_3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2239506919", "image_id": "2", "src": "https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2239506919", "image_id": "3", "src": "https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2239506919", "image_id": "4", "src": "https://jalammar.github.io/images/t/Transformer_encoder.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2239506919", "image_id": "5", "src": "https://jalammar.github.io/images/t/Transformer_decoder.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2239506919", "image_id": "6", "src": "https://jalammar.github.io/images/t/embeddings.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2239506919", "image_id": "7", "src": "https://jalammar.github.io/images/t/encoder_with_tensors.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2239506919", "image_id": "8", "src": "https://jalammar.github.io/images/t/encoder_with_tensors_2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2239506919", "image_id": "9", "src": "https://jalammar.github.io/images/t/transformer_self-attention_visualization.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2239506919", "image_id": "10", "src": "https://jalammar.github.io/images/t/transformer_self_attention_vectors.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "2239506919", "image_id": "11", "src": "https://jalammar.github.io/images/t/transformer_self_attention_score.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "2239506919", "image_id": "12", "src": "https://jalammar.github.io/images/t/self-attention_softmax.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "2239506919", "image_id": "13", "src": "https://jalammar.github.io/images/t/self-attention-output.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "2239506919", "image_id": "14", "src": "https://jalammar.github.io/images/t/self-attention-matrix-calculation.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "15": {"item_id": "2239506919", "image_id": "15", "src": "https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "2239506919", "image_id": "16", "src": "https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "17": {"item_id": "2239506919", "image_id": "17", "src": "https://jalammar.github.io/images/t/transformer_attention_heads_z.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "18": {"item_id": "2239506919", "image_id": "18", "src": "https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "19": {"item_id": "2239506919", "image_id": "19", "src": "https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "20": {"item_id": "2239506919", "image_id": "20", "src": "https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "21": {"item_id": "2239506919", "image_id": "21", "src": "https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "22": {"item_id": "2239506919", "image_id": "22", "src": "https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "23": {"item_id": "2239506919", "image_id": "23", "src": "https://jalammar.github.io/images/t/transformer_positional_encoding_example.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "24": {"item_id": "2239506919", "image_id": "24", "src": "https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "25": {"item_id": "2239506919", "image_id": "25", "src": "https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "26": {"item_id": "2239506919", "image_id": "26", "src": "https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "27": {"item_id": "2239506919", "image_id": "27", "src": "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "28": {"item_id": "2239506919", "image_id": "28", "src": "https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "29": {"item_id": "2239506919", "image_id": "29", "src": "https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "30": {"item_id": "2239506919", "image_id": "30", "src": "https://jalammar.github.io/images/t/vocabulary.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "31": {"item_id": "2239506919", "image_id": "31", "src": "https://jalammar.github.io/images/t/one-hot-vocabulary-example.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "32": {"item_id": "2239506919", "image_id": "32", "src": "https://jalammar.github.io/images/t/transformer_logits_output_and_label.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "33": {"item_id": "2239506919", "image_id": "33", "src": "https://jalammar.github.io/images/t/output_target_probability_distributions.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "34": {"item_id": "2239506919", "image_id": "34", "src": "https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "2239506919", "video_id": "1", "src": "https://www.youtube.com/embed/-QH8fRhqFHM", "width": "560", "height": "315", "type": "1", "vid": "-QH8fRhqFHM", "length": "0"}}, "listen_duration_estimate": 1531}, "3338137195": {"item_id": "3338137195", "resolved_id": "3338137195", "given_url": "https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication", "given_title": "Understanding Transformers, the machine learning model behind GPT-3", "favorite": "0", "status": "1", "time_added": "1621684487", "time_updated": "1671724937", "time_read": "1621693599", "time_favorited": "0", "sort_id": 22, "resolved_title": "Understanding Transformers, the machine learning model behind GPT-3", "resolved_url": "https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication", "excerpt": "You know that expression When you have a hammer, everything looks like a nail? Well, in machine learning, it seems like we really have discovered a magical hammer for which everything is, in fact, a nail, and they’re called Transformers.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2012", "lang": "en", "time_to_read": 9, "amp_url": "https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication/amp", "top_image_url": "https://img-cdn.tnwcdn.com/image/neural?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2021%2F05%2FAI-Transformers-abstract-hed.jpg&signature=7dad3d13328b212e6fc49afed1eb2819", "tags": {"chatbots": {"item_id": "3338137195", "tag": "chatbots"}, "deep-learning": {"item_id": "3338137195", "tag": "deep-learning"}, "nlp": {"item_id": "3338137195", "tag": "nlp"}, "transformers": {"item_id": "3338137195", "tag": "transformers"}}, "authors": {"64134674": {"item_id": "3338137195", "author_id": "64134674", "name": "Dale Markowitz", "url": ""}}, "image": {"item_id": "3338137195", "src": "https://img-cdn.tnwcdn.com/image?fit=1280%2C720&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2021%2F05%2FAI-Transformers-abstract-hed.jpg&signature=0bc21b935991e6805f4be8f370556d70", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3338137195", "image_id": "1", "src": "https://img-cdn.tnwcdn.com/image?fit=1280%2C720&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2021%2F05%2FAI-Transformers-abstract-hed.jpg&signature=0bc21b935991e6805f4be8f370556d70", "width": "0", "height": "0", "credit": "", "caption": "Image by: Shutterstock"}, "2": {"item_id": "3338137195", "image_id": "2", "src": "https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2021/05/renn.png", "width": "2880", "height": "960", "credit": "RNN", "caption": "A typical Recurrent Neural Network"}, "3": {"item_id": "3338137195", "image_id": "3", "src": "https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2021/05/screen-shot-2021-05-06-at-12.12.21-pm-1.png", "width": "1024", "height": "1416", "credit": "", "caption": "Transformer diagram from the original paper"}}, "domain_metadata": {"name": "The Next Web", "logo": "https://logo.clearbit.com/thenextweb.com?size=800", "greyscale_logo": "https://logo.clearbit.com/thenextweb.com?size=800&greyscale=true"}, "listen_duration_estimate": 779}, "4014156452": {"item_id": "4014156452", "resolved_id": "4014156457", "given_url": "https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=rss----7f60cf5620c9---4", "given_title": "Vision Transformers, Explained", "favorite": "0", "status": "1", "time_added": "1708995342", "time_updated": "1709244928", "time_read": "1709244928", "time_favorited": "0", "sort_id": 23, "resolved_title": "Vision Transformers, Explained", "resolved_url": "https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8", "excerpt": "Since their introduction in 2017 with Attention is All You Need¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, An Image is Worth 16x16 Words² successfully adapted transformers for computer vision tasks.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "4790", "lang": "en", "time_to_read": 22, "top_image_url": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*5r-4kq_qAbyFBnTt", "tags": {"transformers": {"item_id": "4014156452", "tag": "transformers"}}, "authors": {"188474637": {"item_id": "4014156452", "author_id": "188474637", "name": "Skylar Jean Callis", "url": "https://medium.com/@sjcallis"}}, "image": {"item_id": "4014156452", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "4014156452", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "4014156452", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1854}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709934965}