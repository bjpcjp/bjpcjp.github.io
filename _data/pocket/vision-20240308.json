{"status": 1, "complete": 1, "list": {"3098126892": {"item_id": "3098126892", "resolved_id": "3098126892", "given_url": "https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e", "given_title": "", "favorite": "0", "status": "1", "time_added": "1600260276", "time_updated": "1638708525", "time_read": "1604364097", "time_favorited": "0", "sort_id": 0, "resolved_title": "New Approaches to Object Detection", "resolved_url": "https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e", "excerpt": "I will start with a short introduction of different approaches to object detection. After both traditional and newer approaches are presented, you can read about the most important parts of CenterNet and TTFNet. Many ideas in both models are similar, therefore they will be introduced together.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1205", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1200/1*_ssXB-7gYmIwif0mxjncYg.jpeg", "tags": {"deep-learning": {"item_id": "3098126892", "tag": "deep-learning"}, "object-detection": {"item_id": "3098126892", "tag": "object-detection"}, "vision": {"item_id": "3098126892", "tag": "vision"}}, "authors": {"152215953": {"item_id": "3098126892", "author_id": "152215953", "name": "Libor Vanek", "url": "https://libor-vanek.medium.com"}}, "image": {"item_id": "3098126892", "src": "https://miro.medium.com/fit/c/56/56/1*YUYMuqs815KxE1iR6QtVJQ.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3098126892", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*YUYMuqs815KxE1iR6QtVJQ.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3098126892", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*_ssXB-7gYmIwif0mxjncYg.jpeg", "width": "700", "height": "394", "credit": "", "caption": "source: pexels.com"}, "3": {"item_id": "3098126892", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*QSdw1M6FkmarZXbD0PQs_A.jpeg", "width": "700", "height": "333", "credit": "", "caption": "source: pexels.com"}, "4": {"item_id": "3098126892", "image_id": "4", "src": "https://miro.medium.com/max/4400/1*60ROU3IyeI3U8ryAUXs-2A.png", "width": "2200", "height": "617", "credit": "Yellow: convolutional layer, red: max pooling, blue: upsampling.", "caption": "Simplified visualization of CenterNet with ResNet18, using upsampling and concatenation."}, "5": {"item_id": "3098126892", "image_id": "5", "src": "https://miro.medium.com/max/1084/1*lVma1W94MGETfUVaGEpF0g.png", "width": "542", "height": "256", "credit": "left", "caption": "A heatmap for CenterNet"}, "6": {"item_id": "3098126892", "image_id": "6", "src": "https://miro.medium.com/max/1084/1*khKe6aSlYkjjzUaCzRbhVw.png", "width": "542", "height": "238", "credit": "", "caption": "Selection of values for standard vs deformable convolution.FI"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 466}, "3119255087": {"item_id": "3119255087", "resolved_id": "3119255087", "given_url": "https://getpocket.com/explore/item/a-mathematical-model-unlocks-the-secrets-of-vision", "given_title": "", "favorite": "0", "status": "1", "time_added": "1602542209", "time_updated": "1602588236", "time_read": "1602588236", "time_favorited": "0", "sort_id": 1, "resolved_title": "A Mathematical Model Unlocks the Secrets of Vision", "resolved_url": "https://getpocket.com/explore/item/a-mathematical-model-unlocks-the-secrets-of-vision", "excerpt": "Mathematicians and neuroscientists have created the first anatomically accurate model that explains how vision is possible.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1898", "lang": "en", "time_to_read": 9, "top_image_url": "https://pocket-image-cache.com/1200x/filters:format(jpg):extract_focal()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2Farticles%2F5732%2F1600778827_Dynamical-Vision_2500_Lede.jpg", "tags": {"vision": {"item_id": "3119255087", "tag": "vision"}}, "authors": {"2866331": {"item_id": "3119255087", "author_id": "2866331", "name": "KEVIN HARTNETT", "url": ""}}, "image": {"item_id": "3119255087", "src": "https://pocket-syndicated-images.s3.amazonaws.com/5f69f22f3a812.jpg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3119255087", "image_id": "1", "src": "https://pocket-syndicated-images.s3.amazonaws.com/5f69f22f3a812.jpg", "width": "0", "height": "0", "credit": "", "caption": "Information from the eye passes through a bottleneck before it gets to the brain’s visual cortex, which heavily processes the sparse signal. Credit: DVDP for Quanta Magazine."}, "2": {"item_id": "3119255087", "image_id": "2", "src": "https://pocket-syndicated-publisher-logos.s3.amazonaws.com/5f64c938572a8.png", "width": "0", "height": "0", "credit": "", "caption": "This post originally appeared on Quanta Magazine and was published August 21, 2019. This article is republished here with permission.Get math and science news, explainers, interviews and more in your inbox.Get Quanta’s weekly newsletter"}}, "domain_metadata": {"name": "Pocket", "logo": "https://logo.clearbit.com/getpocket.com?size=800", "greyscale_logo": "https://logo.clearbit.com/getpocket.com?size=800&greyscale=true"}, "listen_duration_estimate": 735}, "3143047992": {"item_id": "3143047992", "resolved_id": "3143047992", "given_url": "https://www.newsweek.com/hacked-billboards-can-make-teslas-see-phantom-objects-1539478", "given_title": "", "favorite": "0", "status": "1", "time_added": "1602846527", "time_updated": "1638708525", "time_read": "1604360939", "time_favorited": "0", "sort_id": 2, "resolved_title": "Hacked Billboards Can Make Teslas See 'Phantom Objects,' Causing Them to Swerve or Stop Abruptly", "resolved_url": "https://www.newsweek.com/hacked-billboards-can-make-teslas-see-phantom-objects-1539478", "excerpt": "Security researchers have demonstrated how Tesla's Autopilot driver-assistance systems can be tricked into changing speed, swerving or stopping abruptly, simply by projecting fake road signs or virtual objects in front of them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "465", "lang": "en", "amp_url": "https://www.newsweek.com/hacked-billboards-can-make-teslas-see-phantom-objects-1539478?amp=1", "top_image_url": "https://d.newsweek.com/en/full/1653142/tesla-model-x-electric-car-dashboard.jpg", "tags": {"deep-learning": {"item_id": "3143047992", "tag": "deep-learning"}, "deepfakes": {"item_id": "3143047992", "tag": "deepfakes"}, "vision": {"item_id": "3143047992", "tag": "vision"}}, "authors": {"139995941": {"item_id": "3143047992", "author_id": "139995941", "name": "Aatif Sulleyman", "url": "https://www.newsweek.com/authors/aatif-sulleyman"}}, "domain_metadata": {"name": "Newsweek", "logo": "https://logo.clearbit.com/newsweek.com?size=800", "greyscale_logo": "https://logo.clearbit.com/newsweek.com?size=800&greyscale=true"}, "listen_duration_estimate": 180}, "2838901686": {"item_id": "2838901686", "resolved_id": "2838288110", "given_url": "https://www.reddit.com/r/learnmachinelearning/comments/ehv5r6/dive_really_deep_into_yolo_v3_a_beginners_guide/?utm_source=share&utm_medium=ios_app&utm_name=iossmf", "given_title": "", "favorite": "0", "status": "1", "time_added": "1577813552", "time_updated": "1638708525", "time_read": "1582143051", "time_favorited": "0", "sort_id": 3, "resolved_title": "Dive Really Deep into YOLO v3: A Beginner’s Guide", "resolved_url": "https://www.reddit.com/r/learnmachinelearning/comments/ehv5r6/dive_really_deep_into_yolo_v3_a_beginners_guide/", "excerpt": "I wrote this article recently when I implement YOLO v3 in TF2. My english is not good, though. And I just start to work on object detection this year.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "122", "lang": "en", "amp_url": "https://amp.reddit.com/r/learnmachinelearning/comments/ehv5r6/dive_really_deep_into_yolo_v3_a_beginners_guide/", "top_image_url": "https://external-preview.redd.it/5s3C0SFUxDRlhO68uPtlE26U-uq7RcsLkMEo-KDrDG0.jpg?auto=webp&s=90cb8583e26e487743385c1818e009be1c38bced", "tags": {"deep-learning": {"item_id": "2838901686", "tag": "deep-learning"}, "vision": {"item_id": "2838901686", "tag": "vision"}}, "domain_metadata": {"name": "Reddit", "logo": "https://logo.clearbit.com/reddit.com?size=800", "greyscale_logo": "https://logo.clearbit.com/reddit.com?size=800&greyscale=true"}, "listen_duration_estimate": 47}, "3146889011": {"item_id": "3146889011", "resolved_id": "3146889011", "given_url": "https://www.mpg.de/15916063/1016-nepf-113272-from-fluffy-to-valuable-how-the-brain-recognises-objects", "given_title": "", "favorite": "0", "status": "1", "time_added": "1603145409", "time_updated": "1709152706", "time_read": "1603151026", "time_favorited": "0", "sort_id": 4, "resolved_title": "From fluffy to valuable: How the brain recognises objects", "resolved_url": "https://www.mpg.de/15916063/1016-nepf-113272-from-fluffy-to-valuable-how-the-brain-recognises-objects", "excerpt": "To recognise a chair or a dog, our brain separates objects into their individual properties and then puts them back together. Until recently, it has remained unclear what these properties are.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "801", "lang": "en", "time_to_read": 4, "top_image_url": "https://www.mpg.de/15915627/original-1603200133.jpg", "tags": {"neurology": {"item_id": "3146889011", "tag": "neurology"}, "vision": {"item_id": "3146889011", "tag": "vision"}}, "authors": {"79079088": {"item_id": "3146889011", "author_id": "79079088", "name": "Profile", "url": "https://www.mpg.de/11761628/profile-visions"}}, "image": {"item_id": "3146889011", "src": "https://www.mpg.de/15915627/original-1603200133.jpg?t=eyJ3aWR0aCI6MjQ2LCJvYmpfaWQiOjE1OTE1NjI3fQ%3D%3D--e1fef6b2a9784ae0e1b32739585068c7ba16a22b", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3146889011", "image_id": "1", "src": "https://www.mpg.de/15915627/original-1603200133.jpg?t=eyJ3aWR0aCI6MjQ2LCJvYmpfaWQiOjE1OTE1NjI3fQ%3D%3D--e1fef6b2a9784ae0e1b32739585068c7ba16a22b", "width": "0", "height": "0", "credit": "", "caption": "The human brain breaks down the environment into a total of 49 properties, which are sufficient to categorise all objects. Depending on how similar the observed object is to a known category, it is then recognised as a dog or, for instance, a piece of furniture.\n\n© shutterstock"}}, "listen_duration_estimate": 310}, "3115308139": {"item_id": "3115308139", "resolved_id": "3115308139", "given_url": "https://info.cloudfactory.com/image-annotation-for-computer-vision", "given_title": "", "favorite": "0", "status": "1", "time_added": "1600425848", "time_updated": "1638708525", "time_read": "1604363960", "time_favorited": "0", "sort_id": 5, "resolved_title": "Image Annotation for Computer Vision", "resolved_url": "https://info.cloudfactory.com/image-annotation-for-computer-vision", "excerpt": "Getting images annotated according to your specifications can be a challenge that slows your machine learning or deep learning project and, as a result, your speed to market. The choices you make about your image annotation techniques, tools, and workforce are worth thoughtful consideration.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "95", "lang": "en", "tags": {"deep-learning": {"item_id": "3115308139", "tag": "deep-learning"}, "vision": {"item_id": "3115308139", "tag": "vision"}}, "authors": {"5266235": {"item_id": "3115308139", "author_id": "5266235", "name": "CloudFactory", "url": ""}}, "listen_duration_estimate": 37}, "2754998011": {"item_id": "2754998011", "resolved_id": "2754888520", "given_url": "https://www.technologyreview.com/f/614551/ai-computer-vision-algorithms-on-your-phone-mit-ibm/?utm_medium=tr_social&utm_campaign=site_visitor.unpaid.engagement&utm_source=LinkedIn#Echobox=1586193065", "given_title": "", "favorite": "0", "status": "1", "time_added": "1586216092", "time_updated": "1638708525", "time_read": "1589542127", "time_favorited": "0", "sort_id": 6, "resolved_title": "Powerful computer vision algorithms are now small enough to run on your phone", "resolved_url": "https://www.technologyreview.com/f/614551/ai-computer-vision-algorithms-on-your-phone-mit-ibm/", "excerpt": "Researchers have shrunk state-of-the-art computer vision models to run on low-power devices. Growing pains: Visual recognition is deep learning’s strongest skill. Computer vision algorithms are analyzing medical images, enabling self-driving cars, and powering face recognition.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "359", "lang": "en", "top_image_url": "https://cdn.technologyreview.com/i/images/tinymachinevision-01-01.png?sw=1200&cx=0&cy=0&cw=3200&ch=1800", "tags": {"deep-learning": {"item_id": "2754998011", "tag": "deep-learning"}, "semiconductors": {"item_id": "2754998011", "tag": "semiconductors"}, "vision": {"item_id": "2754998011", "tag": "vision"}}, "authors": {"96772099": {"item_id": "2754998011", "author_id": "96772099", "name": "Karen Hao", "url": "https://www.technologyreview.com/profile/karen-hao/"}}, "domain_metadata": {"name": "MIT Technology Review", "logo": "https://logo.clearbit.com/technologyreview.com?size=800", "greyscale_logo": "https://logo.clearbit.com/technologyreview.com?size=800&greyscale=true"}, "listen_duration_estimate": 139}, "2754888520": {"item_id": "2754888520", "resolved_id": "2754888520", "given_url": "https://www.technologyreview.com/f/614551/ai-computer-vision-algorithms-on-your-phone-mit-ibm/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1574511240", "time_updated": "1638708525", "time_read": "1574616600", "time_favorited": "0", "sort_id": 7, "resolved_title": "Powerful computer vision algorithms are now small enough to run on your phone", "resolved_url": "https://www.technologyreview.com/f/614551/ai-computer-vision-algorithms-on-your-phone-mit-ibm/", "excerpt": "Researchers have shrunk state-of-the-art computer vision models to run on low-power devices. Growing pains: Visual recognition is deep learning’s strongest skill. Computer vision algorithms are analyzing medical images, enabling self-driving cars, and powering face recognition.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "359", "lang": "en", "top_image_url": "https://cdn.technologyreview.com/i/images/tinymachinevision-01-01.png?sw=1200&cx=0&cy=0&cw=3200&ch=1800", "tags": {"deep-learning": {"item_id": "2754888520", "tag": "deep-learning"}, "mobile": {"item_id": "2754888520", "tag": "mobile"}, "vision": {"item_id": "2754888520", "tag": "vision"}}, "authors": {"96772099": {"item_id": "2754888520", "author_id": "96772099", "name": "Karen Hao", "url": "https://www.technologyreview.com/profile/karen-hao/"}}, "domain_metadata": {"name": "MIT Technology Review", "logo": "https://logo.clearbit.com/technologyreview.com?size=800", "greyscale_logo": "https://logo.clearbit.com/technologyreview.com?size=800&greyscale=true"}, "listen_duration_estimate": 139}, "3103256197": {"item_id": "3103256197", "resolved_id": "3028725521", "given_url": "https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/?utm_campaign=end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists&utm_medium=social_link&utm_source=missinglettr", "given_title": "", "favorite": "0", "status": "1", "time_added": "1599414945", "time_updated": "1638708525", "time_read": "1604363722", "time_favorited": "0", "sort_id": 8, "resolved_title": "End to End Pipeline for setting up Multiclass Image Classification for Data Scientists", "resolved_url": "https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/", "excerpt": "Have you ever wondered how Facebook takes care of the abusive and inappropriate images shared by some of its users? Or how Facebook’s tagging feature works? Or how Google Lens recognizes products through images? All of the above are examples of image classification in different settings.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3891", "lang": "en", "time_to_read": 18, "top_image_url": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/main.png", "tags": {"deep-learning": {"item_id": "3103256197", "tag": "deep-learning"}, "vision": {"item_id": "3103256197", "tag": "vision"}}, "authors": {"8623619": {"item_id": "3103256197", "author_id": "8623619", "name": "Rahul Agarwal", "url": ""}}, "image": {"item_id": "3103256197", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/0.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3103256197", "image_id": "1", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/0.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3103256197", "image_id": "2", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/1.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3103256197", "image_id": "3", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3103256197", "image_id": "4", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3103256197", "image_id": "5", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/4.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "3103256197", "image_id": "6", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/5.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "3103256197", "image_id": "7", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/6.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "3103256197", "image_id": "8", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/7.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "3103256197", "image_id": "9", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/8.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "3103256197", "image_id": "10", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/9.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "3103256197", "image_id": "11", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/10.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "3103256197", "image_id": "12", "src": "https://mlwhiz.com/images/multiclass_image_classification_pytorch/11.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1506}, "2644506040": {"item_id": "2644506040", "resolved_id": "2644506040", "given_url": "https://www.quantamagazine.org/where-we-see-shapes-ai-sees-textures-20190701/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1595176572", "time_updated": "1638708525", "time_read": "1597023109", "time_favorited": "0", "sort_id": 9, "resolved_title": "Where We See Shapes, AI Sees Textures", "resolved_url": "https://www.quantamagazine.org/where-we-see-shapes-ai-sees-textures-20190701/", "excerpt": "To researchers’ surprise, deep learning vision algorithms often fail at classifying images because they mostly take cues from textures, not shapes.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1921", "lang": "en", "time_to_read": 9, "top_image_url": "https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/07/AI_Textures_1200_social.jpg", "tags": {"deep-learning": {"item_id": "2644506040", "tag": "deep-learning"}, "vision": {"item_id": "2644506040", "tag": "vision"}}, "authors": {"71944334": {"item_id": "2644506040", "author_id": "71944334", "name": "Jordana Cepelewicz", "url": "https://www.quantamagazine.org/authors/jordana-cepelewicz/"}}, "image": {"item_id": "2644506040", "src": "https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/07/AI_Textures_2880x1220_LHPA.jpg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2644506040", "image_id": "1", "src": "https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/07/AI_Textures_2880x1220_LHPA.jpg", "width": "0", "height": "0", "credit": "", "caption": "To make deep learning algorithms use shapes to identify objects, as humans do, researchers trained the systems with images that had been “painted” with irrelevant textures. The systems’ performance improved, a result that may hold clues about the evolution of our own vision.\n\n\nCourtesy of Robert Geirhos"}}, "domain_metadata": {"name": "Quanta Magazine", "logo": "https://logo.clearbit.com/quantamagazine.org?size=800", "greyscale_logo": "https://logo.clearbit.com/quantamagazine.org?size=800&greyscale=true"}, "listen_duration_estimate": 744}, "3184244689": {"item_id": "3184244689", "resolved_id": "3184244689", "given_url": "https://lionbridge.ai/articles/everything-you-need-to-know-about-object-detection-systems/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1606404966", "time_updated": "1638708525", "time_read": "1608290468", "time_favorited": "0", "sort_id": 10, "resolved_title": "Everything You Need to Know About Object Detection Systems", "resolved_url": "https://lionbridge.ai/articles/everything-you-need-to-know-about-object-detection-systems/", "excerpt": "With the advent of deep learning, implementing an object detection system has become fairly trivial. There are a great many frameworks facilitating the process, and as I showed in a previous post, it’s quite easy to create a fast object detection model with YOLOv5.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3902", "lang": "en", "time_to_read": 18, "top_image_url": "https://lionbridge.ai/wp-content/uploads/2020/11/2020-11-20_object-detection.jpg", "tags": {"deep-learning": {"item_id": "3184244689", "tag": "deep-learning"}, "object-detection": {"item_id": "3184244689", "tag": "object-detection"}, "vision": {"item_id": "3184244689", "tag": "vision"}}, "authors": {"8623619": {"item_id": "3184244689", "author_id": "8623619", "name": "Rahul Agarwal", "url": ""}}, "image": {"item_id": "3184244689", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/01-1.png", "width": "700", "height": "295"}, "images": {"1": {"item_id": "3184244689", "image_id": "1", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/01-1.png", "width": "700", "height": "295", "credit": "", "caption": "http://cs231n.github.io/transfer-learning/#tf"}, "2": {"item_id": "3184244689", "image_id": "2", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/02-1.png", "width": "700", "height": "352", "credit": "", "caption": "Source: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf"}, "3": {"item_id": "3184244689", "image_id": "3", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/03-1.png", "width": "700", "height": "253", "credit": "", "caption": "And example of efficient graph-based image segmentation. Source: http://people.cs.uchicago.edu/~pff/papers/seg-ijcv.pdf"}, "4": {"item_id": "3184244689", "image_id": "4", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/05-1.png", "width": "700", "height": "614", "credit": "", "caption": "The algorithm for region proposal used in RCNN"}, "5": {"item_id": "3184244689", "image_id": "5", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/06-1.png", "width": "700", "height": "359", "credit": "", "caption": ""}, "6": {"item_id": "3184244689", "image_id": "6", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/07.jpeg", "width": "700", "height": "496", "credit": "", "caption": "Source: https://www.pyimagesearch.com/wp-content/uploads/2014/10/hog_object_detection_nms.jpg"}, "7": {"item_id": "3184244689", "image_id": "7", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/08-1.png", "width": "700", "height": "352", "credit": "", "caption": ""}, "8": {"item_id": "3184244689", "image_id": "8", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/09-1.png", "width": "470", "height": "276", "credit": "", "caption": "VGG 16 Architecture"}, "9": {"item_id": "3184244689", "image_id": "9", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/10.png", "width": "700", "height": "393", "credit": "", "caption": "We need fixed-sized feature maps for the final classifier."}, "10": {"item_id": "3184244689", "image_id": "10", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/11.gif", "width": "700", "height": "525", "credit": "", "caption": "Source: https://deepsense.ai/region-of-interest-pooling-explained/"}, "11": {"item_id": "3184244689", "image_id": "11", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/13.png", "width": "700", "height": "432", "credit": "", "caption": "Runtime dominated by region proposals!"}, "12": {"item_id": "3184244689", "image_id": "12", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/14.png", "width": "700", "height": "627", "credit": "", "caption": ""}, "13": {"item_id": "3184244689", "image_id": "13", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/15.png", "width": "700", "height": "525", "credit": "", "caption": "Anchor centers throughout the original image."}, "14": {"item_id": "3184244689", "image_id": "14", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/16.png", "width": "700", "height": "253", "credit": "", "caption": "Left: Anchors, Center: Anchor for a single point, Right: All anchors"}, "15": {"item_id": "3184244689", "image_id": "15", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/17.jpeg", "width": "638", "height": "359", "credit": "", "caption": "Results on VOC Dataset for the three different approaches."}, "16": {"item_id": "3184244689", "image_id": "16", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/1_4Q56usZkTr_003z6CnCKhw.png", "width": "1000", "height": "247", "credit": "", "caption": ""}, "17": {"item_id": "3184244689", "image_id": "17", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/18.png", "width": "700", "height": "462", "credit": "", "caption": ""}, "18": {"item_id": "3184244689", "image_id": "18", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/19.png", "width": "700", "height": "376", "credit": "", "caption": "Some images with masks from this paper: https://arxiv.org/abs/1703.06870"}, "19": {"item_id": "3184244689", "image_id": "19", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/20.png", "width": "700", "height": "230", "credit": "", "caption": "Everything remains the same. Just one more output layer to predict masks and ROI pooling replaced by ROIAlign"}, "20": {"item_id": "3184244689", "image_id": "20", "src": "https://lionbridge.ai/wp-content/uploads/2020/11/21.png", "width": "700", "height": "550", "credit": "", "caption": "Source: https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272"}}, "listen_duration_estimate": 1510}, "3030373028": {"item_id": "3030373028", "resolved_id": "3030373028", "given_url": "https://getpocket.com/explore/item/why-red-means-red-in-almost-every-language", "given_title": "", "favorite": "0", "status": "1", "time_added": "1594933457", "time_updated": "1709152706", "time_read": "1597023226", "time_favorited": "0", "sort_id": 11, "resolved_title": "Why Red Means Red in Almost Every Language", "resolved_url": "https://getpocket.com/explore/item/why-red-means-red-in-almost-every-language", "excerpt": "The confounding consistency of color categories.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2412", "lang": "en", "time_to_read": 11, "top_image_url": "https://pocket-image-cache.com/1200x/filters:format(jpg):extract_focal()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2Farticles%2F4877%2F1593177446_ezgif.com-webp-to-jpg153.jpg", "tags": {"neurology": {"item_id": "3030373028", "tag": "neurology"}, "vision": {"item_id": "3030373028", "tag": "vision"}}, "authors": {"98812": {"item_id": "3030373028", "author_id": "98812", "name": "Chelsea Wald", "url": ""}}, "image": {"item_id": "3030373028", "src": "https://pocket-syndicated-images.s3.amazonaws.com/5ef5f55334db1.jpg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3030373028", "image_id": "1", "src": "https://pocket-syndicated-images.s3.amazonaws.com/5ef5f55334db1.jpg", "width": "0", "height": "0", "credit": "", "caption": "Illustration by Francesco Izzo."}, "2": {"item_id": "3030373028", "image_id": "2", "src": "https://pocket-image-cache.com/direct?resize=w2000&url=http%3A%2F%2Fstatic.nautil.us%2F6800_21e60123a3a0df92f391f66b1e51903a.png", "width": "0", "height": "0", "credit": "as shown in the wheel on the left", "caption": "What’s in a Name?: When shown a wheel of similarly colored squares, people identify the offbeat shade more quickly if it comes from a different color category"}, "3": {"item_id": "3030373028", "image_id": "3", "src": "https://pocket-image-cache.com/direct?resize=w2000&url=http%3A%2F%2Fstatic.nautil.us%2F6801_1fb333bc34b8d1f1d1d434f90869367a.jpg", "width": "0", "height": "0", "credit": "", "caption": "Baby Blues: In laboratory tests, psychologists have found that babies tend to stare longer at colors that from categories they haven’t seen before, suggesting that our ability to perceive boundaries between colors may exist as early as infancy. Photo courtesy of Sussex Baby Lab."}, "4": {"item_id": "3030373028", "image_id": "4", "src": "https://pocket-syndicated-images.s3.amazonaws.com/5ef5f6233f770.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3030373028", "image_id": "5", "src": "https://pocket-syndicated-publisher-logos.s3.amazonaws.com/5cba00f3b7a1e.png", "width": "0", "height": "0", "credit": "", "caption": "This post originally appeared on Nautilus and was published October 3, 2019. This article is republished here with permission.Did you enjoy this story?Subscribe to Nautilus"}}, "domain_metadata": {"name": "Pocket", "logo": "https://logo.clearbit.com/getpocket.com?size=800", "greyscale_logo": "https://logo.clearbit.com/getpocket.com?size=800&greyscale=true"}, "listen_duration_estimate": 934}, "3013222001": {"item_id": "3013222001", "resolved_id": "3013222001", "given_url": "https://blog.roboflow.ai/yolov5-is-here/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1591816496", "time_updated": "1638708525", "time_read": "1593020571", "time_favorited": "0", "sort_id": 12, "resolved_title": "YOLOv5 is Here: State-of-the-Art Object Detection at 140 FPS", "resolved_url": "https://blog.roboflow.ai/yolov5-is-here/", "excerpt": "Less than 50 days after the release YOLOv4, YOLOv5 improves accessibility for realtime object detection. June 29, YOLOv5 has released the first official version of the repository. We wrote a new deep dive on YOLOv5.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "835", "lang": "en", "time_to_read": 4, "amp_url": "https://blog.roboflow.ai/yolov5-is-here/amp/", "top_image_url": "https://blog.roboflow.ai/content/images/2020/06/ezgif.com-video-to-gif--3--1.gif", "tags": {"deep-learning": {"item_id": "3013222001", "tag": "deep-learning"}, "vision": {"item_id": "3013222001", "tag": "vision"}}, "authors": {"128257915": {"item_id": "3013222001", "author_id": "128257915", "name": "Joseph Nelson", "url": "https://blog.roboflow.ai/author/joseph/"}, "133025027": {"item_id": "3013222001", "author_id": "133025027", "name": "Jacob Solawetz", "url": "https://blog.roboflow.ai/author/jacob/"}}, "image": {"item_id": "3013222001", "src": "https://blog.roboflow.ai/content/images/2020/06/yolov4-results.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3013222001", "image_id": "1", "src": "https://blog.roboflow.ai/content/images/2020/06/yolov4-results.png", "width": "0", "height": "0", "credit": "", "caption": "Image via the YOLOv4 paper."}, "2": {"item_id": "3013222001", "image_id": "2", "src": "https://blog.roboflow.ai/content/images/2020/06/image-51.png", "width": "0", "height": "0", "credit": "", "caption": "Our YOLOv5 weights file stored in S3 for future inference."}, "3": {"item_id": "3013222001", "image_id": "3", "src": "https://blog.roboflow.ai/content/images/2020/06/yolov5-performance.png", "width": "0", "height": "0", "credit": "", "caption": "YOLO is more accurate and faster than EfficientDet. Credit: Glenn Jocher"}}, "listen_duration_estimate": 323}, "2130262921": {"item_id": "2130262921", "resolved_id": "2130262921", "given_url": "https://deepmind.com/blog/learning-to-generate-images/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1522174137", "time_updated": "1638708525", "time_read": "1528501230", "time_favorited": "0", "sort_id": 13, "resolved_title": "Learning to write programs that generate images", "resolved_url": "https://deepmind.com/blog/learning-to-generate-images/", "excerpt": "Through a human’s eyes, the world is much more than just the images reflected in our corneas. For example, when we look at a building and admire the intricacies of its design, we can appreciate the craftsmanship it requires.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "729", "lang": "en", "time_to_read": 3, "top_image_url": "https://lh3.googleusercontent.com/FhVFQi0vU4ChnmSidjJFjAYJ1uguGjvM699QhzHnN2LB9_jvebxYGAFXqFKiLx9jMmHceHZXgioPr_hFbEHYilmcYq31hyUgsbTF", "tags": {"deep-learning": {"item_id": "2130262921", "tag": "deep-learning"}, "image-generation": {"item_id": "2130262921", "tag": "image-generation"}, "vision": {"item_id": "2130262921", "tag": "vision"}}, "image": {"item_id": "2130262921", "src": "https://lh3.googleusercontent.com/FhVFQi0vU4ChnmSidjJFjAYJ1uguGjvM699QhzHnN2LB9_jvebxYGAFXqFKiLx9jMmHceHZXgioPr_hFbEHYilmcYq31hyUgsbTF=w1440", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2130262921", "image_id": "1", "src": "https://lh3.googleusercontent.com/FhVFQi0vU4ChnmSidjJFjAYJ1uguGjvM699QhzHnN2LB9_jvebxYGAFXqFKiLx9jMmHceHZXgioPr_hFbEHYilmcYq31hyUgsbTF=w1440", "width": "0", "height": "0", "credit": "", "caption": "Blog post"}, "2": {"item_id": "2130262921", "image_id": "2", "src": "https://lh3.googleusercontent.com/892tLzmH606L6BsDLDdhUUnK6dEoUFHPaU9BVWMqTBkW_4cW_b4D364aeOY8UbVTPFmHAHAqEpCwwXkMCKKQwad-CC5Il-Xx6Tcx0A=w1440", "width": "0", "height": "0", "credit": "Shutterstock", "caption": ""}, "3": {"item_id": "2130262921", "image_id": "3", "src": "https://lh3.googleusercontent.com/YgQyDVc33nRcETixZAqQm3qUdoDR0-_6D48cktlXYuWvCJPdF21mquS8n-9vDkREVuU_ck4bwEdGz7MZvNM_Agw6zUyb7uIi8XQE=w1440", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2130262921", "image_id": "4", "src": "https://lh3.googleusercontent.com/U0qP8uUeL9fizBeHFbYEohk93OrbX_yE0jwt91xwO3P1CqvvRS4Pz-gZnX8Gn6sZjiJev1Te7MYid6H-jT3CpsRrGA-C8sdSWhuZpQ=w1440", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2130262921", "image_id": "5", "src": "https://lh3.googleusercontent.com/kG1U-2HgvhO0JW4u80vv6wGVsvs_Li4NwUIuIt7IkEa45lGmNrN7cp-GUnlszl1U80nbj_GnaAM2MitVqwBTP87sndRV-i3m1AN-qw=w1440", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 282}, "3543437705": {"item_id": "3543437705", "resolved_id": "3543437705", "given_url": "https://getpocket.com/explore/item/everything-we-see-is-a-mash-up-of-the-brain-s-last-15-seconds-of-visual-information", "given_title": "", "favorite": "0", "status": "1", "time_added": "1644963924", "time_updated": "1709152706", "time_read": "1645316347", "time_favorited": "0", "sort_id": 14, "resolved_title": "Everything We See Is a Mash-up of the Brain’s Last 15 Seconds of Visual Information", "resolved_url": "https://getpocket.com/explore/item/everything-we-see-is-a-mash-up-of-the-brain-s-last-15-seconds-of-visual-information", "excerpt": "The brain is basically a time machine that ensures what we see is stable and continuous.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "916", "lang": "en", "time_to_read": 4, "top_image_url": "https://pocket-image-cache.com/1200x/filters:format(jpg):extract_focal()/https%3A%2F%2Fpocket-syndicated-images.s3.amazonaws.com%2Farticles%2F7493%2F1643860013_ScreenShot2022-02-02at7.42.07PM.png", "tags": {"memory-recall": {"item_id": "3543437705", "tag": "memory-recall"}, "neurology": {"item_id": "3543437705", "tag": "neurology"}, "vision": {"item_id": "3543437705", "tag": "vision"}}, "authors": {"3991917": {"item_id": "3543437705", "author_id": "3991917", "name": "David Whitney", "url": ""}, "96141152": {"item_id": "3543437705", "author_id": "96141152", "name": "Mauro Manassi", "url": ""}}, "image": {"item_id": "3543437705", "src": "https://pocket-image-cache.com/direct?resize=w2000&url=https%3A%2F%2Fimages.theconversation.com%2Ffiles%2F442574%2Foriginal%2Ffile-20220125-15-123wgyh.jpg%3Fixlib%3Drb-1.1.0%26rect%3D3%252C50%252C2582%252C2562%26q%3D20%26auto%3Dformat%26w%3D320%26fit%3Dclip%26dpr%3D2%26usm%3D12%26cs%3Dstrip", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3543437705", "image_id": "1", "src": "https://pocket-image-cache.com/direct?resize=w2000&url=https%3A%2F%2Fimages.theconversation.com%2Ffiles%2F442574%2Foriginal%2Ffile-20220125-15-123wgyh.jpg%3Fixlib%3Drb-1.1.0%26rect%3D3%252C50%252C2582%252C2562%26q%3D20%26auto%3Dformat%26w%3D320%26fit%3Dclip%26dpr%3D2%26usm%3D12%26cs%3Dstrip", "width": "0", "height": "0", "credit": "", "caption": "Ryger/Shutterstock"}, "2": {"item_id": "3543437705", "image_id": "2", "src": "https://pocket-syndicated-publisher-logos.s3.amazonaws.com/5e3898115340d.png", "width": "0", "height": "0", "credit": "", "caption": "This post originally appeared on The Conversation and was published January 26, 2022. This article is republished here with permission.The Conversation brings you insights and analysis from academic experts, straight to your inbox.Get the daily newsletter"}}, "domain_metadata": {"name": "Pocket", "logo": "https://logo.clearbit.com/getpocket.com?size=800", "greyscale_logo": "https://logo.clearbit.com/getpocket.com?size=800&greyscale=true"}, "listen_duration_estimate": 355}, "3634346418": {"item_id": "3634346418", "resolved_id": "3634346418", "given_url": "https://www.nytimes.com/2022/06/06/science/optical-illusion-tunnel.html", "given_title": "", "favorite": "0", "status": "1", "time_added": "1654775562", "time_updated": "1709152706", "time_read": "1654982647", "time_favorited": "0", "sort_id": 15, "resolved_title": "This Optical Illusion Has a Revelation About Your Brain and Eyes", "resolved_url": "https://www.nytimes.com/2022/06/06/science/optical-illusion-tunnel.html", "excerpt": "Do not be alarmed. The hole you see is not really moving, growing or expanding. The darkness will not swallow you.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "870", "lang": "en", "time_to_read": 4, "top_image_url": "https://static01.nyt.com/images/2022/06/07/science/06tb-illusion1/06tb-illusion1-facebookJumbo.jpg?year=2022&h=550&w=1050&s=3e435228fb5763866c1d2828aa0a7e7cbf70402151c977ebaef8143b17385836&k=ZQJBKqZ0VN", "tags": {"neurology": {"item_id": "3634346418", "tag": "neurology"}, "vision": {"item_id": "3634346418", "tag": "vision"}}, "authors": {"168128187": {"item_id": "3634346418", "author_id": "168128187", "name": "Richard Sima", "url": "https://www.nytimes.com/by/richard-sima"}}, "image": {"item_id": "3634346418", "src": "https://static01.nyt.com/images/2022/06/07/science/06tb-illusion1/merlin_208158213_7de04af1-5e13-406c-9437-2d8ea41aabc8-articleLarge.jpg?quality=75&auto=webp&disable=upscale", "width": "600", "height": "376"}, "images": {"1": {"item_id": "3634346418", "image_id": "1", "src": "https://static01.nyt.com/images/2022/06/07/science/06tb-illusion1/merlin_208158213_7de04af1-5e13-406c-9437-2d8ea41aabc8-articleLarge.jpg?quality=75&auto=webp&disable=upscale", "width": "600", "height": "376", "credit": "", "caption": ""}}, "domain_metadata": {"name": "The New York Times", "logo": "https://logo.clearbit.com/nytimes.com?size=800", "greyscale_logo": "https://logo.clearbit.com/nytimes.com?size=800&greyscale=true"}, "listen_duration_estimate": 337}, "3646795413": {"item_id": "3646795413", "resolved_id": "3646795413", "given_url": "https://www.wired.com/story/the-dress-neuroscience-breakthrough/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1656720069", "time_updated": "1709152706", "time_read": "1656779842", "time_favorited": "0", "sort_id": 16, "resolved_title": "How ‘The Dress’ Sparked a Neuroscience Breakthrough", "resolved_url": "https://www.wired.com/story/the-dress-neuroscience-breakthrough/", "excerpt": "Back in 2015, before Brexit, before Trump, before Macedonian internet trolls, before QAnon and Covid conspiracy theories, before fake news and alternative facts, the disagreement over the Dress was described by one NPR affiliate as “the debate that broke the internet.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2198", "lang": "en", "time_to_read": 10, "amp_url": "https://www.wired.com/story/the-dress-neuroscience-breakthrough/amp", "top_image_url": "https://media.wired.com/photos/62b515e5d8b7da343bdd1019/191:100/w_1280,c_limit/ideas-neuroscience.jpg", "tags": {"neurology": {"item_id": "3646795413", "tag": "neurology"}, "physics": {"item_id": "3646795413", "tag": "physics"}, "vision": {"item_id": "3646795413", "tag": "vision"}}, "authors": {"1562946": {"item_id": "3646795413", "author_id": "1562946", "name": "David McRaney.", "url": ""}}, "image": {"item_id": "3646795413", "src": "https://media.wired.com/photos/62b515e5d8b7da343bdd1019/master/w_2560%2Cc_limit/ideas-neuroscience.jpg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3646795413", "image_id": "1", "src": "https://media.wired.com/photos/62b515e5d8b7da343bdd1019/master/w_2560%2Cc_limit/ideas-neuroscience.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3646795413", "image_id": "2", "src": "https://media.wired.com/photos/62b4bfdae6d36c0218cca4eb/master/w_1600%2Cc_limit/HOW-MINDS-CHANGE-cover-Ideas.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "WIRED", "logo": "https://logo.clearbit.com/wired.com?size=800", "greyscale_logo": "https://logo.clearbit.com/wired.com?size=800&greyscale=true"}, "listen_duration_estimate": 851}, "3805713618": {"item_id": "3805713618", "resolved_id": "3805713618", "given_url": "https://thereader.mitpress.mit.edu/the-art-of-the-shadow-how-painters-have-gotten-it-wrong-for-centuries/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1677339827", "time_updated": "1709152706", "time_read": "1677353459", "time_favorited": "0", "sort_id": 17, "resolved_title": "The Art of the Shadow: How Painters Have Gotten It Wrong for Centuries", "resolved_url": "https://thereader.mitpress.mit.edu/the-art-of-the-shadow-how-painters-have-gotten-it-wrong-for-centuries/", "excerpt": "Shadows do not continue from the floor up the wall on the right. Piero della Francesca, Polittico di Sant’Antonio (detail), 1460–1470. Perugia, Galleria Nazionale dell’Umbria. Image credit: © Galleria Nazionale dell’Umbria.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "4378", "lang": "en", "time_to_read": 20, "top_image_url": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Lead.jpg", "tags": {"art": {"item_id": "3805713618", "tag": "art"}, "neurology": {"item_id": "3805713618", "tag": "neurology"}, "vision": {"item_id": "3805713618", "tag": "vision"}}, "authors": {"12027712": {"item_id": "3805713618", "author_id": "12027712", "name": "Roberto Casati", "url": ""}}, "image": {"item_id": "3805713618", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Lead-700x420.jpg", "width": "700", "height": "420"}, "images": {"1": {"item_id": "3805713618", "image_id": "1", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Lead-700x420.jpg", "width": "700", "height": "420", "credit": "", "caption": ""}, "2": {"item_id": "3805713618", "image_id": "2", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/The-Visual-World-of-Shadows.jpg", "width": "320", "height": "359", "credit": "", "caption": "This article is adapted from Roberto Casati and Patrick Cavanagh’s book “The Visual World of Shadows“"}, "3": {"item_id": "3805713618", "image_id": "3", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Reutersvard-Penrose-triangle.jpg", "width": "300", "height": "265", "credit": "", "caption": "The Reutersvärd-Penrose triangle is quickly seen as being an impossible solid body. Image credit: Roberto Casati."}, "4": {"item_id": "3805713618", "image_id": "4", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-2-700x470.jpg", "width": "700", "height": "470", "credit": "", "caption": "Fig. 2 / A CAD-generated model of Masaccio’s “Tribute.” The shadows climb the bodies of the characters. Image credit: Meeko Kuwahara."}, "5": {"item_id": "3805713618", "image_id": "5", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Carpet-shadows-300x351.jpg", "width": "300", "height": "351", "credit": "", "caption": "Carpet shadows pass under objects as if the objects were not present."}, "6": {"item_id": "3805713618", "image_id": "6", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Shadow.jpg", "width": "382", "height": "447", "credit": "", "caption": "It is unlikely that a shadow ends abruptly precisely where there is a change in surface direction."}, "7": {"item_id": "3805713618", "image_id": "7", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-6-700x320.jpg", "width": "700", "height": "320", "credit": "right", "caption": "Fig. 6 / Shadows bend around corners if the object is in the appropriate location relative to the light source and surface. However, after bending, shadows are largely deformed and generally elongated"}, "8": {"item_id": "3805713618", "image_id": "8", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-10-300x568.jpg", "width": "300", "height": "568", "credit": "–1516", "caption": "Fig. 10 / The man’s shadow only begins on the wall, without passing along the ground to get there. Biagio d’Antonio"}, "9": {"item_id": "3805713618", "image_id": "9", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-12.jpg", "width": "511", "height": "349", "credit": "", "caption": "Fig. 12 / Detail from Konrad Witz’s “The Deliverance of Saint Peter”"}, "10": {"item_id": "3805713618", "image_id": "10", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Shadow-around-the-bend-326x320.jpg", "width": "326", "height": "320", "credit": "", "caption": "When in doubt, check the world: Shadows that may appear odd in a painting could turn out to occur in reality. Image credit: Roberto Casati, Paris, September 2018."}, "11": {"item_id": "3805713618", "image_id": "11", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-16-700x320.jpg", "width": "700", "height": "320", "credit": "ca. 1375–1444", "caption": "Fig. 15 / Light from two openings on the left-hand wall creates double shadows, whose intersection is triangular. Robert Campin"}, "12": {"item_id": "3805713618", "image_id": "12", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-17.jpg", "width": "667", "height": "649", "credit": "1470–1525", "caption": "Fig. 16 / Girolamo di Benvenuto"}, "13": {"item_id": "3805713618", "image_id": "13", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/19-and-20-700x320.jpg", "width": "700", "height": "320", "credit": "left", "caption": "Fig. 18"}, "14": {"item_id": "3805713618", "image_id": "14", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/21-and-22-700x320.jpg", "width": "700", "height": "320", "credit": "left", "caption": "Fig. 20"}, "15": {"item_id": "3805713618", "image_id": "15", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-25-700x310.jpg", "width": "700", "height": "310", "credit": "1450–1523", "caption": "Fig. 24 / Shadow chaos. Items that are parallel should cast parallel shadows. Arrows of identical color indicate the direction of shadows from legs or other items that are reasonably parallel in the scene. Luca Signorelli"}, "16": {"item_id": "3805713618", "image_id": "16", "src": "https://thereader.mitpress.mit.edu/wp-content/uploads/2023/02/Fig-28-new.jpg", "width": "414", "height": "599", "credit": "1480–1556", "caption": "Fig. 27 / An angel can, after all, obstruct light. Lorenzo Lotto"}}, "listen_duration_estimate": 1695}, "3834543227": {"item_id": "3834543227", "resolved_id": "3834543227", "given_url": "https://getpocket.com/explore/item/my-left-and-right-eyes-see-slightly-different-colors-is-that-normal", "given_title": "", "favorite": "0", "status": "1", "time_added": "1680178163", "time_updated": "1709152706", "time_read": "1680225423", "time_favorited": "0", "sort_id": 18, "resolved_title": "", "resolved_url": "https://getpocket.com/explore/item/my-left-and-right-eyes-see-slightly-different-colors-is-that-normal", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"neurology": {"item_id": "3834543227", "tag": "neurology"}, "vision": {"item_id": "3834543227", "tag": "vision"}}, "domain_metadata": {"name": "Pocket", "logo": "https://logo.clearbit.com/getpocket.com?size=800", "greyscale_logo": "https://logo.clearbit.com/getpocket.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "2948496375": {"item_id": "2948496375", "resolved_id": "2948496375", "given_url": "https://towardsdatascience.com/google-open-sources-simclr-a-framework-for-self-supervised-and-semi-supervised-image-training-72b06d5d58a0", "given_title": "", "favorite": "0", "status": "1", "time_added": "1587997825", "time_updated": "1638708525", "time_read": "1588026402", "time_favorited": "0", "sort_id": 19, "resolved_title": "", "resolved_url": "https://towardsdatascience.com/google-open-sources-simclr-a-framework-for-self-supervised-and-semi-supervised-image-training-72b06d5d58a0", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "en", "tags": {"deep-learning": {"item_id": "2948496375", "tag": "deep-learning"}, "vision": {"item_id": "2948496375", "tag": "vision"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "1617384831": {"item_id": "1617384831", "resolved_id": "1617384831", "given_url": "https://arxiv.org/abs/1702.04680v1", "given_title": "[1702.04680v1] Visual Discovery at Pinterest", "favorite": "0", "status": "1", "time_added": "1671647709", "time_updated": "1671648672", "time_read": "1671648672", "time_favorited": "0", "sort_id": 20, "resolved_title": "Title:Visual Discovery at Pinterest", "resolved_url": "https://arxiv.org/abs/1702.04680v1", "excerpt": "Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "77", "lang": "en", "top_image_url": "https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png", "tags": {"arxiv": {"item_id": "1617384831", "tag": "arxiv"}, "discovery": {"item_id": "1617384831", "tag": "discovery"}, "images": {"item_id": "1617384831", "tag": "images"}, "pinterest": {"item_id": "1617384831", "tag": "pinterest"}, "search": {"item_id": "1617384831", "tag": "search"}, "vision": {"item_id": "1617384831", "tag": "vision"}}, "authors": {"63380980": {"item_id": "1617384831", "author_id": "63380980", "name": "cs", "url": "https://arxiv.org/abs/1702.04680?context=cs"}}, "domain_metadata": {"name": "arXiv", "logo": "https://logo.clearbit.com/arxiv.org?size=800", "greyscale_logo": "https://logo.clearbit.com/arxiv.org?size=800&greyscale=true"}, "listen_duration_estimate": 30}, "2622604741": {"item_id": "2622604741", "resolved_id": "2622604741", "given_url": "https://lionbridge.ai/datasets/5-million-faces-top-15-free-image-datasets-for-facial-recognition/", "given_title": "5 Million Faces — Free Image Datasets for Facial Recognition | Lionbridge A", "favorite": "0", "status": "1", "time_added": "1596658522", "time_updated": "1638708525", "time_read": "1606680658", "time_favorited": "0", "sort_id": 21, "resolved_title": "5 Million Faces — Top 15 Free Image Datasets for Facial Recognition", "resolved_url": "https://lionbridge.ai/datasets/5-million-faces-top-15-free-image-datasets-for-facial-recognition/", "excerpt": "From mobile phone security and surveillance cameras to augmented reality and photography, the facial recognition branch of computer vision has a variety of useful applications.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "747", "lang": "en", "time_to_read": 3, "top_image_url": "https://lionbridge.ai/wp-content/uploads/2019/06/2019-06-07_top-10-free-image-dataset-facial-recognition-hero.jpg", "tags": {"datasets": {"item_id": "2622604741", "tag": "datasets"}, "deep-learning": {"item_id": "2622604741", "tag": "deep-learning"}, "vision": {"item_id": "2622604741", "tag": "vision"}}, "authors": {"89330520": {"item_id": "2622604741", "author_id": "89330520", "name": "Limarc Ambalina", "url": ""}}, "listen_duration_estimate": 289}, "2973401798": {"item_id": "2973401798", "resolved_id": "2973399047", "given_url": "https://towardsdatascience.com/a-deep-dive-into-lane-detection-with-hough-transform-8f90fdd1322f?source=rss----7f60cf5620c9---4", "given_title": "A Deep Dive into Lane Detection with Hough Transform", "favorite": "0", "status": "1", "time_added": "1588697755", "time_updated": "1612385105", "time_read": "1588808478", "time_favorited": "0", "sort_id": 22, "resolved_title": "A Deep Dive into Lane Detection with Hough Transform", "resolved_url": "https://towardsdatascience.com/a-deep-dive-into-lane-detection-with-hough-transform-8f90fdd1322f", "excerpt": "Lane line detection is one of the essential components of self-driving cars. There are many approaches to doing this. Here, we’ll look at the simplest approach using Hough Transform. Alright, let’s dive into it! So before we get started, we need a place to write our code.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "4454", "lang": "en", "time_to_read": 20, "top_image_url": "https://miro.medium.com/max/1200/1*nX0QqMYYWA4pT3yppzFzgw.jpeg", "tags": {"machine-learning": {"item_id": "2973401798", "tag": "machine-learning"}, "vision": {"item_id": "2973401798", "tag": "vision"}}, "authors": {"133445068": {"item_id": "2973401798", "author_id": "133445068", "name": "source", "url": "https://www.researchgate.net/profile/Prabal_Patra/publication/335738299/figure/fig1/AS:801797773996033@1568174894886/A-line-in-cartesian-plane-is-represented-as-a-point-in-Hough-Space-or-space.png"}}, "image": {"item_id": "2973401798", "src": "https://miro.medium.com/max/2560/1*nX0QqMYYWA4pT3yppzFzgw.jpeg", "width": "1280", "height": "720"}, "images": {"1": {"item_id": "2973401798", "image_id": "1", "src": "https://miro.medium.com/max/2560/1*nX0QqMYYWA4pT3yppzFzgw.jpeg", "width": "1280", "height": "720", "credit": "here", "caption": "Source"}, "2": {"item_id": "2973401798", "image_id": "2", "src": "https://miro.medium.com/fit/c/56/56/2*6oUrRMdZrak2EkI-eFp_Ng.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "3": {"item_id": "2973401798", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*R9uAxHCLCkhmGbgIXtJS4g.png", "width": "700", "height": "395", "credit": "", "caption": "What a Canny Image looks like"}, "4": {"item_id": "2973401798", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*PcK79ca-c3j-tFG3nwjcjQ.png", "width": "700", "height": "205", "credit": "In reality, it's white, but just to show you, I made it yellow", "caption": "Left: Img1. Right: Img2"}, "5": {"item_id": "2973401798", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*elPSETA-zWv8x_boqmXbpA.png", "width": "700", "height": "202", "credit": "", "caption": "Only the edges in the isolated region are outputted. Everything else is ignored"}, "6": {"item_id": "2973401798", "image_id": "6", "src": "https://miro.medium.com/max/974/1*SXBV933uM5qoJ9LJSFPxuQ.png", "width": "487", "height": "329", "credit": "source", "caption": "Lines in Cartesian Coordinate Space"}, "7": {"item_id": "2973401798", "image_id": "7", "src": "https://miro.medium.com/max/1284/1*xfbDk68SHEETDVh2AJxyng.png", "width": "642", "height": "256", "credit": "", "caption": "Think of the θ as b and the r as m. I will explain the relevance of the θ and the r later on in the article"}, "8": {"item_id": "2973401798", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*lkjL5rOu-ZjCy7X9bmp7sQ.png", "width": "700", "height": "459", "credit": "3, 4", "caption": "The three lines I previously mentioned. They all cross the point"}, "9": {"item_id": "2973401798", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*hCjDcuuE9zD39YXRU44-0g.png", "width": "700", "height": "394", "credit": "match the colours", "caption": "Each point represents the lines previously shown"}, "10": {"item_id": "2973401798", "image_id": "10", "src": "https://miro.medium.com/max/970/1*3Q7a3YMImHkZ1Z5GykAs1g.jpeg", "width": "485", "height": "529", "credit": "", "caption": "In this image, the m and b values which correspond with the darkest blue bin will be the line of best fit"}, "11": {"item_id": "2973401798", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*tsOWWW3I1si8epZrsNf_7Q.png", "width": "700", "height": "481", "credit": "", "caption": "A visual explanation of what the polar coordinates mean"}, "12": {"item_id": "2973401798", "image_id": "12", "src": "https://miro.medium.com/max/1400/1*tC4cjVEAMgJSyefuitEatg.png", "width": "700", "height": "525", "credit": "", "caption": "A visual of what I was trying to explain."}, "13": {"item_id": "2973401798", "image_id": "13", "src": "https://miro.medium.com/max/1400/1*CZV-1B6zssHsK7n_VEzzjA.jpeg", "width": "700", "height": "302", "credit": "source", "caption": "Sinusoids referring to the type of curve, a sinusoidal curve."}, "14": {"item_id": "2973401798", "image_id": "14", "src": "https://miro.medium.com/max/1400/1*DaKfYm7LiDvIt0M4XDSPxg.png", "width": "700", "height": "391", "credit": "", "caption": "This is without averaging the lines. Pretty choppy, huh."}, "15": {"item_id": "2973401798", "image_id": "15", "src": "https://miro.medium.com/max/1400/1*zT7ed-RhcuWbcFej2WRgOw.png", "width": "700", "height": "405", "credit": "", "caption": "A visual example of the make_points function applied to the left line"}, "16": {"item_id": "2973401798", "image_id": "16", "src": "https://miro.medium.com/max/1400/1*GwNQr6WVSJu5qcEKO-CNqw.png", "width": "700", "height": "189", "credit": "", "caption": "Left: Appending Lines to Image Directly. Right: Using the cv2.addWeighted function"}, "17": {"item_id": "2973401798", "image_id": "17", "src": "https://miro.medium.com/max/1400/1*ZOcbrwceRJ0ccJ8m_i6A3w.png", "width": "700", "height": "398", "credit": "", "caption": "What the output should look like"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1724}, "3078968468": {"item_id": "3078968468", "resolved_id": "3078968468", "given_url": "https://www.sciencedaily.com/releases/2020/08/200813103114.htm", "given_title": "AI system for high precision recognition of hand gestures", "favorite": "0", "status": "1", "time_added": "1597346149", "time_updated": "1638708525", "time_read": "1607569814", "time_favorited": "0", "sort_id": 23, "resolved_title": "AI system for high precision recognition of hand gestures", "resolved_url": "https://www.sciencedaily.com/releases/2020/08/200813103114.htm", "excerpt": "Scientists from Nanyang Technological University, Singapore (NTU Singapore) have developed an Artificial Intelligence (AI) system that recognises hand gestures by combining skin-like electronics with computer vision.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "831", "lang": "en", "time_to_read": 4, "top_image_url": "https://www.sciencedaily.com/images/scidaily-icon.png", "tags": {"deep-learning": {"item_id": "3078968468", "tag": "deep-learning"}, "machine-learning": {"item_id": "3078968468", "tag": "machine-learning"}, "vision": {"item_id": "3078968468", "tag": "vision"}}, "authors": {"4112981": {"item_id": "3078968468", "author_id": "4112981", "name": "Nanyang Technological University", "url": ""}}, "domain_metadata": {"name": "ScienceDaily", "logo": "https://logo.clearbit.com/sciencedaily.com?size=800", "greyscale_logo": "https://logo.clearbit.com/sciencedaily.com?size=800&greyscale=true"}, "listen_duration_estimate": 322}, "3170402481": {"item_id": "3170402481", "resolved_id": "3170402501", "given_url": "https://towardsdatascience.com/practical-guide-to-entity-resolution-part-4-299ac89b9415?source=rss----7f60cf5620c9---4", "given_title": "All Personal Feeds", "favorite": "0", "status": "1", "time_added": "1605138040", "time_updated": "1638708525", "time_read": "1608290451", "time_favorited": "0", "sort_id": 24, "resolved_title": "Practical Guide to Entity Resolution", "resolved_url": "https://towardsdatascience.com/practical-guide-to-entity-resolution-part-4-299ac89b9415", "excerpt": "This is part 4 of a mini-series on entity resolution. Check out part 1, part 2, part 3 if you missed it Candidate pair generation is a fairly straightforward part of ER, as it is essentially a self join on the blocking keys.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "520", "lang": "en", "top_image_url": "https://miro.medium.com/max/1200/0*DIy4DcgKrMCLTqVi", "tags": {"deep-learning": {"item_id": "3170402481", "tag": "deep-learning"}, "entity-resolution": {"item_id": "3170402481", "tag": "entity-resolution"}, "vision": {"item_id": "3170402481", "tag": "vision"}}, "authors": {"142254521": {"item_id": "3170402481", "author_id": "142254521", "name": "Yifei Huang", "url": "https://yifei-huang.medium.com"}}, "image": {"item_id": "3170402481", "src": "https://miro.medium.com/fit/c/56/56/1*c01BdvUOn9xPafDWjDKy2A.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3170402481", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*c01BdvUOn9xPafDWjDKy2A.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3170402481", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*DIy4DcgKrMCLTqVi", "width": "700", "height": "467", "credit": "Alina Grubnyak on Unsplash", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 201}, "2932893729": {"item_id": "2932893729", "resolved_id": "2932893774", "given_url": "https://www.kdnuggets.com/2020/03/brain-tumor-detection-mask-r-cnn.html", "given_title": "Brain Tumor Detection using Mask R-CNN", "favorite": "0", "status": "1", "time_added": "1585583620", "time_updated": "1638708525", "time_read": "1585739468", "time_favorited": "0", "sort_id": 25, "resolved_title": "Brain Tumor Detection using Mask R-CNN", "resolved_url": "https://www.kdnuggets.com/brain-tumor-detection-using-mask-r-cnn.html/", "excerpt": "In the health care sector, medical image analysis plays an active role, especially in Non-invasive treatment and clinical study. Medical imaging techniques and analysis tools help medical practitioners and radiologists to correctly diagnose the disease.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2205", "lang": "en", "time_to_read": 10, "top_image_url": "https://miro.medium.com/max/1098/0*-UvZJbq8g1MhotfV.gif", "tags": {"deep-learning": {"item_id": "2932893729", "tag": "deep-learning"}, "vision": {"item_id": "2932893729", "tag": "vision"}}, "authors": {"125144284": {"item_id": "2932893729", "author_id": "125144284", "name": "Nagesh Singh Chauhan", "url": "https://www.kdnuggets.com/author/nagesh-chauhan"}}, "image": {"item_id": "2932893729", "src": "https://miro.medium.com/max/257/0*a_xlnNKpAct01GdD.png", "width": "60", "height": "0"}, "images": {"1": {"item_id": "2932893729", "image_id": "1", "src": "https://miro.medium.com/max/257/0*a_xlnNKpAct01GdD.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2932893729", "image_id": "2", "src": "https://miro.medium.com/max/186/0*D8uirYSMZtVZrsnI.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2932893729", "image_id": "3", "src": "https://miro.medium.com/max/1206/0*oYUx49JrNx8XzvtN.png", "width": "100", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2932893729", "image_id": "4", "src": "https://miro.medium.com/max/704/1*RJ5NQhntiPxkjtjTAjictQ.png", "width": "100", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2932893729", "image_id": "5", "src": "https://miro.medium.com/max/329/1*7ZLWw9D8j4lc485GR1bMlQ.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2932893729", "image_id": "6", "src": "https://miro.medium.com/max/420/1*hsBP-r3NyMyhImSlSSPeWw.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2932893729", "image_id": "7", "src": "https://miro.medium.com/max/357/1*t7qxWZ6lu4B88StODAlptw.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2932893729", "image_id": "8", "src": "https://miro.medium.com/max/470/1*PvpgaIWM1zB1-wPmN9WQow.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2932893729", "image_id": "9", "src": "https://miro.medium.com/max/372/1*EBgARqiai5tc-sfptpyXng.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2932893729", "image_id": "10", "src": "https://miro.medium.com/max/482/1*1aIrEb38DIKa_XPHU0u5ZA.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "2932893729", "image_id": "11", "src": "https://miro.medium.com/max/360/1*oNCKn9tYZNVjxgkN4Gipuw.png", "width": "60", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "2932893729", "image_id": "12", "src": "https://miro.medium.com/max/473/1*wOaqSaXh5Qi-eP43UHUTjQ.png", "width": "60", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "KDnuggets", "logo": "https://logo.clearbit.com/kdnuggets.com?size=800", "greyscale_logo": "https://logo.clearbit.com/kdnuggets.com?size=800&greyscale=true"}, "listen_duration_estimate": 854}, "2928233012": {"item_id": "2928233012", "resolved_id": "2928233051", "given_url": "https://towardsdatascience.com/building-an-image-taking-interface-application-for-your-image-recognition-model-973b121cc9d9?source=rss----7f60cf5620c9---4", "given_title": "Building an Image-Taking Interface Application for Your Image Recognition M", "favorite": "1", "status": "1", "time_added": "1585223943", "time_updated": "1638708525", "time_read": "1585739495", "time_favorited": "1585580102", "sort_id": 26, "resolved_title": "Building an Image-Taking Interface Application for Your Image Recognition Model", "resolved_url": "https://towardsdatascience.com/building-an-image-taking-interface-application-for-your-image-recognition-model-973b121cc9d9", "excerpt": "Often, data scientists build a model for image recognition, see the accuracy, and, if it’s high enough, consider the job done. Ever since I got into machine learning at 13, I never understood that. Why spend all the time building the best model — just to be satisfied with a number?", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "741", "lang": "en", "time_to_read": 3, "top_image_url": "https://miro.medium.com/max/1200/0*JpjypdslhZHdb0-b.jpg", "tags": {"deep-learning": {"item_id": "2928233012", "tag": "deep-learning"}, "vision": {"item_id": "2928233012", "tag": "vision"}}, "authors": {"144200356": {"item_id": "2928233012", "author_id": "144200356", "name": "Andre Ye", "url": "https://andre-ye.medium.com"}}, "image": {"item_id": "2928233012", "src": "https://miro.medium.com/max/2560/0*JpjypdslhZHdb0-b.jpg", "width": "1280", "height": "512"}, "images": {"1": {"item_id": "2928233012", "image_id": "1", "src": "https://miro.medium.com/max/2560/0*JpjypdslhZHdb0-b.jpg", "width": "1280", "height": "512", "credit": "", "caption": "Image from Pixabay"}, "2": {"item_id": "2928233012", "image_id": "2", "src": "https://miro.medium.com/fit/c/56/56/1*agZM8gnrsEIs3InGe8B5EA.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "3": {"item_id": "2928233012", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*LIBJQ4c4IhjDQTUzovdP8w.png", "width": "700", "height": "671", "credit": "", "caption": "The top few printed lines should look like this."}, "4": {"item_id": "2928233012", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*FXeF1mE5vdyojZ26rdnxgQ.png", "width": "700", "height": "180", "credit": "", "caption": ""}, "5": {"item_id": "2928233012", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*RSGmR_oje6FmC3_733Z3lg.png", "width": "700", "height": "575", "credit": "", "caption": ""}, "6": {"item_id": "2928233012", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*9ph6ZVA9hGpCj0F_kbPOyg.png", "width": "700", "height": "341", "credit": "", "caption": "Good job!"}, "7": {"item_id": "2928233012", "image_id": "7", "src": "https://miro.medium.com/max/1014/1*v_khkuKMWwxVUWoiy8OJvQ.png", "width": "507", "height": "499", "credit": "", "caption": ""}, "8": {"item_id": "2928233012", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*uriv4u4LgSZfVDIbbHEQrQ.png", "width": "700", "height": "424", "credit": "", "caption": ""}, "9": {"item_id": "2928233012", "image_id": "9", "src": "https://miro.medium.com/max/962/1*CN0OxCoWJQopSukY4ZlYjw.png", "width": "481", "height": "510", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 287}, "3194859629": {"item_id": "3194859629", "resolved_id": "3194859629", "given_url": "https://ciechanow.ski/cameras-and-lenses/", "given_title": "Cameras and Lenses – Bartosz Ciechanowski", "favorite": "0", "status": "1", "time_added": "1607650794", "time_updated": "1607656608", "time_read": "1607656608", "time_favorited": "0", "sort_id": 27, "resolved_title": "Cameras and Lenses – Bartosz Ciechanowski", "resolved_url": "https://ciechanow.ski/cameras-and-lenses/", "excerpt": "Pictures have always been a meaningful part of the human experience. From the first cave drawings, to sketches and paintings, to modern photography, we’ve mastered the art of recording what we see. Cameras and the lenses inside them may seem a little mystifying.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "6209", "lang": "", "top_image_url": "https://ciechanow.ski/images/og/lenses.jpg", "tags": {"vision": {"item_id": "3194859629", "tag": "vision"}}, "authors": {"554741": {"item_id": "3194859629", "author_id": "554741", "name": "Bartosz Ciechanowski", "url": ""}}, "listen_duration_estimate": 2403}, "2993419557": {"item_id": "2993419557", "resolved_id": "2993419578", "given_url": "https://towardsdatascience.com/classification-of-brain-mri-as-tumor-non-tumor-4409ba2293b5?source=rss----7f60cf5620c9---4", "given_title": "Classification of Brain MRI as Tumor/Non Tumor", "favorite": "0", "status": "1", "time_added": "1590285690", "time_updated": "1638708525", "time_read": "1591029821", "time_favorited": "0", "sort_id": 28, "resolved_title": "Classification of Brain MRI as Tumor/Non Tumor", "resolved_url": "https://towardsdatascience.com/classification-of-brain-mri-as-tumor-non-tumor-4409ba2293b5", "excerpt": "Hi Everyone! Being a Biomedical Undergraduate student, wouldn’t it be wrong on my part if I do not show you all an application of AI in Medicine.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1525", "lang": "en", "time_to_read": 7, "top_image_url": "https://miro.medium.com/max/257/1*BGxLxoHd8MG9m8gesYIprw.jpeg", "tags": {"deep-learning": {"item_id": "2993419557", "tag": "deep-learning"}, "vision": {"item_id": "2993419557", "tag": "vision"}}, "authors": {"134576740": {"item_id": "2993419557", "author_id": "134576740", "name": "Gurucharan M K", "url": "https://towardsdatascience.com/@mk.gurucharan"}}, "image": {"item_id": "2993419557", "src": "https://miro.medium.com/max/594/1*SN8nPp94AYQslNUyDGkmqA.jpeg", "width": "297", "height": "359"}, "images": {"1": {"item_id": "2993419557", "image_id": "1", "src": "https://miro.medium.com/max/594/1*SN8nPp94AYQslNUyDGkmqA.jpeg", "width": "297", "height": "359", "credit": "", "caption": "MRI with Tumor"}, "2": {"item_id": "2993419557", "image_id": "2", "src": "https://miro.medium.com/max/514/1*BGxLxoHd8MG9m8gesYIprw.jpeg", "width": "257", "height": "251", "credit": "", "caption": "MRI without Tumor"}, "3": {"item_id": "2993419557", "image_id": "3", "src": "https://miro.medium.com/max/1840/1*CHs2-J2Gt8VbZcYTr5D3DQ.jpeg", "width": "920", "height": "311", "credit": "Photo from engmrk.com", "caption": "An example of CNN Architecture."}, "4": {"item_id": "2993419557", "image_id": "4", "src": "https://miro.medium.com/max/1652/1*8neV70cHZL5EdxteHKg6Jg.jpeg", "width": "826", "height": "532", "credit": "Photo from Datascience.aero", "caption": "Data Imbalance"}, "5": {"item_id": "2993419557", "image_id": "5", "src": "https://miro.medium.com/max/360/1*ALFgFFmoKpOvThCs4x030Q.jpeg", "width": "180", "height": "218", "credit": "", "caption": ""}, "6": {"item_id": "2993419557", "image_id": "6", "src": "https://miro.medium.com/max/360/1*ZaI4NDoXv7DAs4kfP0ykmA.jpeg", "width": "180", "height": "218", "credit": "", "caption": ""}, "7": {"item_id": "2993419557", "image_id": "7", "src": "https://miro.medium.com/max/360/1*NQbhc_VCtXray2wOE3RJog.jpeg", "width": "180", "height": "218", "credit": "", "caption": "Data Augmentation"}, "8": {"item_id": "2993419557", "image_id": "8", "src": "https://miro.medium.com/max/986/1*cbTKDEA0LDCEQLFTYVZDaA.jpeg", "width": "493", "height": "269", "credit": "", "caption": ""}, "9": {"item_id": "2993419557", "image_id": "9", "src": "https://miro.medium.com/max/934/1*3sDxKeqxQyEmQe_zEwinIg.jpeg", "width": "467", "height": "276", "credit": "", "caption": "Accuracy and Loss Graph"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 590}, "2927071095": {"item_id": "2927071095", "resolved_id": "2927052386", "given_url": "https://towardsdatascience.com/computer-vision-101-working-with-color-images-in-python-7b57381a8a54?source=rss----7f60cf5620c9---4", "given_title": "Computer Vision 101: Working with Color Images in Python", "favorite": "0", "status": "1", "time_added": "1585145075", "time_updated": "1585739583", "time_read": "1585739582", "time_favorited": "0", "sort_id": 29, "resolved_title": "Computer Vision 101: Working with Color Images in Python", "resolved_url": "https://towardsdatascience.com/computer-vision-101-working-with-color-images-in-python-7b57381a8a54", "excerpt": "Every computer vision project — be it a cat/dog classifier or bringing colors to old images/movies — involves working with images. And in the end, the model can only be as good as the underlying data — garbage in, garbage out.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1746", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/max/1200/1*dQsnsb5ndX53BEH-ea98wQ.jpeg", "tags": {"python": {"item_id": "2927071095", "tag": "python"}, "vision": {"item_id": "2927071095", "tag": "vision"}}, "authors": {"91607948": {"item_id": "2927071095", "author_id": "91607948", "name": "Eryk Lewinson", "url": "https://medium.com/@eryk.lewinson"}}, "image": {"item_id": "2927071095", "src": "https://miro.medium.com/max/10368/1*dQsnsb5ndX53BEH-ea98wQ.jpeg", "width": "5184", "height": "3456"}, "images": {"1": {"item_id": "2927071095", "image_id": "1", "src": "https://miro.medium.com/max/10368/1*dQsnsb5ndX53BEH-ea98wQ.jpeg", "width": "5184", "height": "3456", "credit": "", "caption": "Source: pexels.com"}, "2": {"item_id": "2927071095", "image_id": "2", "src": "https://miro.medium.com/fit/c/56/56/1*KUipyYqi56L5EZiI9qe6JQ.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "3": {"item_id": "2927071095", "image_id": "3", "src": "https://miro.medium.com/max/2804/1*aQunkG371Badww70sRF9HA.png", "width": "1402", "height": "576", "credit": "", "caption": "Source"}, "4": {"item_id": "2927071095", "image_id": "4", "src": "https://miro.medium.com/max/2440/1*aY8otylp46Jz4USxN6N_Mw.png", "width": "1220", "height": "868", "credit": "", "caption": ""}, "5": {"item_id": "2927071095", "image_id": "5", "src": "https://miro.medium.com/max/6424/1*-2EVHtSCFwyFRzmYSzRCyw.png", "width": "3212", "height": "608", "credit": "", "caption": ""}, "6": {"item_id": "2927071095", "image_id": "6", "src": "https://miro.medium.com/max/6404/1*PvFSwUfqiJYG_M9rKd6hwQ.png", "width": "3202", "height": "596", "credit": "", "caption": ""}, "7": {"item_id": "2927071095", "image_id": "7", "src": "https://miro.medium.com/max/6408/1*xglC0aD1jZs2BBl-49g8FA.png", "width": "3204", "height": "544", "credit": "", "caption": "1st attempt at plotting the Lab image"}, "8": {"item_id": "2927071095", "image_id": "8", "src": "https://miro.medium.com/max/6416/1*UOcrblHxSHL-s3LTfD5Nng.png", "width": "3208", "height": "598", "credit": "", "caption": "2nd attempt at plotting the Lab image"}, "9": {"item_id": "2927071095", "image_id": "9", "src": "https://miro.medium.com/max/6444/1*n2KAEwR-CFAfkL4Zr9csCQ.png", "width": "3222", "height": "590", "credit": "", "caption": "3rd attempt at plotting the Lab image"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 676}, "2661583071": {"item_id": "2661583071", "resolved_id": "2661583071", "given_url": "https://www.kdnuggets.com/2019/07/computer-vision-beginners.html", "given_title": "Computer Vision for Beginners: Part 1", "favorite": "0", "status": "1", "time_added": "1563365083", "time_updated": "1608346614", "time_read": "1566599865", "time_favorited": "0", "sort_id": 30, "resolved_title": "Computer Vision for Beginners: Part 1", "resolved_url": "https://www.kdnuggets.com/2019/07/computer-vision-beginners.html", "excerpt": "Computer Vision is one of the hottest topics in artificial intelligence. It is making tremendous advances in self-driving cars, robotics as well as in various photo correction apps. Steady progress in object detection is being made every day.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2138", "lang": "en", "time_to_read": 10, "tags": {"vision": {"item_id": "2661583071", "tag": "vision"}}, "image": {"item_id": "2661583071", "src": "https://i.ibb.co/pyqRcr7/1-Cw-Svase-Yli4-Jst1ib-NXTg-Q.jpg", "width": "99", "height": "0"}, "images": {"1": {"item_id": "2661583071", "image_id": "1", "src": "https://i.ibb.co/pyqRcr7/1-Cw-Svase-Yli4-Jst1ib-NXTg-Q.jpg", "width": "99", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2661583071", "image_id": "2", "src": "https://i.ibb.co/nrpG1hj/1-Oe3-Jzh-YDb3-I6wov-NZ-488w.png", "width": "70", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2661583071", "image_id": "3", "src": "https://i.ibb.co/6n9Yt6W/1-d-IUzw-VFYun-Vz-Bt-Rd-Ob7-BSA.png", "width": "70", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2661583071", "image_id": "4", "src": "https://i.ibb.co/hDRhtqm/1-E1c-Rhyj4-By-J-qr-Tev3h-Tl-A.png", "width": "99", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2661583071", "image_id": "5", "src": "https://i.ibb.co/xj5vvpb/1-ze-VGe-XCBFE4-FLSbv-b-Ua-Lw.png", "width": "99", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2661583071", "image_id": "6", "src": "https://i.ibb.co/Br0H9XR/1-t6g-JMc-MAu8-EUXcgbhh-Gy5-Q.png", "width": "99", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2661583071", "image_id": "7", "src": "https://i.ibb.co/zXbzXwW/1-f3p-RIVbutpa9-KBwuqsl43w.png", "width": "99", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2661583071", "image_id": "8", "src": "https://i.ibb.co/HXC3H6Y/1-6elxr-I90-FEi-Dh-Wplc74m-Yg.png", "width": "99", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2661583071", "image_id": "9", "src": "https://i.ibb.co/PTnvG6x/1-f-MZj-Dp-Fb-Vw-Tncr-QAv-Oazdg.png", "width": "70", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2661583071", "image_id": "10", "src": "https://i.ibb.co/zH7YhTs/1-Zn-Wz-Oppbx-Jo-Hp6-Gb2-R3-OZA.png", "width": "70", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "KDnuggets", "logo": "https://logo.clearbit.com/kdnuggets.com?size=800", "greyscale_logo": "https://logo.clearbit.com/kdnuggets.com?size=800&greyscale=true"}, "listen_duration_estimate": 828}, "2569019666": {"item_id": "2569019666", "resolved_id": "2569019691", "given_url": "https://medium.com/@jiwon.jeong/computer-vision-for-beginners-part-4-64a8d9856208?source=rss------artificial_intelligence-5", "given_title": "Computer Vision for Beginners: Part 4", "favorite": "0", "status": "1", "time_added": "1556015608", "time_updated": "1638708525", "time_read": "1567118934", "time_favorited": "0", "sort_id": 31, "resolved_title": "Computer Vision for Beginners: Part 4", "resolved_url": "https://medium.com/@jiwon.jeong/computer-vision-for-beginners-part-4-64a8d9856208", "excerpt": "There are lots of ways out there one can adapt to make learning progress efficiently. As for me, combining studies with a little bit of fun is the best strategy. In this series of tutorials, many images have been used to demonstrate image processing concepts.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2194", "lang": "en", "time_to_read": 10, "top_image_url": "https://cdn-images-1.medium.com/max/1200/1*_-Gxkq3Uj804Nc6Wu1DBdA.png", "tags": {"deep-learning": {"item_id": "2569019666", "tag": "deep-learning"}, "vision": {"item_id": "2569019666", "tag": "vision"}}, "authors": {"102777513": {"item_id": "2569019666", "author_id": "102777513", "name": "Jiwon Jeong", "url": "https://medium.com/@jiwon.jeong"}}, "image": {"item_id": "2569019666", "src": "https://cdn-images-1.medium.com/max/1600/1*_-Gxkq3Uj804Nc6Wu1DBdA.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2569019666", "image_id": "1", "src": "https://cdn-images-1.medium.com/max/1600/1*_-Gxkq3Uj804Nc6Wu1DBdA.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2569019666", "image_id": "2", "src": "https://cdn-images-1.medium.com/max/1600/1*F2dznG-AJRkklpQcRQm25g.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2569019666", "image_id": "3", "src": "https://cdn-images-1.medium.com/max/1600/1*9p5ZlGFJz0_HFKHthyEvRw.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2569019666", "image_id": "4", "src": "https://cdn-images-1.medium.com/max/1600/1*ulXyqlK5LAVhgHe3i4HsKA.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2569019666", "image_id": "5", "src": "https://cdn-images-1.medium.com/max/1600/1*OMU_8F-NDbeLbIy8sFj9UA.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2569019666", "image_id": "6", "src": "https://cdn-images-1.medium.com/max/1600/1*zh_AX-nV5T-1DqlM2j2mnw.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2569019666", "image_id": "7", "src": "https://cdn-images-1.medium.com/max/1600/1*B2veTSIBJBQCw9DmRFrDqg.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2569019666", "image_id": "8", "src": "https://cdn-images-1.medium.com/max/1600/1*wtJWfEVQUKyJ-7YullGDbw.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2569019666", "image_id": "9", "src": "https://cdn-images-1.medium.com/max/1600/1*GvVB4WgAgRc5WO5NqYZtrw.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2569019666", "image_id": "10", "src": "https://cdn-images-1.medium.com/max/1600/1*WhEkPyj_3_e8HgMV05dSqQ.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "2569019666", "image_id": "11", "src": "https://cdn-images-1.medium.com/max/1600/1*JVSrcLV0sK_WJJN8n7ErTA.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "2569019666", "image_id": "12", "src": "https://cdn-images-1.medium.com/max/1600/1*HLMJBoiVt_MkQlrTdpBw4w.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "2569019666", "image_id": "13", "src": "https://cdn-images-1.medium.com/max/1600/1*eBQDrL6BaspKXXfV1mxBgQ.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Medium", "logo": "https://logo.clearbit.com/medium.com?size=800", "greyscale_logo": "https://logo.clearbit.com/medium.com?size=800&greyscale=true"}, "listen_duration_estimate": 849}, "3099309929": {"item_id": "3099309929", "resolved_id": "3099309976", "given_url": "https://www.kdnuggets.com/2020/09/computer-vision-recipes-best-practices-examples.html", "given_title": "Computer Vision Recipes: Best Practices and Examples", "favorite": "0", "status": "1", "time_added": "1599070194", "time_updated": "1638708525", "time_read": "1604368816", "time_favorited": "0", "sort_id": 32, "resolved_title": "Computer Vision Recipes: Best Practices and Examples", "resolved_url": "https://www.kdnuggets.com/computer-vision-recipes-best-practices-and-examples.html/", "excerpt": "Having recently spotlighted a similar resource from Microsoft, the Natural Language Processing Best Practices & Examples repository, today we bring to you its computer vision counterpart.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "387", "lang": "en", "top_image_url": "https://www.kdnuggets.com/wp-content/uploads/computer-vision-recipes.jpg", "tags": {"deep-learning": {"item_id": "3099309929", "tag": "deep-learning"}, "vision": {"item_id": "3099309929", "tag": "vision"}}, "authors": {"77311567": {"item_id": "3099309929", "author_id": "77311567", "name": "Matthew Mayo", "url": "https://www.kdnuggets.com/author/matt-mayo"}}, "image": {"item_id": "3099309929", "src": "https://www.kdnuggets.com/wp-content/uploads/computer-vision-recipes.jpg", "width": "70", "height": "0"}, "images": {"1": {"item_id": "3099309929", "image_id": "1", "src": "https://www.kdnuggets.com/wp-content/uploads/computer-vision-recipes.jpg", "width": "70", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3099309929", "image_id": "2", "src": "https://i.ibb.co/xDMvNBS/cv-recipes-scenarios.jpg", "width": "100", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3099309929", "image_id": "3", "src": "https://www.kdnuggets.com/wp-content/uploads/cv-recipes-best-practices-example-folder.jpg", "width": "100", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "KDnuggets", "logo": "https://logo.clearbit.com/kdnuggets.com?size=800", "greyscale_logo": "https://logo.clearbit.com/kdnuggets.com?size=800&greyscale=true"}, "listen_duration_estimate": 150}, "3020537325": {"item_id": "3020537325", "resolved_id": "3020537325", "given_url": "https://distill.pub/2020/circuits/curve-detectors", "given_title": "Curve Detectors", "favorite": "0", "status": "1", "time_added": "1592392744", "time_updated": "1638708525", "time_read": "1592418626", "time_favorited": "0", "sort_id": 33, "resolved_title": "", "resolved_url": "https://distill.pub/2020/circuits/curve-detectors/", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"deep-learning": {"item_id": "3020537325", "tag": "deep-learning"}, "vision": {"item_id": "3020537325", "tag": "vision"}}, "listen_duration_estimate": 0}, "2985112150": {"item_id": "2985112150", "resolved_id": "2985112186", "given_url": "https://towardsdatascience.com/data-augmentation-in-yolov4-c16bd22b2617?source=rss----7f60cf5620c9---4", "given_title": "Data Augmentation in YOLOv4", "favorite": "0", "status": "1", "time_added": "1589628132", "time_updated": "1638708525", "time_read": "1591029912", "time_favorited": "0", "sort_id": 34, "resolved_title": "The “Secret” to YOLOv4 isn’t Architecture: It’s in Data Preparation. Note: We have also published Data Augmentation in YOLOv4 on our blog.", "resolved_url": "https://towardsdatascience.com/data-augmentation-in-yolov4-c16bd22b2617", "excerpt": "The object detection space continues to move quickly. No more than two months ago, the Google Brain team released EfficientDet for object detection, challenging YOLOv3 as the premier model for (near) realtime object detection, and pushing the boundaries of what is possible in object detection.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1292", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1136/0*jkbdj4syiIqFGvaR.png", "tags": {"deep-learning": {"item_id": "2985112150", "tag": "deep-learning"}, "object-detection": {"item_id": "2985112150", "tag": "object-detection"}, "vision": {"item_id": "2985112150", "tag": "vision"}}, "authors": {"145053877": {"item_id": "2985112150", "author_id": "145053877", "name": "Jacob Solawetz", "url": "https://jacobsolawetz.medium.com"}}, "image": {"item_id": "2985112150", "src": "https://miro.medium.com/fit/c/56/56/1*KjdQOfp6mREAOFJD9JnpTw.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2985112150", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*KjdQOfp6mREAOFJD9JnpTw.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2985112150", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*jkbdj4syiIqFGvaR.png", "width": "700", "height": "429", "credit": "Citation", "caption": ""}, "3": {"item_id": "2985112150", "image_id": "3", "src": "https://miro.medium.com/max/1200/0*fQGQyR37Mfk29mCW.gif", "width": "600", "height": "158", "credit": "", "caption": "Adjusting brightness on our platform"}, "4": {"item_id": "2985112150", "image_id": "4", "src": "https://miro.medium.com/max/1200/0*goXR99C5Q4wrSLH_.gif", "width": "600", "height": "185", "credit": "", "caption": "Flipping images on our platform"}, "5": {"item_id": "2985112150", "image_id": "5", "src": "https://miro.medium.com/max/1400/0*8Ua5bbxFK-LI1gG8.png", "width": "700", "height": "437", "credit": "Citation", "caption": ""}, "6": {"item_id": "2985112150", "image_id": "6", "src": "https://miro.medium.com/max/1400/0*9J7eZ6ujm4MO-fWC.png", "width": "700", "height": "467", "credit": "Citation", "caption": ""}, "7": {"item_id": "2985112150", "image_id": "7", "src": "https://miro.medium.com/max/1400/0*MMWEzuQbxD0GctgC.png", "width": "700", "height": "454", "credit": "Citation", "caption": ""}, "8": {"item_id": "2985112150", "image_id": "8", "src": "https://miro.medium.com/max/1400/0*CBm_F3PXphVUFKhO.png", "width": "700", "height": "676", "credit": "Citation", "caption": ""}, "9": {"item_id": "2985112150", "image_id": "9", "src": "https://miro.medium.com/max/684/0*_bse6j2icTykSdCo.png", "width": "342", "height": "565", "credit": "Citation", "caption": ""}, "10": {"item_id": "2985112150", "image_id": "10", "src": "https://miro.medium.com/max/1400/0*hJclmhf9cBewUvmP.png", "width": "700", "height": "725", "credit": "Citation", "caption": ""}, "11": {"item_id": "2985112150", "image_id": "11", "src": "https://miro.medium.com/max/1400/0*ShgLxN9VYhgYPLnv.png", "width": "700", "height": "435", "credit": "Citation", "caption": ""}, "12": {"item_id": "2985112150", "image_id": "12", "src": "https://miro.medium.com/max/1400/0*VYF0ts2Y9qV5QUpZ.png", "width": "700", "height": "430", "credit": "Citation", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 500}, "3010869462": {"item_id": "3010869462", "resolved_id": "3010869483", "given_url": "https://towardsdatascience.com/dimensionality-reduction-in-hyperspectral-images-using-python-611b40b6accc?source=rss----7f60cf5620c9---4", "given_title": "Dimensionality Reduction in Hyperspectral Images using Python", "favorite": "0", "status": "1", "time_added": "1591628916", "time_updated": "1612385105", "time_read": "1593020464", "time_favorited": "0", "sort_id": 35, "resolved_title": "Dimensionality Reduction in Hyperspectral Images using Python", "resolved_url": "https://towardsdatascience.com/dimensionality-reduction-in-hyperspectral-images-using-python-611b40b6accc", "excerpt": "This article provides detailed implementation of dimensionality reduction technique for Hyperspectral Images(HSI).", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "851", "lang": "en", "time_to_read": 4, "top_image_url": "https://miro.medium.com/max/1000/0*YYQJ_xcOc8G_4YaK", "tags": {"machine-learning": {"item_id": "3010869462", "tag": "machine-learning"}, "vision": {"item_id": "3010869462", "tag": "vision"}}, "authors": {"143204604": {"item_id": "3010869462", "author_id": "143204604", "name": "Syam Kakarla", "url": "https://syamkakarla.medium.com"}}, "image": {"item_id": "3010869462", "src": "https://miro.medium.com/fit/c/56/56/2*NMr6r8DM07yvL4TyI40MdA.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3010869462", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*NMr6r8DM07yvL4TyI40MdA.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3010869462", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*YYQJ_xcOc8G_4YaK", "width": "700", "height": "700", "credit": "USGS on Unsplash", "caption": ""}, "3": {"item_id": "3010869462", "image_id": "3", "src": "https://miro.medium.com/max/826/1*oxyIQMHnxHkDVMUKBaqG7Q.png", "width": "413", "height": "576", "credit": "", "caption": "Ground Truth of Pavia University HSI — Image by Author"}, "4": {"item_id": "3010869462", "image_id": "4", "src": "https://miro.medium.com/max/1138/1*_4-p1_T7aIQmCkFaNeYuwA.png", "width": "569", "height": "356", "credit": "", "caption": "Visualization of sample bands of Pavia University HSI — Image by Author"}, "5": {"item_id": "3010869462", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*sTu7fnTOBcrEqTcpEUHlUQ.png", "width": "700", "height": "355", "credit": "", "caption": "Graph Between Cumulative Explained Variance and Number of Components — Image by Author"}, "6": {"item_id": "3010869462", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*bAF6YiPsKLaIlAY8YwdFLQ.png", "width": "700", "height": "388", "credit": "", "caption": "Visualization of Bands after applying PCA — Image by Author"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 329}, "2927658359": {"item_id": "2927658359", "resolved_id": "2927658359", "given_url": "https://github.com/natanielruiz/disrupting-deepfakes", "given_title": "Disrupting Deepfakes: Adversarial Attacks on Image Translation Networks (Co", "favorite": "0", "status": "1", "time_added": "1585159826", "time_updated": "1638708525", "time_read": "1585739557", "time_favorited": "0", "sort_id": 36, "resolved_title": "natanielruiz/disrupting-deepfakes : 🔥🔥Defending Against Deepfakes Using Adversarial Attacks on Conditional Image Translation Networks", "resolved_url": "https://github.com/natanielruiz/disrupting-deepfakes", "excerpt": "Official PyTorch implementation of Disrupting Deepfakes (to be presented at the CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision). This repository contains code for adversarial attacks (disruptions) for (conditional) image translation networks.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "811", "lang": "en", "time_to_read": 4, "top_image_url": "https://opengraph.githubassets.com/e06e7fd3ba76e29055f9f5f804c5ac1bda545941c5ac7a21100d9ce05597f255/natanielruiz/disrupting-deepfakes", "tags": {"deep-learning": {"item_id": "2927658359", "tag": "deep-learning"}, "vision": {"item_id": "2927658359", "tag": "vision"}}, "image": {"item_id": "2927658359", "src": "https://github.com/natanielruiz/disrupting-deepfakes/raw/master/imgs/demo.gif", "width": "100", "height": "0"}, "images": {"1": {"item_id": "2927658359", "image_id": "1", "src": "https://github.com/natanielruiz/disrupting-deepfakes/raw/master/imgs/demo.gif", "width": "100", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2927658359", "image_id": "2", "src": "https://github.com/natanielruiz/disrupting-deepfakes/raw/master/imgs/main_1.gif", "width": "100", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 314}, "3170259804": {"item_id": "3170259804", "resolved_id": "3170259831", "given_url": "https://towardsdatascience.com/practical-guide-to-entity-resolution-part-2-ab6e42572405?source=rss----7f60cf5620c9---4", "given_title": "Favorites", "favorite": "0", "status": "1", "time_added": "1605143156", "time_updated": "1638708525", "time_read": "1608290452", "time_favorited": "0", "sort_id": 37, "resolved_title": "Practical Guide to Entity Resolution", "resolved_url": "https://towardsdatascience.com/practical-guide-to-entity-resolution-part-2-ab6e42572405", "excerpt": "This is part 2 of a mini-series on entity resolution. Check out part 1 if you missed it Part 2 of this series will focus on the source normalization step of entity resolution, and will use the Amazon-GoogleProducts dataset obtained here as an example to illustrate ideas and implementation.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "464", "lang": "en", "top_image_url": "https://miro.medium.com/max/1200/0*xhdkyR60zzjh2AGn", "tags": {"deep-learning": {"item_id": "3170259804", "tag": "deep-learning"}, "entity-resolution": {"item_id": "3170259804", "tag": "entity-resolution"}, "vision": {"item_id": "3170259804", "tag": "vision"}}, "authors": {"142254521": {"item_id": "3170259804", "author_id": "142254521", "name": "Yifei Huang", "url": "https://yifei-huang.medium.com"}}, "image": {"item_id": "3170259804", "src": "https://miro.medium.com/fit/c/56/56/1*c01BdvUOn9xPafDWjDKy2A.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3170259804", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*c01BdvUOn9xPafDWjDKy2A.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3170259804", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*xhdkyR60zzjh2AGn", "width": "700", "height": "467", "credit": "", "caption": "Normalizing data is like forging metal — precision and care are required. Photo by Joni Gutierrez — Dr Joni Multimedia on Unsplash"}, "3": {"item_id": "3170259804", "image_id": "3", "src": "https://miro.medium.com/max/1400/0*3G5_Ennii315yGyC.png", "width": "700", "height": "148", "credit": "", "caption": ""}, "4": {"item_id": "3170259804", "image_id": "4", "src": "https://miro.medium.com/max/1400/0*NY6WPjiR5S3228bD.png", "width": "700", "height": "231", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 180}, "3196806772": {"item_id": "3196806772", "resolved_id": "3196806772", "given_url": "https://feedly.com/i/collection/content/user/8a4d6e54-3325-4a82-9cfb-659993fa74a2/category/global.must", "given_title": "Favorites", "favorite": "0", "status": "1", "time_added": "1607528988", "time_updated": "1607528997", "time_read": "1607528997", "time_favorited": "0", "sort_id": 38, "resolved_title": "Feedly - Goodbye information overload", "resolved_url": "https://feedly.com/i/collection/content/user/8a4d6e54-3325-4a82-9cfb-659993fa74a2/category/global.must", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "en", "top_image_url": "https://s1.feedly.com/images/fx/og/welcome@2x.png", "tags": {"vision": {"item_id": "3196806772", "tag": "vision"}}, "domain_metadata": {"name": "Feedly", "logo": "https://logo.clearbit.com/feedly.com?size=800", "greyscale_logo": "https://logo.clearbit.com/feedly.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "2779527809": {"item_id": "2779527809", "resolved_id": "2779527809", "given_url": "https://github.com/FORTH-ModelBasedTracker/MocapNET", "given_title": "FORTH-ModelBasedTracker/MocapNET: We present MocapNET, an ensemble of SNN e", "favorite": "0", "status": "1", "time_added": "1594391205", "time_updated": "1594426577", "time_read": "1594426577", "time_favorited": "0", "sort_id": 39, "resolved_title": "MocapNET Project", "resolved_url": "https://github.com/FORTH-ModelBasedTracker/MocapNET", "excerpt": "A new version of MocapNET has landed! It contains a very big list of improvements that have been carried out during 2020 over the original work that allows higher accuracy, smoother BVH output and better occlusion robustness while maintaining realtime perfomance.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2674", "lang": "en", "time_to_read": 12, "top_image_url": "https://opengraph.githubassets.com/5078d72a22f228000428da6b51c644935879b705eba2b629e1312dd36641744c/FORTH-ModelBasedTracker/MocapNET", "tags": {"pose-estimation": {"item_id": "2779527809", "tag": "pose-estimation"}, "vision": {"item_id": "2779527809", "tag": "vision"}}, "image": {"item_id": "2779527809", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/mnet2.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2779527809", "image_id": "1", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/mnet2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2779527809", "image_id": "2", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/shuffle.gif", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2779527809", "image_id": "3", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/youtube.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2779527809", "image_id": "4", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/bvh.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2779527809", "image_id": "5", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/blender.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2779527809", "image_id": "6", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/leedsDataset.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2779527809", "image_id": "7", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/youtubevideolink2.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2779527809", "image_id": "8", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/youtubevideolink.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2779527809", "image_id": "9", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/ICPR2020_posterYoutubeVideoLink.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2779527809", "image_id": "10", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/transparentTab.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "2779527809", "image_id": "11", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show0.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "2779527809", "image_id": "12", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show3.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "2779527809", "image_id": "13", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show1.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "14": {"item_id": "2779527809", "image_id": "14", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/mnet2/doc/show0ogl.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "15": {"item_id": "2779527809", "image_id": "15", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/mocapnet_rosnode/main/doc/screenshot.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "16": {"item_id": "2779527809", "image_id": "16", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/CSVClusterPlot.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "17": {"item_id": "2779527809", "image_id": "17", "src": "https://raw.githubusercontent.com/FORTH-ModelBasedTracker/MocapNET/master/doc/BVHGUI2.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 1035}, "3140253273": {"item_id": "3140253273", "resolved_id": "3140253289", "given_url": "https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34?source=rss----7f60cf5620c9---4", "given_title": "How to cluster images based on visual similarity", "favorite": "0", "status": "1", "time_added": "1602586790", "time_updated": "1638708525", "time_read": "1604361156", "time_favorited": "0", "sort_id": 40, "resolved_title": "How to cluster images based on visual similarity", "resolved_url": "https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34", "excerpt": "In this tutorial, I'm going to walk you through using a pre-trained neural network to extract a feature vector from images and cluster the images based on how similar the feature vectors are.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1062", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1200/0*RARg2hIOV2DCGO-h", "tags": {"clustering": {"item_id": "3140253273", "tag": "clustering"}, "deep-learning": {"item_id": "3140253273", "tag": "deep-learning"}, "vision": {"item_id": "3140253273", "tag": "vision"}}, "authors": {"140656943": {"item_id": "3140253273", "author_id": "140656943", "name": "Gabe Flomo", "url": "https://medium.com/@gabeflomo821"}}, "image": {"item_id": "3140253273", "src": "https://miro.medium.com/fit/c/56/56/2*i_XHzhbfOqybNBQmvtK30g.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3140253273", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*i_XHzhbfOqybNBQmvtK30g.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3140253273", "image_id": "2", "src": "https://miro.medium.com/max/2000/0*RARg2hIOV2DCGO-h", "width": "1000", "height": "667", "credit": "Pietro Jeng on Unsplash", "caption": ""}, "3": {"item_id": "3140253273", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*xIIBn2Yajg4naTqJif7q1w.png", "width": "700", "height": "134", "credit": "", "caption": ""}, "4": {"item_id": "3140253273", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*wiGcYoDls0bInqoq8ixPqA.png", "width": "700", "height": "202", "credit": "", "caption": ""}, "5": {"item_id": "3140253273", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*cHlZ4tJp91n5DODlS-oZXQ.png", "width": "700", "height": "72", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 411}, "2912422231": {"item_id": "2912422231", "resolved_id": "2912422231", "given_url": "https://github.com/nandinib1999/object-detection-yolo-opencv", "given_title": "https://github.com/nandinib1999/object-detection-yolo-opencv", "favorite": "0", "status": "1", "time_added": "1583951202", "time_updated": "1638708525", "time_read": "1585739835", "time_favorited": "0", "sort_id": 41, "resolved_title": "nandinib1999/object-detection-yolo-opencv", "resolved_url": "https://github.com/nandinib1999/object-detection-yolo-opencv", "excerpt": "Object Detection using Yolo V3 and OpenCV . Contribute to nandinib1999/object-detection-yolo-opencv development by creating an account on GitHub.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "en", "top_image_url": "https://opengraph.githubassets.com/864c25fcb0d87a42e07876a455b8932b0746d5eab4e43e16098aa5eeac56946e/nandinib1999/object-detection-yolo-opencv", "tags": {"deep-learning": {"item_id": "2912422231", "tag": "deep-learning"}, "object-detection": {"item_id": "2912422231", "tag": "object-detection"}, "opencv": {"item_id": "2912422231", "tag": "opencv"}, "vision": {"item_id": "2912422231", "tag": "vision"}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "3011329450": {"item_id": "3011329450", "resolved_id": "3011329503", "given_url": "https://towardsdatascience.com/image-augmentation-mastering-15-techniques-and-useful-functions-with-python-codes-44c3f8c1ea1f?source=rss----7f60cf5620c9---4", "given_title": "Image Augmentation Mastering: 15  Techniques and Useful Functions with Pyth", "favorite": "0", "status": "1", "time_added": "1591661968", "time_updated": "1638708525", "time_read": "1593020454", "time_favorited": "0", "sort_id": 42, "resolved_title": "Image Augmentation Mastering: 15+ Techniques and Useful Functions with Python Codes", "resolved_url": "https://towardsdatascience.com/image-augmentation-mastering-15-techniques-and-useful-functions-with-python-codes-44c3f8c1ea1f", "excerpt": "Whether we are enjoying Keras or Pytorch we have access to wonderful libraries to efficiently enhance our images. But what about those special cases where: For all these cases and many others, we must be able to master our image augmentation.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1809", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/max/1200/1*jNwS-HWYMQqfnhkFBVf8ng.png", "tags": {"deep-learning": {"item_id": "3011329450", "tag": "deep-learning"}, "vision": {"item_id": "3011329450", "tag": "vision"}}, "authors": {"152254245": {"item_id": "3011329450", "author_id": "152254245", "name": "⭐Axel Thevenot", "url": "https://axel-thevenot.medium.com"}}, "image": {"item_id": "3011329450", "src": "https://miro.medium.com/fit/c/56/56/2*FObZji7hjGS6NYCI1rdY8w.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3011329450", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*FObZji7hjGS6NYCI1rdY8w.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3011329450", "image_id": "2", "src": "https://miro.medium.com/max/2540/1*jNwS-HWYMQqfnhkFBVf8ng.png", "width": "1270", "height": "748", "credit": "", "caption": ""}, "3": {"item_id": "3011329450", "image_id": "3", "src": "https://miro.medium.com/max/5080/1*jAqTra_7VQoycR9jQyqZSA.png", "width": "2540", "height": "748", "credit": "", "caption": ""}, "4": {"item_id": "3011329450", "image_id": "4", "src": "https://miro.medium.com/max/1920/1*NceyXUSU77fyoSq6EPawkw.png", "width": "960", "height": "540", "credit": "", "caption": ""}, "5": {"item_id": "3011329450", "image_id": "5", "src": "https://miro.medium.com/max/3580/1*OFQoXZBLmgaIcjt9fkQpIg.png", "width": "1790", "height": "205", "credit": "", "caption": ""}, "6": {"item_id": "3011329450", "image_id": "6", "src": "https://miro.medium.com/max/3586/1*Xn5n5Ia6VLOGU5ZZBrxp6g.png", "width": "1793", "height": "338", "credit": "", "caption": ""}, "7": {"item_id": "3011329450", "image_id": "7", "src": "https://miro.medium.com/max/2400/1*bHrhBjYNA63jv6zI7fwA3w.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "8": {"item_id": "3011329450", "image_id": "8", "src": "https://miro.medium.com/max/2400/1*fK_ZfGDNrmwa9V4T3IhIFg.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "9": {"item_id": "3011329450", "image_id": "9", "src": "https://miro.medium.com/max/2400/1*VXCITQ9KaOzcOW5l0Y4S9w.gif", "width": "1200", "height": "338", "credit": "", "caption": "value of center from 0 to 65."}, "10": {"item_id": "3011329450", "image_id": "10", "src": "https://miro.medium.com/max/2400/1*-TZfVtfseat6CQ9xyghwJQ.gif", "width": "1200", "height": "338", "credit": "", "caption": "kernel size from 1 to 35"}, "11": {"item_id": "3011329450", "image_id": "11", "src": "https://miro.medium.com/max/2400/1*Ij2eJ_4T3PSw6Pkv27u0zw.gif", "width": "1200", "height": "338", "credit": "", "caption": "kernel size from 1 to 35"}, "12": {"item_id": "3011329450", "image_id": "12", "src": "https://miro.medium.com/max/2400/1*V9UC_C_mtVvO2Bpq3h7N-g.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "13": {"item_id": "3011329450", "image_id": "13", "src": "https://miro.medium.com/max/2400/1*k-vEFThmu9_g7nx_s-v2JQ.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "14": {"item_id": "3011329450", "image_id": "14", "src": "https://miro.medium.com/max/2400/1*1pyricY3Uanu1zcwWTciHQ.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "15": {"item_id": "3011329450", "image_id": "15", "src": "https://miro.medium.com/max/2400/1*50kSsrLM05CCh_YoqBTLqQ.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "16": {"item_id": "3011329450", "image_id": "16", "src": "https://miro.medium.com/max/2400/1*8l-bYXw35G0d2PrVMTa9ag.gif", "width": "1200", "height": "338", "credit": "", "caption": "Combining random rotation translation shearing and scale"}, "17": {"item_id": "3011329450", "image_id": "17", "src": "https://miro.medium.com/max/2400/1*d9t9qE8Y6EUPoyhMldlwUw.gif", "width": "1200", "height": "338", "credit": "", "caption": "Cutout replacement by 0, on the whole input and cropping the target at the same time"}, "18": {"item_id": "3011329450", "image_id": "18", "src": "https://miro.medium.com/max/2400/1*br0bn7GmLE-3ctwR_30syw.gif", "width": "1200", "height": "338", "credit": "", "caption": "Cutout replacement by 1, channel size on input without cropping the target"}, "19": {"item_id": "3011329450", "image_id": "19", "src": "https://miro.medium.com/max/2400/1*sG-ygbx75S5-N2Mm6v60jA.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "20": {"item_id": "3011329450", "image_id": "20", "src": "https://miro.medium.com/max/2400/1*lMdkvfvziGUvWkTYCiHW9Q.gif", "width": "1200", "height": "338", "credit": "", "caption": "Brightness from -100 to 100"}, "21": {"item_id": "3011329450", "image_id": "21", "src": "https://miro.medium.com/max/2400/1*7DwHkd7zSzGHr3seE7TwPw.gif", "width": "1200", "height": "338", "credit": "", "caption": "Contrasts from -100 to 100"}, "22": {"item_id": "3011329450", "image_id": "22", "src": "https://miro.medium.com/max/2400/1*vu6GiC7FXmY04tHhCR6DAQ.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "23": {"item_id": "3011329450", "image_id": "23", "src": "https://miro.medium.com/max/2400/1*mgWhaCrkF7vNAqIUOKCbbw.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "24": {"item_id": "3011329450", "image_id": "24", "src": "https://miro.medium.com/max/2400/1*JN-gVx1SkJ07qZk9IAVVHg.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "25": {"item_id": "3011329450", "image_id": "25", "src": "https://miro.medium.com/max/2400/1*DHL2a28YL-CBY8Jx5dF8-w.gif", "width": "1200", "height": "338", "credit": "", "caption": ""}, "26": {"item_id": "3011329450", "image_id": "26", "src": "https://miro.medium.com/max/1912/1*eGtKKRvZfNm6mIdQlVXisA.png", "width": "956", "height": "113", "credit": "", "caption": ""}, "27": {"item_id": "3011329450", "image_id": "27", "src": "https://miro.medium.com/max/1912/1*y7Q5U2MjIji7pvt6DRzq3A.png", "width": "956", "height": "113", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 700}, "2911755133": {"item_id": "2911755133", "resolved_id": "2911755174", "given_url": "https://towardsdatascience.com/image-data-labelling-and-annotation-everything-you-need-to-know-86ede6c684b1?source=rss----7f60cf5620c9---4", "given_title": "Image Data Labelling and Annotation — Everything you need to know", "favorite": "1", "status": "1", "time_added": "1583927927", "time_updated": "1638708525", "time_read": "1585739853", "time_favorited": "1583949996", "sort_id": 43, "resolved_title": "Image Data Labelling and Annotation — Everything you need to know", "resolved_url": "https://towardsdatascience.com/image-data-labelling-and-annotation-everything-you-need-to-know-86ede6c684b1", "excerpt": "Data labelling is an essential step in a supervised machine learning task. Garbage In Garbage Out is a phrase commonly used in the machine learning community, which means that the quality of the training data determines the quality of the model.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1114", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1200/1*WE9PltBhSfl_gRPhOhyiUQ.jpeg", "tags": {"deep-learning": {"item_id": "2911755133", "tag": "deep-learning"}, "labeling": {"item_id": "2911755133", "tag": "labeling"}, "vision": {"item_id": "2911755133", "tag": "vision"}}, "authors": {"119290816": {"item_id": "2911755133", "author_id": "119290816", "name": "Sabina Pokhrel", "url": "https://medium.com/@sabinaa.pokhrel"}}, "image": {"item_id": "2911755133", "src": "https://miro.medium.com/fit/c/56/56/2*Q7OmhTLUFhxwbuoM-iyilg.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2911755133", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*Q7OmhTLUFhxwbuoM-iyilg.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2911755133", "image_id": "2", "src": "https://miro.medium.com/max/7500/1*WE9PltBhSfl_gRPhOhyiUQ.jpeg", "width": "3750", "height": "3000", "credit": "Photo by Debby Hudson on Unsplash", "caption": "Labeled bottle of blueberries"}, "3": {"item_id": "2911755133", "image_id": "3", "src": "https://miro.medium.com/max/5264/1*__7JNqqc-nQCu0-WduK6rw.png", "width": "2632", "height": "1664", "credit": "Original Photo by Patricia Jekki on Unsplash", "caption": "Bounding box for detected cars"}, "4": {"item_id": "2911755133", "image_id": "4", "src": "https://miro.medium.com/max/12000/1*GTRFOXAF5fUf853dHKgZEA.jpeg", "width": "6000", "height": "4000", "credit": "w", "caption": "Bounding Box showing co-ordinates x1, y1, x2, y2, width"}, "5": {"item_id": "2911755133", "image_id": "5", "src": "https://miro.medium.com/max/1344/1*mvAz7lcEqusO24AyXyIxLQ.png", "width": "672", "height": "1002", "credit": "", "caption": ""}, "6": {"item_id": "2911755133", "image_id": "6", "src": "https://miro.medium.com/max/1964/1*K-GgaRHr6a1QhMBBKlJ_nw.png", "width": "982", "height": "1282", "credit": "Source", "caption": "Polygonal segmentation of images from COCO dataset"}, "7": {"item_id": "2911755133", "image_id": "7", "src": "https://miro.medium.com/max/2048/1*P0NAnrDIsWSIWGkP-2M2eg.png", "width": "1024", "height": "512", "credit": "", "caption": ""}, "8": {"item_id": "2911755133", "image_id": "8", "src": "https://miro.medium.com/max/2048/1*UYNcOtnhDFTp8qHEu5nafQ.png", "width": "1024", "height": "512", "credit": "Source", "caption": "Semantic segmentation of images from Cityscapes Dataset"}, "9": {"item_id": "2911755133", "image_id": "9", "src": "https://miro.medium.com/max/12000/1*QEn9NeCy63TqxjhfCTlakA.jpeg", "width": "6000", "height": "4000", "credit": "Original Photo by Jose Carbajal on Unsplash", "caption": "3D Cuboid annotation on image"}, "10": {"item_id": "2911755133", "image_id": "10", "src": "https://miro.medium.com/max/4100/1*i7xQ1IciXpQ0XNer5hzg-g.png", "width": "2050", "height": "960", "credit": "Source", "caption": "Key-point annotation examples from COCO dataset"}, "11": {"item_id": "2911755133", "image_id": "11", "src": "https://miro.medium.com/max/10944/1*scjV5Tzr48a2H-OnXEVP7g.jpeg", "width": "5472", "height": "3648", "credit": "Original Photo by Karsten Würth on Unsplash", "caption": "Line annotation on road"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 431}, "2993419586": {"item_id": "2993419586", "resolved_id": "2993419616", "given_url": "https://towardsdatascience.com/image-segmentation-with-six-lines-0f-code-acb870a462e8?source=rss----7f60cf5620c9---4", "given_title": "Image Segmentation With 5 Lines 0f Code", "favorite": "0", "status": "1", "time_added": "1590285677", "time_updated": "1638708525", "time_read": "1591029821", "time_favorited": "0", "sort_id": 44, "resolved_title": "Image Segmentation With 5 Lines 0f Code", "resolved_url": "https://towardsdatascience.com/image-segmentation-with-six-lines-0f-code-acb870a462e8", "excerpt": "Computer vision is evolving on a daily basis. Popular computer vision techniques such as image classification and object detection have been used extensively to solve a lot of computer vision problems. In image classification, an entire image is classified.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1155", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/800/1*9cLgTN_qKJusj54j2VbJrg.jpeg", "tags": {"deep-learning": {"item_id": "2993419586", "tag": "deep-learning"}, "images": {"item_id": "2993419586", "tag": "images"}, "vision": {"item_id": "2993419586", "tag": "vision"}}, "authors": {"152241588": {"item_id": "2993419586", "author_id": "152241588", "name": "Ayoola Olafenwa (she/her)", "url": "https://olafenwaayoola.medium.com"}}, "image": {"item_id": "2993419586", "src": "https://miro.medium.com/fit/c/56/56/1*PxmZHGM2czHVUMvRtJiFoA.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2993419586", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*PxmZHGM2czHVUMvRtJiFoA.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2993419586", "image_id": "2", "src": "https://miro.medium.com/max/1600/1*9cLgTN_qKJusj54j2VbJrg.jpeg", "width": "800", "height": "533", "credit": "", "caption": ""}, "3": {"item_id": "2993419586", "image_id": "3", "src": "https://miro.medium.com/max/2560/1*tOTobkaJqQ3ZGuRQdxUCPQ.jpeg", "width": "1280", "height": "853", "credit": "CC0", "caption": "Source:Wikicommons.com"}, "4": {"item_id": "2993419586", "image_id": "4", "src": "https://miro.medium.com/max/2560/1*1R4gHSMLsWHlL8ehlR_R6A.jpeg", "width": "1280", "height": "853", "credit": "", "caption": "Semantic Segmentation"}, "5": {"item_id": "2993419586", "image_id": "5", "src": "https://miro.medium.com/max/2560/1*pgyg1yfWGtbDq8w-FmnR1A.jpeg", "width": "1280", "height": "853", "credit": "", "caption": "Instance Segmentation"}, "6": {"item_id": "2993419586", "image_id": "6", "src": "https://miro.medium.com/max/1140/1*RFhMuK1iVZYmgK36BFIQFg.jpeg", "width": "570", "height": "768", "credit": "CCO", "caption": "Source: pxhere.com"}, "7": {"item_id": "2993419586", "image_id": "7", "src": "https://miro.medium.com/max/1140/1*tuAi2R-hwfPD72Y66Uum7g.jpeg", "width": "570", "height": "768", "credit": "", "caption": ""}, "8": {"item_id": "2993419586", "image_id": "8", "src": "https://miro.medium.com/max/1140/1*qXk459MSvWAQBahxV1wJ7Q.jpeg", "width": "570", "height": "768", "credit": "", "caption": ""}, "9": {"item_id": "2993419586", "image_id": "9", "src": "https://miro.medium.com/max/1272/1*4lheOrxSzl4iFVHPK1_hpw.png", "width": "636", "height": "863", "credit": "", "caption": ""}, "10": {"item_id": "2993419586", "image_id": "10", "src": "https://miro.medium.com/max/1600/1*hO_md3xNtcncjAmGrTwxYA.jpeg", "width": "800", "height": "533", "credit": "CC0", "caption": "Source: wikicommons.com"}, "11": {"item_id": "2993419586", "image_id": "11", "src": "https://miro.medium.com/max/1600/1*JmVwviEBcAgkuoVC_VwdOw.jpeg", "width": "800", "height": "533", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 447}, "2909948311": {"item_id": "2909948311", "resolved_id": "2909948311", "given_url": "https://www.allaboutcircuits.com/technical-articles/introduction-to-histogram-equalization/", "given_title": "Introduction to Histogram Equalization", "favorite": "0", "status": "1", "time_added": "1583770584", "time_updated": "1589510210", "time_read": "1583784761", "time_favorited": "0", "sort_id": 45, "resolved_title": "", "resolved_url": "https://www.allaboutcircuits.com/technical-articles/introduction-to-histogram-equalization/", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "en", "tags": {"vision": {"item_id": "2909948311", "tag": "vision"}}, "listen_duration_estimate": 0}, "2629159612": {"item_id": "2629159612", "resolved_id": "2621664015", "given_url": "https://www.pyimagesearch.com/2019/06/10/keras-mask-r-cnn/?utm_source=facebook&utm_medium=ad-10-6-2019&utm_campaign=10+June+2019+BP+-+Traffic&utm_content=GIF+-+Traffic+-+Image+1&fbid_campaign=6113968903246&fbid_adset=6113968904446&utm_adset=10+June+2019+BP+-+Email+List+-+United+States+-+18%2B&fbid_ad=6113968907246&fbclid=IwAR2rlxdNju0U5dTkZepqTZKoM8CxPY_RWIjbpF11nxIyZGx07wsCcEXNIpo", "given_title": "Keras Mask R-CNN - PyImageSearch", "favorite": "0", "status": "1", "time_added": "1560778160", "time_updated": "1638708525", "time_read": "1567131457", "time_favorited": "0", "sort_id": 46, "resolved_title": "Keras Mask R-CNN", "resolved_url": "https://www.pyimagesearch.com/2019/06/10/keras-mask-r-cnn/", "excerpt": "In this tutorial, you will learn how to use Keras and Mask R-CNN to perform instance segmentation (both with and without a GPU).", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3323", "lang": "en", "time_to_read": 15, "top_image_url": "https://www.pyimagesearch.com/wp-content/uploads/2019/06/keras_mask_rcnn_featured.jpg", "tags": {"deep-learning": {"item_id": "2629159612", "tag": "deep-learning"}, "vision": {"item_id": "2629159612", "tag": "vision"}}, "authors": {"76030979": {"item_id": "2629159612", "author_id": "76030979", "name": "Adrian Rosebrock", "url": "https://www.pyimagesearch.com/author/adrian/"}}, "image": {"item_id": "2629159612", "src": "https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/keras-mask-rcnn/keras_mask_rcnn_animation.gif", "width": "600", "height": "600"}, "images": {"1": {"item_id": "2629159612", "image_id": "1", "src": "https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/keras-mask-rcnn/keras_mask_rcnn_animation.gif", "width": "600", "height": "600", "credit": "", "caption": ""}, "2": {"item_id": "2629159612", "image_id": "2", "src": "https://www.pyimagesearch.com/wp-content/uploads/2020/01/source-code-icon.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2629159612", "image_id": "3", "src": "https://pyimagesearch.com/wp-content/uploads/2018/11/mask_rcnn_arch.png", "width": "600", "height": "327", "credit": "", "caption": "Figure 1: The Mask R-CNN architecture by He et al. enables object detection and pixel-wise instance segmentation. This blog post uses Keras to work with a Mask R-CNN model trained on the COCO dataset."}, "4": {"item_id": "2629159612", "image_id": "4", "src": "https://pyimagesearch.com/wp-content/uploads/2019/06/keras_mask_rcnn_30th_birthday.jpg", "width": "424", "height": "600", "credit": "truck", "caption": "Figure 2: The Mask R-CNN model trained on COCO created a pixel-wise map of the Jurassic Park jeep"}, "5": {"item_id": "2629159612", "image_id": "5", "src": "https://pyimagesearch.com/wp-content/uploads/2019/06/keras_mask_rcnn_couch.jpg", "width": "424", "height": "600", "credit": "", "caption": "Figure 3: My dog, Janie, has been segmented from the couch and chair using a Keras and Mask R-CNN deep learning model."}, "6": {"item_id": "2629159612", "image_id": "6", "src": "https://pyimagesearch.com/wp-content/uploads/2019/06/keras_mask_rcnn_page_az.jpg", "width": "424", "height": "600", "credit": "created with Keras, TensorFlow, and Matterport’s Mask R-CNN implementation", "caption": "Figure 4: A Mask R-CNN segmented image"}, "7": {"item_id": "2629159612", "image_id": "7", "src": "https://pyimagesearch.com/wp-content/uploads/2019/06/keras_mask_rcnn_ybor_city.jpg", "width": "421", "height": "600", "credit": "", "caption": "Figure 5: Keras + Mask R-CNN with Python of a picture from Ybor City."}, "8": {"item_id": "2629159612", "image_id": "8", "src": "https://www.pyimagesearch.com/wp-content/uploads/2020/01/cta-source-guide-1.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1286}, "2779314935": {"item_id": "2779314935", "resolved_id": "2779314935", "given_url": "https://github.com/kornia/kornia", "given_title": "kornia/kornia: Open Source Differentiable Computer Vision Library for PyTor", "favorite": "0", "status": "1", "time_added": "1578973729", "time_updated": "1608264752", "time_read": "1582142817", "time_favorited": "0", "sort_id": 47, "resolved_title": "Open Source Differentiable Computer Vision Library for PyTorch", "resolved_url": "https://github.com/kornia/kornia", "excerpt": "Kornia is a differentiable computer vision library for PyTorch. It consists of a set of routines and differentiable modules to solve generic computer vision problems.", "is_article": "1", "is_index": "1", "has_video": "0", "has_image": "1", "word_count": "483", "lang": "en", "top_image_url": "https://opengraph.githubassets.com/63b50a69e8ab122d37bd1a33cd090512165133a04894957344b8dd82f65d8216/kornia/kornia", "tags": {"pytorch": {"item_id": "2779314935", "tag": "pytorch"}, "vision": {"item_id": "2779314935", "tag": "vision"}}, "image": {"item_id": "2779314935", "src": "https://github.com/kornia/kornia/blob/master/docs/source/_static/img/kornia_logo.png", "width": "50", "height": "0"}, "images": {"1": {"item_id": "2779314935", "image_id": "1", "src": "https://github.com/kornia/kornia/blob/master/docs/source/_static/img/kornia_logo.png", "width": "50", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2779314935", "image_id": "2", "src": "https://camo.githubusercontent.com/0cc23d1c2df343a2e1311c0455127207437483715d8ce64dabf3533abb47cff7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6b6f726e6961", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2779314935", "image_id": "3", "src": "https://camo.githubusercontent.com/62413b3f716b956b722ebe64eeb5c6c1c7c5b1880c5f0aafb3abaa97749fce00/68747470733a2f2f62616467652e667572792e696f2f70792f6b6f726e69612e737667", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2779314935", "image_id": "4", "src": "https://camo.githubusercontent.com/a7bfb5443c90b645547e8164ccbdb20747216d0d8eccaacc1812941f34f8c1c6/68747470733a2f2f706570792e746563682f62616467652f6b6f726e6961", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2779314935", "image_id": "5", "src": "https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2779314935", "image_id": "6", "src": "https://camo.githubusercontent.com/8df26cc38dabf1035cddfbed79714744bb93785bc8341cb883fef4cdc412572d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f536c61636b2d3441313534423f6c6f676f3d736c61636b266c6f676f436f6c6f723d7768697465", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2779314935", "image_id": "7", "src": "https://camo.githubusercontent.com/0373ac8b17412d8ff181c95d8e1c161d89872a1730de3dccb6e4fd4a79d29578/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6b6f726e69615f666f73733f7374796c653d736f6369616c", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2779314935", "image_id": "8", "src": "https://github.com/kornia/kornia/actions/workflows/tests_cpu_versions.yml/badge.svg", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2779314935", "image_id": "9", "src": "https://github.com/kornia/kornia/actions/workflows/tests_cuda.yml/badge.svg", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2779314935", "image_id": "10", "src": "https://camo.githubusercontent.com/254f67cde5f9cd67837ee3d8d3fe36f61b9a530ba32e638e19a783f5b6a28b29/68747470733a2f2f636f6465636f762e696f2f67682f6b6f726e69612f6b6f726e69612f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d467a436237653042736f", "width": "0", "height": "0", "credit": "", "caption": ""}, "11": {"item_id": "2779314935", "image_id": "11", "src": "https://camo.githubusercontent.com/3dd6e103c9af571cb95c9b020f0fd31bb5476862c53fb10c141ca69899b9e75b/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6b6f726e69612f62616467652f3f76657273696f6e3d6c6174657374", "width": "0", "height": "0", "credit": "", "caption": ""}, "12": {"item_id": "2779314935", "image_id": "12", "src": "https://camo.githubusercontent.com/147c6adce2a56712a4c21e04d47972d5ad0c4381f8a95615774892052fbcc056/68747470733a2f2f726573756c74732e7072652d636f6d6d69742e63692f62616467652f6769746875622f6b6f726e69612f6b6f726e69612f6d61737465722e737667", "width": "0", "height": "0", "credit": "", "caption": ""}, "13": {"item_id": "2779314935", "image_id": "13", "src": "https://raw.githubusercontent.com/kornia/data/main/hello_world_arturito.png", "width": "75", "height": "75", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 187}, "2882841415": {"item_id": "2882841415", "resolved_id": "2882841415", "given_url": "http://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html", "given_title": "Learning to See Transparent Objects", "favorite": "0", "status": "1", "time_added": "1582807635", "time_updated": "1638708525", "time_read": "1583785007", "time_favorited": "0", "sort_id": 48, "resolved_title": "Learning to See Transparent Objects", "resolved_url": "http://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html", "excerpt": "Optical 3D range sensors, like RGB-D cameras and LIDAR, have found widespread use in robotics to generate rich and accurate 3D maps of the environment, from self-driving cars to autonomous manipulators.", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "1347", "lang": "en", "time_to_read": 6, "top_image_url": "http://2.bp.blogspot.com/-qRz-hnwUdY4/WulXSQ6Rv4I/AAAAAAAATvQ/shk7KsphA0c3E3nUMsDVASqYaH0PhLPNwCK4BGAYYCw/s1600/GoogleAI_logo_horizontal_color_rgb.png", "tags": {"deep-learning": {"item_id": "2882841415", "tag": "deep-learning"}, "vision": {"item_id": "2882841415", "tag": "vision"}}, "authors": {"83475403": {"item_id": "2882841415", "author_id": "83475403", "name": "Shreeyak Sajjan", "url": ""}}, "image": {"item_id": "2882841415", "src": "https://1.bp.blogspot.com/-VNB-xPguois/XkQ7sqF3VWI/AAAAAAAAFRc/ek6wUpjOGNw0u9vALnnW9zXCZ8VsWTjcACLcBGAsYHQ/s1600/image3.png", "width": "640", "height": "174"}, "images": {"1": {"item_id": "2882841415", "image_id": "1", "src": "https://1.bp.blogspot.com/-VNB-xPguois/XkQ7sqF3VWI/AAAAAAAAFRc/ek6wUpjOGNw0u9vALnnW9zXCZ8VsWTjcACLcBGAsYHQ/s1600/image3.png", "width": "640", "height": "174", "credit": "", "caption": ""}, "2": {"item_id": "2882841415", "image_id": "2", "src": "https://1.bp.blogspot.com/-mcdDuutDw5Q/XkQ7ssTGUvI/AAAAAAAAFRg/OIyqAa7Qbgs1yGWB7vFKL2E90hCPXiuYACEwYBhgL/s640/image2.gif", "width": "640", "height": "268", "credit": "", "caption": "Transparent objects often fail to be detected by optical 3D sensors. Top, Right: For instance, glass bottles do not show up in the 3D depth imagery captured from an Intel® RealSense™ D415 RGB-D camera. Bottom: A 3D visualization via point clouds constructed from the depth image."}, "3": {"item_id": "2882841415", "image_id": "3", "src": "https://1.bp.blogspot.com/-oncutBVkLQU/XkQ7rQz4zLI/AAAAAAAAFSI/Y1dSYj38GkgDTtw-P-UaP1e7pRzmC6izgCEwYBhgL/s1600/image11.png", "width": "640", "height": "232", "credit": "", "caption": "Some example data of transparent objects from the ClearGrasp synthetic dataset."}, "4": {"item_id": "2882841415", "image_id": "4", "src": "https://1.bp.blogspot.com/-QcyTESfw4AM/XkQ7rJaGdVI/AAAAAAAAFR8/CUPvkzCJuygr6xw9S36vYIQ1r0wfi3rogCEwYBhgL/s1600/image10.jpg", "width": "400", "height": "226", "credit": "", "caption": "Specular reflections on transparent objects create distinct features that vary based on the object shape and provide strong visual cues for estimating surface normals."}, "5": {"item_id": "2882841415", "image_id": "5", "src": "https://1.bp.blogspot.com/-xbiK9-km_BE/XkQ7rJmME0I/AAAAAAAAFSA/2kRSqd3DvlcOpv6ql4qILwn3HdmmryfBACEwYBhgL/s1600/image1.png", "width": "640", "height": "202", "credit": "", "caption": "Overview of our method. The point cloud was generated using the output depth and is colored with its surface normals."}, "6": {"item_id": "2882841415", "image_id": "6", "src": "https://1.bp.blogspot.com/-QBobUQ2ssow/XkQ7txFy4gI/AAAAAAAAFSI/wIR43QtxSd48ghg9bEOAyN617pqIfvsPwCEwYBhgL/s1600/image7.png", "width": "640", "height": "202", "credit": "MP+SN), b) our synthetic dataset only, and c", "caption": "Surface Normal estimation on real images when trained on a) Matterport3D and ScanNet only"}, "7": {"item_id": "2882841415", "image_id": "7", "src": "https://1.bp.blogspot.com/-iaTKEeKdlTA/XkQ7u6Mv6gI/AAAAAAAAFSE/_59HIFZptyoP9tDllG_0WVy2HKD4edPcwCEwYBhgL/s1600/image9.png", "width": "640", "height": "242", "credit": "", "caption": "Qualitative results on real images. Top two rows: results on known objects. Bottom two rows: results on novel objects. The point clouds, colored with their surface normals, are generated from the corresponding depth images."}, "8": {"item_id": "2882841415", "image_id": "8", "src": "https://1.bp.blogspot.com/-fHSXEBQAp_8/XkQ7uzz9jzI/AAAAAAAAFSI/FKoaYnuntS8vA9IhQCUduBrRyrEnjtVhQCEwYBhgL/s640/image8.gif", "width": "640", "height": "360", "credit": "the patterns of light that occur when light rays are reflected or refracted from a surface", "caption": "Manipulation of novel transparent objects using ClearGrasp. Note the challenging conditions: textureless background, complex object shapes and the directional light causing confusing shadows and caustics"}}, "videos": {"1": {"item_id": "2882841415", "video_id": "1", "src": "https://www.youtube.com/embed/lbmklphGgGE?rel=0&feature=player_embedded", "width": "640", "height": "360", "type": "1", "vid": "lbmklphGgGE", "length": "0"}}, "listen_duration_estimate": 521}, "2971132614": {"item_id": "2971132614", "resolved_id": "2971132628", "given_url": "https://towardsdatascience.com/lines-detection-with-hough-transform-84020b3b1549?source=rss----7f60cf5620c9---4", "given_title": "Lines Detection with Hough Transform", "favorite": "0", "status": "1", "time_added": "1588530206", "time_updated": "1612385105", "time_read": "1588808535", "time_favorited": "0", "sort_id": 49, "resolved_title": "Lines Detection with Hough Transform", "resolved_url": "https://towardsdatascience.com/lines-detection-with-hough-transform-84020b3b1549", "excerpt": "Note: You can read the Chinese version of this article here. Recently, I found myself having to incorporate a document scanner feature into an app. After doing some research, I came across an article written by Ying Xiong who was a member of the Dropbox’s machine learning team.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1326", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1200/1*pSAwZyau08leeYmTM9203Q.png", "tags": {"machine-learning": {"item_id": "2971132614", "tag": "machine-learning"}, "vision": {"item_id": "2971132614", "tag": "vision"}}, "authors": {"79489806": {"item_id": "2971132614", "author_id": "79489806", "name": "Socret Lee", "url": "https://medium.com/@socret.lee"}}, "image": {"item_id": "2971132614", "src": "https://miro.medium.com/fit/c/56/56/1*4o_qpFPgAV0GQY4V4gzOoQ@2x.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2971132614", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*4o_qpFPgAV0GQY4V4gzOoQ@2x.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2971132614", "image_id": "2", "src": "https://miro.medium.com/max/2000/1*pSAwZyau08leeYmTM9203Q.png", "width": "1000", "height": "544", "credit": "", "caption": "Line Detection using the Hough Transform Algorithm"}, "3": {"item_id": "2971132614", "image_id": "3", "src": "https://miro.medium.com/max/1280/1*dIx2EJj7ZQMNWSpoOHwefw.jpeg", "width": "640", "height": "175", "credit": "", "caption": "Canny Edge Detection Algorithm. Source: AI Shack"}, "4": {"item_id": "2971132614", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*Y9ljU_--SdFe7on9W_axrA.png", "width": "700", "height": "301", "credit": "", "caption": "Mapping from edge points to the Hough Space."}, "5": {"item_id": "2971132614", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*X1WO4jfw3qSsttBcfg2d3g.png", "width": "700", "height": "301", "credit": "", "caption": "The equation to calculate a slope of a line."}, "6": {"item_id": "2971132614", "image_id": "6", "src": "https://miro.medium.com/max/1400/1*Cr73Mte5NNgO16D4moKDQg.png", "width": "700", "height": "302", "credit": "", "caption": "An alternative representation of a straight line and its corresponding Hough Space."}, "7": {"item_id": "2971132614", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*F4IzvGyS6jSyuuUTOF0OcA.png", "width": "700", "height": "381", "credit": "", "caption": "The process of detecting lines in an image. The yellow dots in the Hough Space indicate that lines exist and is represented by the θ and ρ pairs."}, "8": {"item_id": "2971132614", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*gS6Sh6i8g535gOafY4Wl1w.png", "width": "700", "height": "197", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 513}, "1847534625": {"item_id": "1847534625", "resolved_id": "1847534625", "given_url": "https://github.com/LouieYang/deep-photo-styletransfer-tf", "given_title": "LouieYang/deep-photo-styletransfer-tf: Tensorflow (Python API) implementati", "favorite": "0", "status": "1", "time_added": "1519321521", "time_updated": "1638708525", "time_read": "1528501272", "time_favorited": "0", "sort_id": 50, "resolved_title": "deep-photo-styletransfer-tf", "resolved_url": "https://github.com/LouieYang/deep-photo-styletransfer-tf", "excerpt": "This implementation support L-BFGS-B (which is what the original authors used) and Adam in case the ScipyOptimizerInterface incompatible when Tensorflow upgrades to higher version. Additionally, there is no dependency on MATLAB thanks to another repository computing Matting Laplacian Sparse Matrix.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "645", "lang": "en", "time_to_read": 3, "top_image_url": "https://opengraph.githubassets.com/e1fe2ecba5f764951b9607795b239d3141d6c186eab888ac78c045dccdecc786/LouieYang/deep-photo-styletransfer-tf", "tags": {"deep-learning": {"item_id": "1847534625", "tag": "deep-learning"}, "vision": {"item_id": "1847534625", "tag": "vision"}}, "image": {"item_id": "1847534625", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best5.png", "width": "512", "height": "0"}, "images": {"1": {"item_id": "1847534625", "image_id": "1", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best5.png", "width": "512", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "1847534625", "image_id": "2", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/readme_examples/intar5.png", "width": "290", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "1847534625", "image_id": "3", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/input/in6.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "4": {"item_id": "1847534625", "image_id": "4", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/style/tar6.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "5": {"item_id": "1847534625", "image_id": "5", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/final_results/best6_t_1000.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "6": {"item_id": "1847534625", "image_id": "6", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best6.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "7": {"item_id": "1847534625", "image_id": "7", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/input/in7.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "8": {"item_id": "1847534625", "image_id": "8", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/style/tar7.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "9": {"item_id": "1847534625", "image_id": "9", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/final_results/best7_t_1000.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "10": {"item_id": "1847534625", "image_id": "10", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best7.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "11": {"item_id": "1847534625", "image_id": "11", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/input/in8.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "12": {"item_id": "1847534625", "image_id": "12", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/style/tar8.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "13": {"item_id": "1847534625", "image_id": "13", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/final_results/best8_t_1000.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "14": {"item_id": "1847534625", "image_id": "14", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best8.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "15": {"item_id": "1847534625", "image_id": "15", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/input/in9.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "16": {"item_id": "1847534625", "image_id": "16", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/style/tar9.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "17": {"item_id": "1847534625", "image_id": "17", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/final_results/best9_t_1000.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "18": {"item_id": "1847534625", "image_id": "18", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best9.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "19": {"item_id": "1847534625", "image_id": "19", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/input/in10.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "20": {"item_id": "1847534625", "image_id": "20", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/style/tar10.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "21": {"item_id": "1847534625", "image_id": "21", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/final_results/best10_t_1000.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "22": {"item_id": "1847534625", "image_id": "22", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best10.png", "width": "210", "height": "140", "credit": "", "caption": ""}, "23": {"item_id": "1847534625", "image_id": "23", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/input/in11.png", "width": "210", "height": "0", "credit": "", "caption": ""}, "24": {"item_id": "1847534625", "image_id": "24", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/style/tar11.png", "width": "210", "height": "0", "credit": "", "caption": ""}, "25": {"item_id": "1847534625", "image_id": "25", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/examples/final_results/best11_t_1000.png", "width": "210", "height": "0", "credit": "", "caption": ""}, "26": {"item_id": "1847534625", "image_id": "26", "src": "https://github.com/LouieYang/deep-photo-styletransfer-tf/blob/master/some_results/best11.png", "width": "210", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 250}, "3156392066": {"item_id": "3156392066", "resolved_id": "3156392066", "given_url": "https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/", "given_title": "Machine Learning Attack Series: Image Scaling Attacks · wunderwuzzi blog", "favorite": "0", "status": "1", "time_added": "1603980827", "time_updated": "1638708525", "time_read": "1604187210", "time_favorited": "0", "sort_id": 51, "resolved_title": "Machine Learning Attack Series: Image Scaling Attacks", "resolved_url": "https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/", "excerpt": "This post is part of a series about machine learning and artificial intelligence. Click on the blog tag “huskyai” to see related posts. The basic idea is to hide a smaller image inside a larger image (it should be about 5-10x the size). The attack is easy to explain actually:", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "463", "lang": "en", "tags": {"deep-learning": {"item_id": "3156392066", "tag": "deep-learning"}, "vision": {"item_id": "3156392066", "tag": "vision"}}, "image": {"item_id": "3156392066", "src": "https://embracethered.com/blog/images/2020/image-rescale-attack.gif", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3156392066", "image_id": "1", "src": "https://embracethered.com/blog/images/2020/image-rescale-attack.gif", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3156392066", "image_id": "2", "src": "https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3156392066", "image_id": "3", "src": "https://embracethered.com/blog/images/2020/image-rescaling-attack.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 179}, "1950743425": {"item_id": "1950743425", "resolved_id": "1950743425", "given_url": "https://technology.condenast.com/story/handbag-brand-and-color-detection", "given_title": "Machine Learning: Handbag Brand and Color Detection using Deep Neural Netwo", "favorite": "0", "status": "1", "time_added": "1510156068", "time_updated": "1638708525", "time_read": "1510184940", "time_favorited": "0", "sort_id": 52, "resolved_title": "Machine Learning at Condé Nast, Part 2: Handbag Brand and Color Detection", "resolved_url": "https://technology.condenast.com/story/handbag-brand-and-color-detection", "excerpt": "For a primer on Neural Network concepts, please visit our first post in this series. Over the past few years, we here at Condé Nast have invested heavily in building Machine Learning (ML) tools to help us understand our content and how our users interact with it.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "1595", "lang": "en", "time_to_read": 7, "amp_url": "https://technology.condenast.com/story/handbag-brand-and-color-detection/amp", "top_image_url": "https://media.condenast.io/photos/59fcc07c5faf870879f21f0a/master/pass/jacobs_hm.png", "tags": {"deep-learning": {"item_id": "1950743425", "tag": "deep-learning"}, "ecommerce": {"item_id": "1950743425", "tag": "ecommerce"}, "machine-learning": {"item_id": "1950743425", "tag": "machine-learning"}, "vision": {"item_id": "1950743425", "tag": "vision"}}, "authors": {"78104804": {"item_id": "1950743425", "author_id": "78104804", "name": "Johan Edvinsson", "url": "https://technology.condenast.com/contributor/johan-edvinsson"}}, "listen_duration_estimate": 617}, "2972103631": {"item_id": "2972103631", "resolved_id": "2972103667", "given_url": "https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-1-of-2-732712631047?source=rss----7f60cf5620c9---4", "given_title": "Master the COCO Dataset for Semantic Image Segmentation", "favorite": "0", "status": "1", "time_added": "1588614655", "time_updated": "1638708525", "time_read": "1589542049", "time_favorited": "0", "sort_id": 53, "resolved_title": "Master the COCO Dataset for Semantic Image Segmentation", "resolved_url": "https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-1-of-2-732712631047", "excerpt": ", being one of the most popular image datasets out there, with appliations like object detection, segmentation, and captioning - it is quite surprising how few comprehensive but simple, end-to-end tutorials exist. When I first started out with this dataset, I was quite lost and intimidated.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1339", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1200/1*taqoyoo-5DlTvktWYM21tQ.jpeg", "tags": {"deep-learning": {"item_id": "2972103631", "tag": "deep-learning"}, "vision": {"item_id": "2972103631", "tag": "vision"}}, "authors": {"133317569": {"item_id": "2972103631", "author_id": "133317569", "name": "Viraf", "url": "https://medium.com/@virafpatrawala"}}, "image": {"item_id": "2972103631", "src": "https://miro.medium.com/fit/c/56/56/2*R04h_eboCIBsrrwArUc5aA.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2972103631", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*R04h_eboCIBsrrwArUc5aA.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2972103631", "image_id": "2", "src": "https://miro.medium.com/max/12000/1*taqoyoo-5DlTvktWYM21tQ.jpeg", "width": "6000", "height": "4000", "credit": "Original photo by Ylanite Koppens from Pexels", "caption": "Did you mean COCOA?"}, "3": {"item_id": "2972103631", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*vP5WkxR8oY2Iz0Ox0tXW1w.jpeg", "width": "700", "height": "213", "credit": "", "caption": "The COCO Dataset"}, "4": {"item_id": "2972103631", "image_id": "4", "src": "https://miro.medium.com/max/1000/1*ShJDpj-tEmuxJ0BdvsvIOw.png", "width": "500", "height": "375", "credit": "", "caption": "An example image from the dataset."}, "5": {"item_id": "2972103631", "image_id": "5", "src": "https://miro.medium.com/max/608/1*QZd5XQXNiy9EnL1BXzSXXw.png", "width": "304", "height": "231", "credit": "", "caption": "A sample image containing the filtered output classes."}, "6": {"item_id": "2972103631", "image_id": "6", "src": "https://miro.medium.com/max/608/1*C0BP5dLinScCUjH3DrNhNA.png", "width": "304", "height": "231", "credit": "", "caption": "Annotations for filtered classes neatly drawn out."}, "7": {"item_id": "2972103631", "image_id": "7", "src": "https://miro.medium.com/max/660/1*VIBddw3Pj29rATNbc4rk9g.png", "width": "330", "height": "252", "credit": "", "caption": "A normal 2-channel mask for semantic segmentation."}, "8": {"item_id": "2972103631", "image_id": "8", "src": "https://miro.medium.com/max/660/1*DlLjP-00rEHjVMNhvE1wPQ.png", "width": "330", "height": "252", "credit": "", "caption": "A binary semantic segmentation mask. Yellow represents pixel value 1, violet represents pixel value 0."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 518}, "2973971072": {"item_id": "2973971072", "resolved_id": "2973971128", "given_url": "https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-2-of-2-c0d1f593096a?source=rss----7f60cf5620c9---4", "given_title": "Master the COCO Dataset for Semantic Image Segmentation", "favorite": "0", "status": "1", "time_added": "1588764125", "time_updated": "1638708525", "time_read": "1589542035", "time_favorited": "0", "sort_id": 54, "resolved_title": "Master the COCO Dataset for Semantic Image Segmentation", "resolved_url": "https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-2-of-2-c0d1f593096a", "excerpt": "For new readers, you can find Part 1 of this series here. I strongly recommend going through it to better understand the following article.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1462", "lang": "en", "time_to_read": 7, "top_image_url": "https://miro.medium.com/max/1200/1*hki1s4QtFbvYmQV_TrvWAQ.jpeg", "tags": {"deep-learning": {"item_id": "2973971072", "tag": "deep-learning"}, "vision": {"item_id": "2973971072", "tag": "vision"}}, "authors": {"133317569": {"item_id": "2973971072", "author_id": "133317569", "name": "Viraf", "url": "https://medium.com/@virafpatrawala"}}, "image": {"item_id": "2973971072", "src": "https://miro.medium.com/max/1400/1*hki1s4QtFbvYmQV_TrvWAQ.jpeg", "width": "700", "height": "467"}, "images": {"1": {"item_id": "2973971072", "image_id": "1", "src": "https://miro.medium.com/max/1400/1*hki1s4QtFbvYmQV_TrvWAQ.jpeg", "width": "700", "height": "467", "credit": "Original photo by Craig Adderley from Pexels", "caption": "Have you gone COCOnuts?!"}, "2": {"item_id": "2973971072", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*lmHKDnlqSML3mrNo6WVKHg.png", "width": "700", "height": "342", "credit": "", "caption": "Visualization of the images generated by the generator for mask_type=’binary’."}, "3": {"item_id": "2973971072", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*FX-sdv4ZJiYbty_xThD8qw.png", "width": "700", "height": "342", "credit": "", "caption": "Visualization of the images generated by the generator for mask_type=’normal’."}, "4": {"item_id": "2973971072", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*kxJ9729dpVC-4RNuWjrD7A.png", "width": "700", "height": "342", "credit": "", "caption": "Visualization of the images generated by the augmentations generator for mask_type=’binary’."}, "5": {"item_id": "2973971072", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*lrjlS18mS2qALnfI-d_Uow.png", "width": "700", "height": "342", "credit": "", "caption": "Visualization of the images generated by the augmentations generator for mask_type=’normal’."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 566}, "659566663": {"item_id": "659566663", "resolved_id": "659566663", "given_url": "https://www.nngroup.com/articles/recognition-and-recall/", "given_title": "Memory Recognition and Recall in User Interfaces", "favorite": "0", "status": "1", "time_added": "1705409517", "time_updated": "1705475989", "time_read": "1705475989", "time_favorited": "0", "sort_id": 55, "resolved_title": "Memory Recognition and Recall in User Interfaces", "resolved_url": "https://www.nngroup.com/articles/recognition-and-recall/", "excerpt": "Summary: Showing users things they can recognize improves usability over needing to recall items from scratch because the extra context helps users retrieve information from memory. Download a free poster of Jakob’s Usability Heuristic #6 at the bottom of this article.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2089", "lang": "en", "time_to_read": 9, "top_image_url": "https://media.nngroup.com/media/articles/opengraph_images/Slide06articlesrecognition-and-recall.png", "tags": {"memory-recall": {"item_id": "659566663", "tag": "memory-recall"}, "ui-ux": {"item_id": "659566663", "tag": "ui-ux"}, "vision": {"item_id": "659566663", "tag": "vision"}}, "authors": {"17161532": {"item_id": "659566663", "author_id": "17161532", "name": "Raluca Budiu", "url": ""}}, "image": {"item_id": "659566663", "src": "https://s3.amazonaws.com/media.nngroup.com/media/editor/2014/05/12/2014-05-12-bing1.png", "width": "720", "height": "455"}, "images": {"1": {"item_id": "659566663", "image_id": "1", "src": "https://s3.amazonaws.com/media.nngroup.com/media/editor/2014/05/12/2014-05-12-bing1.png", "width": "720", "height": "455", "credit": "", "caption": "Bing has a link to the user’s search history. The link helps users remember previous searches."}, "2": {"item_id": "659566663", "image_id": "2", "src": "https://s3.amazonaws.com/media.nngroup.com/media/editor/2014/05/12/2014-5-12-amazon.png", "width": "720", "height": "76", "credit": "", "caption": "When a user goes back to Amazon.com, the personalized homepage includes a list of recently viewed items."}, "3": {"item_id": "659566663", "image_id": "3", "src": "https://s3.amazonaws.com/media.nngroup.com/media/editor/2014/05/12/mailbox1.png", "width": "720", "height": "540", "credit": "with the exception of the X perhaps", "caption": "Mailbox is an email app for iPad. The icons at the top of the screen do not promote recognition: it’s hard for users to recognize what those buttons might mean"}}, "listen_duration_estimate": 809}, "3098126871": {"item_id": "3098126871", "resolved_id": "3098126892", "given_url": "https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e?source=rss----7f60cf5620c9---4", "given_title": "New Approaches to Object Detection", "favorite": "0", "status": "1", "time_added": "1598988620", "time_updated": "1638708525", "time_read": "1599011208", "time_favorited": "0", "sort_id": 56, "resolved_title": "New Approaches to Object Detection", "resolved_url": "https://towardsdatascience.com/new-approaches-to-object-detection-f5cbc925e00e", "excerpt": "I will start with a short introduction of different approaches to object detection. After both traditional and newer approaches are presented, you can read about the most important parts of CenterNet and TTFNet. Many ideas in both models are similar, therefore they will be introduced together.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1205", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1200/1*_ssXB-7gYmIwif0mxjncYg.jpeg", "tags": {"deep-learning": {"item_id": "3098126871", "tag": "deep-learning"}, "vision": {"item_id": "3098126871", "tag": "vision"}}, "authors": {"152215953": {"item_id": "3098126871", "author_id": "152215953", "name": "Libor Vanek", "url": "https://libor-vanek.medium.com"}}, "image": {"item_id": "3098126871", "src": "https://miro.medium.com/fit/c/56/56/1*YUYMuqs815KxE1iR6QtVJQ.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3098126871", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*YUYMuqs815KxE1iR6QtVJQ.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3098126871", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*_ssXB-7gYmIwif0mxjncYg.jpeg", "width": "700", "height": "394", "credit": "", "caption": "source: pexels.com"}, "3": {"item_id": "3098126871", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*QSdw1M6FkmarZXbD0PQs_A.jpeg", "width": "700", "height": "333", "credit": "", "caption": "source: pexels.com"}, "4": {"item_id": "3098126871", "image_id": "4", "src": "https://miro.medium.com/max/4400/1*60ROU3IyeI3U8ryAUXs-2A.png", "width": "2200", "height": "617", "credit": "Yellow: convolutional layer, red: max pooling, blue: upsampling.", "caption": "Simplified visualization of CenterNet with ResNet18, using upsampling and concatenation."}, "5": {"item_id": "3098126871", "image_id": "5", "src": "https://miro.medium.com/max/1084/1*lVma1W94MGETfUVaGEpF0g.png", "width": "542", "height": "256", "credit": "left", "caption": "A heatmap for CenterNet"}, "6": {"item_id": "3098126871", "image_id": "6", "src": "https://miro.medium.com/max/1084/1*khKe6aSlYkjjzUaCzRbhVw.png", "width": "542", "height": "238", "credit": "", "caption": "Selection of values for standard vs deformable convolution.FI"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 466}, "3141820159": {"item_id": "3141820159", "resolved_id": "3141820159", "given_url": "https://www.microsoft.com/en-us/research/blog/novel-object-captioning-surpasses-human-performance-on-benchmarks/", "given_title": "Novel object captioning surpasses human performance on benchmarks", "favorite": "0", "status": "1", "time_added": "1602753845", "time_updated": "1638708525", "time_read": "1604361029", "time_favorited": "0", "sort_id": 57, "resolved_title": "Novel object captioning surpasses human performance on benchmarks", "resolved_url": "https://www.microsoft.com/en-us/research/blog/novel-object-captioning-surpasses-human-performance-on-benchmarks/", "excerpt": "Consider for a moment what it takes to visually identify and describe something to another person. Now imagine that the other person can’t see the object or image, so every detail matters.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1463", "lang": "en", "time_to_read": 7, "top_image_url": "https://www.microsoft.com/en-us/research/uploads/prod/2020/10/1200x627_NoCaps_WithLogo_Still.png", "tags": {"deep-learning": {"item_id": "3141820159", "tag": "deep-learning"}, "object-detection": {"item_id": "3141820159", "tag": "object-detection"}, "vision": {"item_id": "3141820159", "tag": "vision"}}, "authors": {"141349258": {"item_id": "3141820159", "author_id": "141349258", "name": "Kevin Lin", "url": "https://www.microsoft.com/en-us/research/people/keli/"}}, "image": {"item_id": "3141820159", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/10/NOCAPS_Figure1.png", "width": "624", "height": "361"}, "images": {"1": {"item_id": "3141820159", "image_id": "1", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/10/NOCAPS_Figure1.png", "width": "624", "height": "361", "credit": "", "caption": ""}, "2": {"item_id": "3141820159", "image_id": "2", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/10/Nocaps_figure2_updateres-1024x579.jpg", "width": "1024", "height": "579", "credit": "", "caption": ""}, "3": {"item_id": "3141820159", "image_id": "3", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_WebinarCollage1400x788.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3141820159", "image_id": "4", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/10/Nocaps_figre-3_updatedres-1024x652.jpg", "width": "1024", "height": "652", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Microsoft", "logo": "https://logo.clearbit.com/microsoft.com?size=800", "greyscale_logo": "https://logo.clearbit.com/microsoft.com?size=800&greyscale=true"}, "listen_duration_estimate": 566}, "2910367561": {"item_id": "2910367561", "resolved_id": "2910357773", "given_url": "https://towardsdatascience.com/object-detection-using-yolov3-and-opencv-19ee0792a420?source=rss----7f60cf5620c9---4", "given_title": "Object Detection using YoloV3 and OpenCV", "favorite": "0", "status": "1", "time_added": "1583795188", "time_updated": "1638708525", "time_read": "1585739876", "time_favorited": "0", "sort_id": 58, "resolved_title": "Object Detection using YoloV3 and OpenCV", "resolved_url": "https://towardsdatascience.com/object-detection-using-yolov3-and-opencv-19ee0792a420", "excerpt": "Computer Vision has always been a topic of fascination for me. In layman's terms, computer vision is all about replicating the complexity of the human vision and his understanding of his surroundings. It is emerging to be one of the most powerful fields of application of AI.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1141", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1200/1*pOBd3QvpLL88gurjZ8g_GQ.jpeg", "tags": {"deep-learning": {"item_id": "2910367561", "tag": "deep-learning"}, "object-detection": {"item_id": "2910367561", "tag": "object-detection"}, "vision": {"item_id": "2910367561", "tag": "vision"}}, "authors": {"152894092": {"item_id": "2910367561", "author_id": "152894092", "name": "Nandini Bansal", "url": "https://nandinibansal1811.medium.com"}}, "image": {"item_id": "2910367561", "src": "https://miro.medium.com/fit/c/56/56/0*et5WzdK_dJ0FhQSg.jpg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2910367561", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/0*et5WzdK_dJ0FhQSg.jpg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2910367561", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*pOBd3QvpLL88gurjZ8g_GQ.jpeg", "width": "700", "height": "394", "credit": "", "caption": "Object Detection using YoloV3 and OpenCV"}, "3": {"item_id": "2910367561", "image_id": "3", "src": "https://miro.medium.com/max/1160/1*C_9fyPCt15vw0tjUZuu7lQ.jpeg", "width": "580", "height": "418", "credit": "", "caption": "Object Detection with Multiple Bounding Boxes"}, "4": {"item_id": "2910367561", "image_id": "4", "src": "https://miro.medium.com/max/1160/1*AlCFUk7x4ZJbZocVrWiNzw.jpeg", "width": "580", "height": "418", "credit": "", "caption": "Final Output"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 442}, "3098605006": {"item_id": "3098605006", "resolved_id": "3098600472", "given_url": "https://towardsdatascience.com/oil-storage-tanks-volume-occupancy-on-satellite-imagery-using-yolov3-3cf251362d9d?source=rss----7f60cf5620c9---4", "given_title": "Oil Storage Tank’s Volume Occupancy On Satellite Imagery Using YoloV3", "favorite": "0", "status": "1", "time_added": "1599042450", "time_updated": "1638708525", "time_read": "1599042960", "time_favorited": "0", "sort_id": 59, "resolved_title": "Oil Storage Tank’s Volume Occupancy On Satellite Imagery Using YoloV3", "resolved_url": "https://towardsdatascience.com/oil-storage-tanks-volume-occupancy-on-satellite-imagery-using-yolov3-3cf251362d9d", "excerpt": "Oil Storage Tank’s Volume Occupancy On Satellite Imagery Using YoloV3Recognition of Oil Storage Tanks in satellite images using the Yolov3 object detection model from scratch using Tensorflow 2.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3704", "lang": "en", "time_to_read": 17, "top_image_url": "https://miro.medium.com/max/1200/1*hJSg0vEbU2MRs4a9UPOsKg.jpeg", "tags": {"deep-learning": {"item_id": "3098605006", "tag": "deep-learning"}, "vision": {"item_id": "3098605006", "tag": "vision"}}, "authors": {"137650792": {"item_id": "3098605006", "author_id": "137650792", "name": "Md. Mubasir", "url": "https://medium.com/@talk2mubasir0587"}}, "image": {"item_id": "3098605006", "src": "https://miro.medium.com/fit/c/56/56/1*4vTf4g_Jp4R1_DT4ZWu0Jg.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3098605006", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*4vTf4g_Jp4R1_DT4ZWu0Jg.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3098605006", "image_id": "2", "src": "https://miro.medium.com/max/9590/1*hJSg0vEbU2MRs4a9UPOsKg.jpeg", "width": "4795", "height": "1502", "credit": "", "caption": "source"}, "3": {"item_id": "3098605006", "image_id": "3", "src": "https://miro.medium.com/max/1400/0*jtf7E2TfLJ6Pzjve", "width": "700", "height": "466", "credit": "NASA on Unsplash", "caption": ""}, "4": {"item_id": "3098605006", "image_id": "4", "src": "https://miro.medium.com/max/1210/1*62qgv4v-X-fK_VmC6Z4n_Q.png", "width": "605", "height": "251", "credit": "", "caption": "source"}, "5": {"item_id": "3098605006", "image_id": "5", "src": "https://miro.medium.com/max/878/1*AhQhu34KcnU-VOSzqnKumg.png", "width": "439", "height": "419", "credit": "", "caption": "source"}, "6": {"item_id": "3098605006", "image_id": "6", "src": "https://miro.medium.com/max/1356/1*y7PZzHVzz_eVsjfhVi3big.png", "width": "678", "height": "313", "credit": "", "caption": "source"}, "7": {"item_id": "3098605006", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*8MgEQLskukMGjhJXbZgVtw.jpeg", "width": "700", "height": "115", "credit": "", "caption": "source"}, "8": {"item_id": "3098605006", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*4Yk1CKX8fktt3GLcW83d1A.png", "width": "700", "height": "543", "credit": "", "caption": "source"}, "9": {"item_id": "3098605006", "image_id": "9", "src": "https://miro.medium.com/max/2000/1*vfYEhTl820FF_9uEvoGf7Q.png", "width": "1000", "height": "414", "credit": "", "caption": "source"}, "10": {"item_id": "3098605006", "image_id": "10", "src": "https://miro.medium.com/max/1400/1*TLE1XuWsJT0F1OuffOaeTA.png", "width": "700", "height": "712", "credit": "", "caption": ""}, "11": {"item_id": "3098605006", "image_id": "11", "src": "https://miro.medium.com/max/1172/1*5V7kKiWgWHXE9_Dc_etxsw.png", "width": "586", "height": "767", "credit": "", "caption": "Image by author"}, "12": {"item_id": "3098605006", "image_id": "12", "src": "https://miro.medium.com/max/2000/1*DHXkTEXQQWXHO4PiFiSkfw.png", "width": "1000", "height": "365", "credit": "", "caption": "Image by author"}, "13": {"item_id": "3098605006", "image_id": "13", "src": "https://miro.medium.com/max/1174/1*j0cLsyYFS4Mid75fGiwBUw.png", "width": "587", "height": "218", "credit": "", "caption": "Image by author"}, "14": {"item_id": "3098605006", "image_id": "14", "src": "https://miro.medium.com/max/2000/1*8lnJQIE_bpaK6R5B2BALyQ.png", "width": "1000", "height": "442", "credit": "", "caption": "Image by author"}, "15": {"item_id": "3098605006", "image_id": "15", "src": "https://miro.medium.com/max/1400/1*eENhWw9vTYuwgg8k-J5GtQ.png", "width": "700", "height": "268", "credit": "", "caption": "Image by author"}, "16": {"item_id": "3098605006", "image_id": "16", "src": "https://miro.medium.com/max/1400/1*TpzoU_2gJnbKT7OxnnTw8A.png", "width": "700", "height": "343", "credit": "", "caption": "Image by author"}, "17": {"item_id": "3098605006", "image_id": "17", "src": "https://miro.medium.com/max/1400/1*lnwrh8kBQCEOPJA03at3aQ.png", "width": "700", "height": "492", "credit": "", "caption": ""}, "18": {"item_id": "3098605006", "image_id": "18", "src": "https://miro.medium.com/max/2000/1*1ZovyQgriyXdPmKG_JqYNw.png", "width": "1000", "height": "351", "credit": "", "caption": "Image by author"}, "19": {"item_id": "3098605006", "image_id": "19", "src": "https://miro.medium.com/max/1068/1*TQnV__vNvwqQoydzfQ6IuA.png", "width": "534", "height": "347", "credit": "", "caption": "source"}, "20": {"item_id": "3098605006", "image_id": "20", "src": "https://miro.medium.com/max/1304/1*nLHXhLJAd0lfDPPQMbNORg.png", "width": "652", "height": "123", "credit": "", "caption": "Image by author"}, "21": {"item_id": "3098605006", "image_id": "21", "src": "https://miro.medium.com/max/1170/1*cQRtTqqkui65KCqZ9_TGDg.png", "width": "585", "height": "127", "credit": "", "caption": "Image by author"}, "22": {"item_id": "3098605006", "image_id": "22", "src": "https://miro.medium.com/max/1400/1*yXzL7jVZfJ9Lpdu31biq9Q.png", "width": "700", "height": "324", "credit": "", "caption": "Image by author"}, "23": {"item_id": "3098605006", "image_id": "23", "src": "https://miro.medium.com/max/1400/1*-twOa0WG507U0wchAwsRcA.png", "width": "700", "height": "325", "credit": "", "caption": "Image by author"}, "24": {"item_id": "3098605006", "image_id": "24", "src": "https://miro.medium.com/max/2000/1*ij5Trvv_xxWCrH95zQN7Uw.png", "width": "1000", "height": "1000", "credit": "", "caption": "Image by author"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1434}, "3170451341": {"item_id": "3170451341", "resolved_id": "3170451362", "given_url": "https://towardsdatascience.com/practical-guide-to-entity-resolution-part-5-5ecdd0005470?source=rss----7f60cf5620c9---4", "given_title": "Practical Guide to Entity Resolution — part 5", "favorite": "0", "status": "1", "time_added": "1605137900", "time_updated": "1638708525", "time_read": "1608290449", "time_favorited": "0", "sort_id": 60, "resolved_title": "Practical Guide to Entity Resolution", "resolved_url": "https://towardsdatascience.com/practical-guide-to-entity-resolution-part-5-5ecdd0005470", "excerpt": "This is part 5 of a mini-series on entity resolution. Check out part 1, part 2, part 3, part 4 if you missed it In most real world ER use cases, there is no ground truth on which candidate pair should match and which should not match.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "967", "lang": "en", "time_to_read": 4, "top_image_url": "https://miro.medium.com/max/1200/0*0qWXV_wUH8mA_S8n", "tags": {"deep-learning": {"item_id": "3170451341", "tag": "deep-learning"}, "entity-resolution": {"item_id": "3170451341", "tag": "entity-resolution"}, "vision": {"item_id": "3170451341", "tag": "vision"}}, "authors": {"142254521": {"item_id": "3170451341", "author_id": "142254521", "name": "Yifei Huang", "url": "https://yifei-huang.medium.com"}}, "image": {"item_id": "3170451341", "src": "https://miro.medium.com/fit/c/56/56/1*c01BdvUOn9xPafDWjDKy2A.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3170451341", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*c01BdvUOn9xPafDWjDKy2A.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3170451341", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*0qWXV_wUH8mA_S8n", "width": "700", "height": "472", "credit": "Laura Ockel on Unsplash", "caption": ""}, "3": {"item_id": "3170451341", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*yRokOvNUlhS1pv13lrOMwQ.png", "width": "700", "height": "238", "credit": "", "caption": "score iteration learning loop"}, "4": {"item_id": "3170451341", "image_id": "4", "src": "https://miro.medium.com/max/1400/1*XT8AslWjcIVoYeYHP4Q5ug.png", "width": "700", "height": "420", "credit": "", "caption": "Example outputs from the sampled candidate pairs"}, "5": {"item_id": "3170451341", "image_id": "5", "src": "https://miro.medium.com/max/1400/1*_ilNg_Z_iEmWmpP1k6HInA.png", "width": "700", "height": "313", "credit": "", "caption": "Sampled output with modeled based match probability"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 374}, "1895214690": {"item_id": "1895214690", "resolved_id": "1895214690", "given_url": "https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/#", "given_title": "Real-time object detection with deep learning and OpenCV - PyImageSearch", "favorite": "0", "status": "1", "time_added": "1523289629", "time_updated": "1612385265", "time_read": "1528501186", "time_favorited": "0", "sort_id": 61, "resolved_title": "Real-time object detection with deep learning and OpenCV", "resolved_url": "https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/", "excerpt": "Today’s blog post was inspired by PyImageSearch reader, Emmanuel. Emmanuel emailed me after last week’s tutorial on object detection with deep learning + OpenCV and asked: What is the best way to do this?", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "2167", "lang": "en", "time_to_read": 10, "top_image_url": "https://www.pyimagesearch.com/wp-content/uploads/2017/09/real_time_object_detection_featured.jpg", "tags": {"object-detection": {"item_id": "1895214690", "tag": "object-detection"}, "vision": {"item_id": "1895214690", "tag": "vision"}}, "authors": {"76030979": {"item_id": "1895214690", "author_id": "76030979", "name": "Adrian Rosebrock", "url": "https://www.pyimagesearch.com/author/adrian/"}}, "image": {"item_id": "1895214690", "src": "https://www.pyimagesearch.com/wp-content/uploads/2020/01/source-code-icon.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "1895214690", "image_id": "1", "src": "https://www.pyimagesearch.com/wp-content/uploads/2020/01/source-code-icon.png", "width": "0", "height": "0", "credit": "", "caption": "Figure 1: A short clip of real-time object detection with deep learning and OpenCV + Python."}, "2": {"item_id": "1895214690", "image_id": "2", "src": "https://www.pyimagesearch.com/wp-content/uploads/2020/01/cta-source-guide-1.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "1895214690", "video_id": "1", "src": "https://www.youtube.com/embed/hMFx1TXjAJc?feature=oembed", "width": "630", "height": "354", "type": "1", "vid": "hMFx1TXjAJc", "length": "0"}}, "listen_duration_estimate": 839}, "1863447707": {"item_id": "1863447707", "resolved_id": "1863447707", "given_url": "https://blog.deepsense.ai/region-of-interest-pooling-explained/", "given_title": "Region of interest pooling explained", "favorite": "1", "status": "1", "time_added": "1517098871", "time_updated": "1638708525", "time_read": "1517594931", "time_favorited": "1517594930", "sort_id": 62, "resolved_title": "Region of interest pooling explained", "resolved_url": "https://blog.deepsense.ai/region-of-interest-pooling-explained/", "excerpt": "Region of interest pooling (also known as RoI pooling) is an operation widely used in object detection tasks using convolutional neural networks. For example, to detect multiple cars and pedestrians in a single image.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1228", "lang": "en", "time_to_read": 6, "top_image_url": "https://blog.deepsense.ai/wp-content/uploads/2017/02/region-of-interest-pooling-explained.jpg", "tags": {"deep-learning": {"item_id": "1863447707", "tag": "deep-learning"}, "vision": {"item_id": "1863447707", "tag": "vision"}}, "authors": {"74242860": {"item_id": "1863447707", "author_id": "74242860", "name": "Tomasz Grel", "url": "https://blog.deepsense.ai/author/tomasz-grel/"}}, "image": {"item_id": "1863447707", "src": "https://blog.deepsense.ai/wp-content/uploads/2017/02/region_proposal_cat.png", "width": "300", "height": "300"}, "images": {"1": {"item_id": "1863447707", "image_id": "1", "src": "https://blog.deepsense.ai/wp-content/uploads/2017/02/region_proposal_cat.png", "width": "300", "height": "300", "credit": "", "caption": ""}, "2": {"item_id": "1863447707", "image_id": "2", "src": "https://blog.deepsense.ai/wp-content/uploads/2017/02/1.jpg", "width": "800", "height": "600", "credit": "", "caption": ""}, "3": {"item_id": "1863447707", "image_id": "3", "src": "https://blog.deepsense.ai/wp-content/uploads/2017/02/2.jpg", "width": "800", "height": "600", "credit": "", "caption": ""}, "4": {"item_id": "1863447707", "image_id": "4", "src": "https://blog.deepsense.ai/wp-content/uploads/2017/02/3.jpg", "width": "800", "height": "600", "credit": "", "caption": ""}, "5": {"item_id": "1863447707", "image_id": "5", "src": "https://blog.deepsense.ai/wp-content/uploads/2017/02/output.jpg", "width": "200", "height": "200", "credit": "", "caption": ""}}, "listen_duration_estimate": 475}, "2901453769": {"item_id": "2901453769", "resolved_id": "2901453796", "given_url": "https://towardsdatascience.com/self-supervised-depth-estimation-breaking-down-the-ideas-f212e4f05ffa?source=rss----7f60cf5620c9---4", "given_title": "Self Supervised Depth Estimation: Breaking down the ideas", "favorite": "0", "status": "1", "time_added": "1583073556", "time_updated": "1612385105", "time_read": "1583785007", "time_favorited": "0", "sort_id": 63, "resolved_title": "SFM Self Supervised Depth Estimation: Breaking Down The Ideas", "resolved_url": "https://towardsdatascience.com/self-supervised-depth-estimation-breaking-down-the-ideas-f212e4f05ffa", "excerpt": "This post is dedicated to exploring the idea of depth estimation via self supervise learning. Some conceptual ideas about depth estimation serve as a prerequisite. You can refer to this post which discusses the relevant topics and associated problems.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3104", "lang": "en", "time_to_read": 14, "top_image_url": "https://miro.medium.com/max/1200/1*hoVbXvHz-qNZ5u3dxBWR9g.png", "tags": {"machine-learning": {"item_id": "2901453769", "tag": "machine-learning"}, "vision": {"item_id": "2901453769", "tag": "vision"}}, "authors": {"146689907": {"item_id": "2901453769", "author_id": "146689907", "name": "Daryl Tan", "url": "https://daryl-tan.medium.com"}}, "image": {"item_id": "2901453769", "src": "https://miro.medium.com/fit/c/56/56/2*HNVsBoEb2aYvPS_Mitihog.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2901453769", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*HNVsBoEb2aYvPS_Mitihog.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2901453769", "image_id": "2", "src": "https://miro.medium.com/max/7416/1*hoVbXvHz-qNZ5u3dxBWR9g.png", "width": "3708", "height": "1100", "credit": "", "caption": ""}, "3": {"item_id": "2901453769", "image_id": "3", "src": "https://miro.medium.com/max/960/1*FVBlpYzJYNzQ2NENci_0KQ.gif", "width": "480", "height": "206", "credit": "", "caption": ""}, "4": {"item_id": "2901453769", "image_id": "4", "src": "https://miro.medium.com/max/960/1*Fd5x2dTT9nI6E4ODwj_qzw.gif", "width": "480", "height": "270", "credit": "", "caption": "Scene reconstructed from RGBD sequence"}, "5": {"item_id": "2901453769", "image_id": "5", "src": "https://miro.medium.com/max/872/1*jR6za9H8oM1SeGPoaUHC4Q.jpeg", "width": "436", "height": "442", "credit": "", "caption": "Self Supervised Depth Estimation Pipeline"}, "6": {"item_id": "2901453769", "image_id": "6", "src": "https://miro.medium.com/max/872/1*HENzoBqZzy6RKWbT2JBV4w.jpeg", "width": "436", "height": "442", "credit": "", "caption": "SFM Pipeline"}, "7": {"item_id": "2901453769", "image_id": "7", "src": "https://miro.medium.com/max/1216/1*bIWguzoKjgAykr6_iCG0OQ.png", "width": "608", "height": "172", "credit": "", "caption": "Depth inference pipeline"}, "8": {"item_id": "2901453769", "image_id": "8", "src": "https://miro.medium.com/max/1082/1*1c11cu3fNMceq9Az0PwWBw.png", "width": "541", "height": "295", "credit": "", "caption": "Pose inference pipeline"}, "9": {"item_id": "2901453769", "image_id": "9", "src": "https://miro.medium.com/max/3484/1*35IavQnuhxItF3PluznQYw.png", "width": "1742", "height": "502", "credit": "", "caption": "View synthesis pipeline"}, "10": {"item_id": "2901453769", "image_id": "10", "src": "https://miro.medium.com/max/1410/1*LAIVUsMDVmOdYrXdaQJeLA.png", "width": "705", "height": "541", "credit": "", "caption": ""}, "11": {"item_id": "2901453769", "image_id": "11", "src": "https://miro.medium.com/max/2416/1*0EWvpDi46imm9DiKvAVfSg.png", "width": "1208", "height": "481", "credit": "", "caption": "The figure shows the pixel-wise absolute difference between prediction and target image, it is observed that the model struggle near boundaries and thin elongated structures. Higher intensity in L1 error map denotes higher absolute difference."}, "12": {"item_id": "2901453769", "image_id": "12", "src": "https://miro.medium.com/max/2172/1*VWv0KKGFaO9flarXSM_kpQ.png", "width": "1086", "height": "194", "credit": "", "caption": "Source"}, "13": {"item_id": "2901453769", "image_id": "13", "src": "https://miro.medium.com/max/976/1*tsysHO1rNbI3rUMKeyVZOg.png", "width": "488", "height": "480", "credit": "", "caption": ""}, "14": {"item_id": "2901453769", "image_id": "14", "src": "https://miro.medium.com/max/1228/1*Ok7r4I9wT-pyVnLFw3RQ8w.png", "width": "614", "height": "156", "credit": "", "caption": "Scenarios that are not consistent in SFM setting due to Non-Lambertian surfaces"}, "15": {"item_id": "2901453769", "image_id": "15", "src": "https://miro.medium.com/max/1024/1*fWD8E3TUq1AnlfWVH061mw.png", "width": "512", "height": "374", "credit": "", "caption": "Source"}, "16": {"item_id": "2901453769", "image_id": "16", "src": "https://miro.medium.com/max/960/1*OdiFwNkAR68lpSDyKVyXmQ.gif", "width": "480", "height": "283", "credit": "", "caption": ""}, "17": {"item_id": "2901453769", "image_id": "17", "src": "https://miro.medium.com/max/826/1*AnX5Lfr0Oavs8x6Qtaj_Bg.png", "width": "413", "height": "327", "credit": "", "caption": "Since I_t+1 prediction is not occluded, pick pixels in this region and discard those from I_t-1"}, "18": {"item_id": "2901453769", "image_id": "18", "src": "https://miro.medium.com/max/2572/1*BiS1fWzTaV0MOe6-nBeXeQ.png", "width": "1286", "height": "652", "credit": "", "caption": "Subpixel convolution: Pixels are shuffled between depth and spatial dimension"}, "19": {"item_id": "2901453769", "image_id": "19", "src": "https://miro.medium.com/max/2224/1*3iXs7DIkVFYs-RNL6VdFhA.png", "width": "1112", "height": "740", "credit": "", "caption": ""}, "20": {"item_id": "2901453769", "image_id": "20", "src": "https://miro.medium.com/max/2272/1*xDXV4d6Wsa1hO4VNA9Kwbg.png", "width": "1136", "height": "1086", "credit": "", "caption": "Packing and Unpacking"}, "21": {"item_id": "2901453769", "image_id": "21", "src": "https://miro.medium.com/max/3324/1*3_R45scGc60EmyogzAzc4w.png", "width": "1662", "height": "432", "credit": "", "caption": ""}, "22": {"item_id": "2901453769", "image_id": "22", "src": "https://miro.medium.com/max/4040/1*yqTqQXc_82nIEsZseDlUMA.png", "width": "2020", "height": "650", "credit": "", "caption": ""}, "23": {"item_id": "2901453769", "image_id": "23", "src": "https://miro.medium.com/max/4800/1*mYSIic2kWGTgLDM04JMYyA.png", "width": "2400", "height": "1064", "credit": "", "caption": "GeoNet"}, "24": {"item_id": "2901453769", "image_id": "24", "src": "https://miro.medium.com/max/1992/1*tza-Gi5pZ_ta6cPQiAZjig.png", "width": "996", "height": "596", "credit": "", "caption": "SceneNet"}, "25": {"item_id": "2901453769", "image_id": "25", "src": "https://miro.medium.com/max/2512/1*8DqKqieYvQ76dIMLK0vIZQ.png", "width": "1256", "height": "472", "credit": "", "caption": "Source"}, "26": {"item_id": "2901453769", "image_id": "26", "src": "https://miro.medium.com/max/4352/1*pHdLcKnClzKmeLsabMmk8Q.png", "width": "2176", "height": "1196", "credit": "", "caption": "Source"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1202}, "3189740136": {"item_id": "3189740136", "resolved_id": "3189740171", "given_url": "https://towardsdatascience.com/semantic-hand-segmentation-using-pytorch-3e7a0a0386fa?source=rss----7f60cf5620c9---4", "given_title": "Semantic hand segmentation using Pytorch", "favorite": "0", "status": "1", "time_added": "1606909319", "time_updated": "1638708525", "time_read": "1608254890", "time_favorited": "0", "sort_id": 64, "resolved_title": "Semantic hand segmentation using Pytorch", "resolved_url": "https://towardsdatascience.com/semantic-hand-segmentation-using-pytorch-3e7a0a0386fa", "excerpt": "Semantic segmentation is the task of predicting the class of each pixel in an image. This problem is more difficult than object detection, where you have to predict a box around the object.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1179", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/freeze/max/600/1*J_ZxnuUMcCQt_fxoG4ffOQ.gif", "tags": {"deep-learning": {"item_id": "3189740136", "tag": "deep-learning"}, "pytorch": {"item_id": "3189740136", "tag": "pytorch"}, "vision": {"item_id": "3189740136", "tag": "vision"}}, "authors": {"143511438": {"item_id": "3189740136", "author_id": "143511438", "name": "Saurabh Kumar", "url": "https://saurabhk660.medium.com"}}, "image": {"item_id": "3189740136", "src": "https://miro.medium.com/fit/c/56/56/1*Kv-dyWlyTEb_KEHs8a8zlA.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3189740136", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*Kv-dyWlyTEb_KEHs8a8zlA.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3189740136", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*aNkDoJu9EkH_15f1v3Pdcg.png", "width": "700", "height": "303", "credit": "", "caption": "A sample of semantic hand segmentation. (images from HOF dataset[1])"}, "3": {"item_id": "3189740136", "image_id": "3", "src": "https://miro.medium.com/max/1400/1*CPtuThn8l44a6jj0wcXeog.png", "width": "700", "height": "219", "credit": "", "caption": "Outputs : 1. Hands prediction mask 2. No-hands prediction mask 3. Mask generated after comparing"}, "4": {"item_id": "3189740136", "image_id": "4", "src": "https://miro.medium.com/max/1200/1*J_ZxnuUMcCQt_fxoG4ffOQ.gif", "width": "600", "height": "532", "credit": "", "caption": "Sample hand segmentation"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 456}, "2945247373": {"item_id": "2945247373", "resolved_id": "2945247373", "given_url": "https://arstechnica.com/features/2020/04/some-shirts-hide-you-from-cameras-but-will-anyone-wear-them/", "given_title": "Some shirts hide you from cameras—but will anyone wear them?", "favorite": "0", "status": "1", "time_added": "1586711575", "time_updated": "1638708525", "time_read": "1587120358", "time_favorited": "0", "sort_id": 65, "resolved_title": "Some shirts hide you from cameras—but will anyone wear them?", "resolved_url": "https://arstechnica.com/features/2020/04/some-shirts-hide-you-from-cameras-but-will-anyone-wear-them/", "excerpt": "Right now, you're more than likely spending the vast majority of your time at home. Someday, however, we will all be able to leave the house once again and emerge, blinking, into society to work, travel, eat, play, and congregate in all of humanity's many bustling crowds.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3511", "lang": "en", "time_to_read": 16, "amp_url": "https://arstechnica.com/features/2020/04/some-shirts-hide-you-from-cameras-but-will-anyone-wear-them/amp/", "top_image_url": "https://cdn.arstechnica.net/wp-content/uploads/2020/03/adversarial-fashion-shoot-640x360.jpg", "tags": {"deep-learning": {"item_id": "2945247373", "tag": "deep-learning"}, "public-policy": {"item_id": "2945247373", "tag": "public-policy"}, "vision": {"item_id": "2945247373", "tag": "vision"}}, "authors": {"114573933": {"item_id": "2945247373", "author_id": "114573933", "name": "Kate Cox", "url": "https://arstechnica.com/author/katecox/"}}, "image": {"item_id": "2945247373", "src": "https://cdn.arstechnica.net/wp-content/uploads/2020/03/adversarial-fashion-shoot-800x450.jpg", "width": "640", "height": "360"}, "images": {"1": {"item_id": "2945247373", "image_id": "1", "src": "https://cdn.arstechnica.net/wp-content/uploads/2020/03/adversarial-fashion-shoot-800x450.jpg", "width": "640", "height": "360", "credit": "Aurich Lawson / Getty", "caption": "Aurich Lawson / Getty"}, "2": {"item_id": "2945247373", "image_id": "2", "src": "https://cdn.arstechnica.net/wp-content/uploads/2020/03/goldstein_shirt.png", "width": "640", "height": "285", "credit": "Tom Goldstein | University of Maryland", "caption": "Enlarge / The bright adversarial pattern, which a human viewer can darn-near see from space, renders the wearer invisible to the software looking at him."}, "3": {"item_id": "2945247373", "image_id": "3", "src": "https://cdn.arstechnica.net/wp-content/uploads/2020/03/bertash_fashion.png", "width": "640", "height": "394", "credit": "", "caption": "Enlarge / A slide from Bertash's 2019 Defcon presentation showing how the ALPR-foiling fabric works."}}, "domain_metadata": {"name": "Ars Technica", "logo": "https://logo.clearbit.com/arstechnica.com?size=800", "greyscale_logo": "https://logo.clearbit.com/arstechnica.com?size=800&greyscale=true"}, "listen_duration_estimate": 1359}, "2983257059": {"item_id": "2983257059", "resolved_id": "2983257059", "given_url": "https://www.theverge.com/2020/5/14/21258403/sony-image-sensor-integrated-ai-chip-imx500-specs-price", "given_title": "Sony’s first AI image sensor will make cameras everywhere smarter", "favorite": "0", "status": "1", "time_added": "1589455817", "time_updated": "1613039250", "time_read": "1589456195", "time_favorited": "0", "sort_id": 66, "resolved_title": "Sony’s first AI image sensor will make cameras everywhere smarter", "resolved_url": "https://www.theverge.com/2020/5/14/21258403/sony-image-sensor-integrated-ai-chip-imx500-specs-price", "excerpt": "Sony has announced the world’s first image sensor with integrated AI smarts. The new IMX500 sensor incorporates both processing power and memory, allowing it to perform machine learning-powered computer vision tasks without extra hardware.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "934", "lang": "en", "time_to_read": 4, "amp_url": "https://www.theverge.com/platform/amp/2020/5/14/21258403/sony-image-sensor-integrated-ai-chip-imx500-specs-price", "top_image_url": "https://cdn.vox-cdn.com/thumbor/nKA3S7hGOdMLK823KV2QPlUgZYM=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/12162773/akrales_180816_2793_0073.jpg", "tags": {"semiconductors": {"item_id": "2983257059", "tag": "semiconductors"}, "vision": {"item_id": "2983257059", "tag": "vision"}}, "authors": {"97592195": {"item_id": "2983257059", "author_id": "97592195", "name": "James Vincent", "url": "https://www.theverge.com/authors/james-vincent"}}, "image": {"item_id": "2983257059", "src": "https://cdn.vox-cdn.com/uploads/chorus_image/image/66795902/akrales_180816_2793_0073.0.jpg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2983257059", "image_id": "1", "src": "https://cdn.vox-cdn.com/uploads/chorus_image/image/66795902/akrales_180816_2793_0073.0.jpg", "width": "0", "height": "0", "credit": "Amelia Holowaty Krales / The Verge", "caption": "Sony’s new image sensor could one day appear in cameras like this one."}, "2": {"item_id": "2983257059", "image_id": "2", "src": "https://cdn.vox-cdn.com/uploads/chorus_asset/file/19970186/1.jpeg", "width": "0", "height": "0", "credit": "Sony Electronics Inc", "caption": "From left to right: the IMX500 as a bare chip and IMX501 as a package product."}, "3": {"item_id": "2983257059", "image_id": "3", "src": "https://cdn.vox-cdn.com/uploads/chorus_asset/file/19970164/908824086.jpg.jpg", "width": "0", "height": "0", "credit": "Stephen Brashear/Getty Images", "caption": "Many applications of AI computer vision, like Amazon Go, require lots of expensive cameras."}, "4": {"item_id": "2983257059", "image_id": "4", "src": "https://cdn.vox-cdn.com/uploads/chorus_asset/file/19970165/977013452.jpg.jpg", "width": "0", "height": "0", "credit": "Julian Stratenschulte/picture alliance via Getty Images", "caption": "AI cameras are also useful for keeping robots designed to work alongside humans safe."}}, "domain_metadata": {"name": "The Verge", "logo": "https://logo.clearbit.com/theverge.com?size=800", "greyscale_logo": "https://logo.clearbit.com/theverge.com?size=800&greyscale=true"}, "listen_duration_estimate": 362}, "2844724045": {"item_id": "2844724045", "resolved_id": "2844724045", "given_url": "https://nanonets.com/blog/table-extraction-deep-learning/", "given_title": "Table Detection and Extraction Using Deep Learning", "favorite": "0", "status": "1", "time_added": "1579687034", "time_updated": "1638708525", "time_read": "1582142665", "time_favorited": "0", "sort_id": 67, "resolved_title": "Table OCR for Detecting & Extracting Tabular Information", "resolved_url": "https://nanonets.com/blog/table-extraction-deep-learning/", "excerpt": "The amount of data being collected is drastically increasing day-by-day with lots of applications, tools, and online platforms booming in the present technological era. To handle and access this humongous data productively, it’s necessary to develop valuable information extraction tools.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "5849", "lang": "en", "time_to_read": 27, "amp_url": "https://nanonets.com/blog/table-extraction-deep-learning/amp/", "top_image_url": "https://nanonets.com/blog/content/images/2020/01/Comp-5-1.gif", "tags": {"deep-learning": {"item_id": "2844724045", "tag": "deep-learning"}, "text": {"item_id": "2844724045", "tag": "text"}, "vision": {"item_id": "2844724045", "tag": "vision"}}, "authors": {"120397522": {"item_id": "2844724045", "author_id": "120397522", "name": "Vihar Kurama", "url": "https://nanonets.com/blog/author/vihar/"}}, "image": {"item_id": "2844724045", "src": "https://lh4.googleusercontent.com/Ru-0lxAfURZR7yiRDXkcK5poCPAwZ7h-Q2o3SUixphuu1YuVqeA-GfAH0cjsDlTyyurVZA8ak15lmKpI73mK_LGijTLd2ATV1wU7fye7tLYh1V9Nqlu7zZozNNgWbfoZMWdabXGe", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2844724045", "image_id": "1", "src": "https://lh4.googleusercontent.com/Ru-0lxAfURZR7yiRDXkcK5poCPAwZ7h-Q2o3SUixphuu1YuVqeA-GfAH0cjsDlTyyurVZA8ak15lmKpI73mK_LGijTLd2ATV1wU7fye7tLYh1V9Nqlu7zZozNNgWbfoZMWdabXGe", "width": "0", "height": "0", "credit": "", "caption": "Source: Patrick Tomasso, Unsplash"}, "2": {"item_id": "2844724045", "image_id": "2", "src": "https://lh6.googleusercontent.com/vYJJ-9MnJGCYEWv-HpumDwpa9s7HcF4xgHO-poAmW08NDoUkQLDAyPGOzu4IAF5Dsx-fNvj3ustKYg9EiTvUeZNtrBrC6Ljg9FvtxWvWw9CvAwjqaV5RLn71_zGp9mQ0xLsQZKp-", "width": "0", "height": "0", "credit": "", "caption": "The architecture of TableNet"}, "3": {"item_id": "2844724045", "image_id": "3", "src": "https://lh5.googleusercontent.com/gYzN-1HX_8fbVAQ11CzHplKgGA9iGSUZ74RNfRkbfOep-Mno5mt7XkrvOVisOQkRha5q-6e7kjFZDDRTtlN9KqTAi3WI0h0L3owQEXpdSdjNHqsBLUHv4fE3JyN7w6G6_cZHY9K7", "width": "0", "height": "0", "credit": "", "caption": "Outputs of Table Detection"}, "4": {"item_id": "2844724045", "image_id": "4", "src": "https://lh6.googleusercontent.com/egaTXs01MugcwwdNygLQLYHLpxjzcEqHkzwLJi1weWeG2Tq0jOQbxLJJCXoVRGEeIhk_o89QD0T_7H98bbX7xUH7DPMeos6hJYHMc_OgYSi6ojL4xIBeuR-RnlF1H20-exeaMJ7n", "width": "0", "height": "0", "credit": "6", "caption": "Outputs of Structure Recognition"}, "5": {"item_id": "2844724045", "image_id": "5", "src": "https://nanonets.com/blog/content/images/2019/12/image-2.png", "width": "0", "height": "0", "credit": "", "caption": "Outputs generated by Graph Neural Networks"}, "6": {"item_id": "2844724045", "image_id": "6", "src": "https://lh4.googleusercontent.com/fCBQ5-Vmz9x1cyHdP0drfAM7qwkYNkEdaZuKq5CW-tLKWFFFtjtgHEZnvVk8tCUTi_uEOAc_RNij52fp43kjpHB4LtY_T7nBoCYlb8K3dEMP32ao36WLAULmQQk8ASEjjaNkqIxv", "width": "0", "height": "0", "credit": "", "caption": "General schematic of the approach"}, "7": {"item_id": "2844724045", "image_id": "7", "src": "https://lh4.googleusercontent.com/qh5rnqcWMD8O-eUm4k9h7tiwTx0CkA_WxzwfYMXMtk26MC8ReeMPPRKu7HSXCVuVTuQj-oK8zbVtP1skeYrIjaXQTD8_9Y8o5KVYxwD5wLnPLwRBH9GRA8uX3Q7T9xgK-GgJT1JC", "width": "0", "height": "0", "credit": "", "caption": "Image of the table"}, "8": {"item_id": "2844724045", "image_id": "8", "src": "https://lh6.googleusercontent.com/JJkw69pKVcRvOFeJ6TBhFJO-2P9kMjruz_vcQgWS1NSI8dOwljN8LGc8xbEQeolLX4YZLNVuw_Ontov1yBEKk3h7vZdLbyCX01SsUQ3bNnCyD_brHCOp9XYhl4W_zKUlzXiIlita", "width": "0", "height": "0", "credit": "", "caption": "Outputs"}, "9": {"item_id": "2844724045", "image_id": "9", "src": "https://nanonets.com/blog/content/images/2019/12/Screenshot-2019-12-28-at-4.53.00-PM.png", "width": "0", "height": "0", "credit": "", "caption": "Screenshot of the Items extracted from tables using Regular Expressions"}, "10": {"item_id": "2844724045", "image_id": "10", "src": "https://nanonets.com/blog/content/images/2019/10/Screen-Shot-2019-10-02-at-15.24.13-copy.png", "width": "400", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 2264}, "2597639946": {"item_id": "2597639946", "resolved_id": "2597639946", "given_url": "https://mlwhiz.com/blog/2019/05/19/feature_extraction/", "given_title": "The Hitchhiker’s Guide to Feature Extraction", "favorite": "0", "status": "1", "time_added": "1564255838", "time_updated": "1612385105", "time_read": "1566340340", "time_favorited": "0", "sort_id": 68, "resolved_title": "The Hitchhiker’s Guide to Feature Extraction", "resolved_url": "https://mlwhiz.com/blog/2019/05/19/feature_extraction/", "excerpt": "Good Features are the backbone of any machine learning model. And good feature creation often needs domain knowledge, creativity, and lots of time.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3394", "lang": "en", "time_to_read": 15, "top_image_url": "https://mlwhiz.com/images/features/brain.png", "tags": {"machine-learning": {"item_id": "2597639946", "tag": "machine-learning"}, "vision": {"item_id": "2597639946", "tag": "vision"}}, "authors": {"8623619": {"item_id": "2597639946", "author_id": "8623619", "name": "Rahul Agarwal", "url": ""}}, "image": {"item_id": "2597639946", "src": "https://mlwhiz.com/images/features/bot.jpeg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2597639946", "image_id": "1", "src": "https://mlwhiz.com/images/features/bot.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2597639946", "image_id": "2", "src": "https://mlwhiz.com/images/features/table_structure.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2597639946", "image_id": "3", "src": "https://mlwhiz.com/images/features/bucket.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2597639946", "image_id": "4", "src": "https://mlwhiz.com/images/features/ingredients.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2597639946", "image_id": "5", "src": "https://mlwhiz.com/images/features/coffee.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2597639946", "image_id": "6", "src": "https://mlwhiz.com/images/features/manhattan.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2597639946", "image_id": "7", "src": "https://mlwhiz.com/images/features/retail.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2597639946", "image_id": "8", "src": "https://mlwhiz.com/images/features/bulb.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 1314}, "2117687217": {"item_id": "2117687217", "resolved_id": "2117687217", "given_url": "https://github.com/topics/computer-vision", "given_title": "Topic: computer-vision", "favorite": "0", "status": "1", "time_added": "1521292788", "time_updated": "1638708525", "time_read": "1526153076", "time_favorited": "0", "sort_id": 69, "resolved_title": "computer-vision", "resolved_url": "https://github.com/topics/computer-vision", "excerpt": "computer-vision Loading… opencv / opencv 23k Open Source Computer Vision Library opencv c-plus-plus computer-vision deep-learning image-processing C++ Updated Mar 17, 2018 5 issues need help  Developer-Y / cs-video-courses 11.4k List of Computer Science courses with video lectures.", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "798", "lang": "en", "time_to_read": 4, "top_image_url": "https://assets-cdn.github.com/images/modules/open_graph/github-logo.png", "tags": {"deep-learning": {"item_id": "2117687217", "tag": "deep-learning"}, "vision": {"item_id": "2117687217", "tag": "vision"}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 309}, "2965467555": {"item_id": "2965467555", "resolved_id": "2965467574", "given_url": "https://towardsdatascience.com/understanding-associative-embedding-1b22677751f3?source=rss----7f60cf5620c9---4", "given_title": "Understanding Associative Embedding", "favorite": "0", "status": "1", "time_added": "1588088520", "time_updated": "1706233547", "time_read": "1589922083", "time_favorited": "0", "sort_id": 70, "resolved_title": "Understanding Associative Embedding", "resolved_url": "https://towardsdatascience.com/understanding-associative-embedding-1b22677751f3", "excerpt": "In some tasks of computer vision and deep learning, we need to predict all the results first and then split the results to several individual results.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "667", "lang": "en", "time_to_read": 3, "top_image_url": "https://miro.medium.com/max/1200/0*BWknDjcI00-udQlD", "tags": {"algorithms-math": {"item_id": "2965467555", "tag": "algorithms-math"}, "deep-learning": {"item_id": "2965467555", "tag": "deep-learning"}, "machine-learning": {"item_id": "2965467555", "tag": "machine-learning"}, "vision": {"item_id": "2965467555", "tag": "vision"}}, "authors": {"125021155": {"item_id": "2965467555", "author_id": "125021155", "name": "Shuchen Du", "url": "https://towardsdatascience.com/@dushuchen"}}, "image": {"item_id": "2965467555", "src": "https://miro.medium.com/fit/c/96/96/1*4xJpqeNstDW1l3Iae-TaBQ@2x.jpeg", "width": "48", "height": "48"}, "images": {"1": {"item_id": "2965467555", "image_id": "1", "src": "https://miro.medium.com/fit/c/96/96/1*4xJpqeNstDW1l3Iae-TaBQ@2x.jpeg", "width": "48", "height": "48", "credit": "", "caption": ""}, "2": {"item_id": "2965467555", "image_id": "2", "src": "https://miro.medium.com/max/9792/0*BWknDjcI00-udQlD", "width": "4896", "height": "3264", "credit": "Alex Alvarez on Unsplash", "caption": ""}, "3": {"item_id": "2965467555", "image_id": "3", "src": "https://miro.medium.com/max/3998/1*vg0rGk7YzcY5iAInSZkfrA.png", "width": "1999", "height": "752", "credit": "Newell et al.", "caption": "Fig. 1: Pose estimation for multi-people"}, "4": {"item_id": "2965467555", "image_id": "4", "src": "https://miro.medium.com/max/1766/1*7zoUiX4zq8ObBdWsI2PDwA.png", "width": "883", "height": "481", "credit": "Law et al.", "caption": "Fig. 2: CornerNet for object detection"}, "5": {"item_id": "2965467555", "image_id": "5", "src": "https://miro.medium.com/max/3152/1*E8n_uSFtNGLagfk2mCtEfw.png", "width": "1576", "height": "691", "credit": "", "caption": "Equ. 1: loss function for learning tag values for associative embedding"}, "6": {"item_id": "2965467555", "image_id": "6", "src": "https://miro.medium.com/max/2652/1*Fz-IPnKX3QXUfSRgpLkJ7Q.png", "width": "1326", "height": "769", "credit": "", "caption": "Fig. 3: explanation of the second item of Equ. 1"}, "7": {"item_id": "2965467555", "image_id": "7", "src": "https://miro.medium.com/max/3670/1*c8Ielujj8xNyntzj3f2KtA.png", "width": "1835", "height": "669", "credit": "Newell et al.", "caption": "Fig. 4: associative embedding for multi-people pose estimation"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 258}, "2908238511": {"item_id": "2908238511", "resolved_id": "2908238511", "given_url": "https://armaizadenwala.com/blog/pytesseract-images-to-html/", "given_title": "Using Pytesseract to Convert Images into a HTML Site", "favorite": "0", "status": "1", "time_added": "1583633871", "time_updated": "1589510307", "time_read": "1583784804", "time_favorited": "0", "sort_id": 71, "resolved_title": "Using Pytesseract To Convert Images Into A HTML Site", "resolved_url": "https://armaizadenwala.com/blog/pytesseract-images-to-html/", "excerpt": "Using Google's Tesseract OCR library, we will scan images from a dataset and create a HTML website out of it with navigation. We will be covering an array of topics including the Pytesseract library, Google's Tesseract library, Makefiles, regex, and more.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "5562", "lang": "en", "time_to_read": 25, "top_image_url": "https://armaizadenwala.com/static/pytesseract-convert-image-to-html-site-2eb71aa3a94dd3c94672d691c8d5e7eb.png", "tags": {"ocr": {"item_id": "2908238511", "tag": "ocr"}, "python": {"item_id": "2908238511", "tag": "python"}, "vision": {"item_id": "2908238511", "tag": "vision"}}, "authors": {"125417111": {"item_id": "2908238511", "author_id": "125417111", "name": "Armaiz Adenwala", "url": ""}}, "listen_duration_estimate": 2153}, "2992941024": {"item_id": "2992941024", "resolved_id": "2992941053", "given_url": "https://towardsdatascience.com/virtual-background-in-webcam-with-body-segmentation-technique-fc8106ca3038?source=rss----7f60cf5620c9---4", "given_title": "Virtual Background in Webcam with Body Segmentation Technique", "favorite": "0", "status": "1", "time_added": "1590233895", "time_updated": "1638708525", "time_read": "1591029912", "time_favorited": "0", "sort_id": 72, "resolved_title": "Virtual Background in webcam with Body Segmentation technique", "resolved_url": "https://towardsdatascience.com/virtual-background-in-webcam-with-body-segmentation-technique-fc8106ca3038", "excerpt": "Have you ever had a moment when browsing those pretty travel selfies on social media, you talk to yourself: “I wish I could be there”? Guess what, we are going to make it come true today.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1188", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1000/1*7JZFg4utLSm5qSXxDs-hnA.jpeg", "tags": {"deep-learning": {"item_id": "2992941024", "tag": "deep-learning"}, "vision": {"item_id": "2992941024", "tag": "vision"}}, "authors": {"132256372": {"item_id": "2992941024", "author_id": "132256372", "name": "Benson Ruan", "url": "https://medium.com/@bensonruan"}}, "image": {"item_id": "2992941024", "src": "https://miro.medium.com/max/2000/1*7JZFg4utLSm5qSXxDs-hnA.jpeg", "width": "1000", "height": "450"}, "images": {"1": {"item_id": "2992941024", "image_id": "1", "src": "https://miro.medium.com/max/2000/1*7JZFg4utLSm5qSXxDs-hnA.jpeg", "width": "1000", "height": "450", "credit": "", "caption": "source: bensonruan.com"}, "2": {"item_id": "2992941024", "image_id": "2", "src": "https://miro.medium.com/max/2048/1*wI5UkUo1KziqXX2c5ocAaQ.jpeg", "width": "1024", "height": "800", "credit": "", "caption": ""}, "3": {"item_id": "2992941024", "image_id": "3", "src": "https://miro.medium.com/max/14562/1*8fn1OfDPeaoqmSIXrKVrCQ.jpeg", "width": "7281", "height": "4860", "credit": "Julia M Cameron from Pexels", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 460}, "3140651372": {"item_id": "3140651372", "resolved_id": "3140651395", "given_url": "https://towardsdatascience.com/visual-perception-how-we-perceive-graphical-information-de5f30516009?source=rss----7f60cf5620c9---4", "given_title": "Visual Perception: How we perceive graphical information", "favorite": "0", "status": "1", "time_added": "1602595401", "time_updated": "1709152706", "time_read": "1604361119", "time_favorited": "0", "sort_id": 73, "resolved_title": "Visual Perception", "resolved_url": "https://towardsdatascience.com/visual-perception-how-we-perceive-graphical-information-de5f30516009", "excerpt": "We can speak of two types of visualizations. Sensory or arbitrary. Sensory representations can be understood without training and are often understood fast and across cultures. Arbitrary visualizations are not so easy to understand and need to be learned.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "966", "lang": "en", "time_to_read": 4, "top_image_url": "https://miro.medium.com/max/1200/0*PU5UOuRq_AdUAA2X", "tags": {"neurology": {"item_id": "3140651372", "tag": "neurology"}, "vision": {"item_id": "3140651372", "tag": "vision"}}, "authors": {"134383918": {"item_id": "3140651372", "author_id": "134383918", "name": "Roman Studer", "url": "https://medium.com/@roman.studer"}}, "image": {"item_id": "3140651372", "src": "https://miro.medium.com/max/1400/0*PU5UOuRq_AdUAA2X", "width": "700", "height": "548"}, "images": {"1": {"item_id": "3140651372", "image_id": "1", "src": "https://miro.medium.com/max/1400/0*PU5UOuRq_AdUAA2X", "width": "700", "height": "548", "credit": "Steve Johnson on Unsplash", "caption": ""}, "2": {"item_id": "3140651372", "image_id": "2", "src": "https://miro.medium.com/max/400/1*DGtLMB5mT0M_BC8M6yvqVA.png", "width": "200", "height": "72", "credit": "", "caption": "Size, Image by Author"}, "3": {"item_id": "3140651372", "image_id": "3", "src": "https://miro.medium.com/max/428/1*2Ff7aZAuh2RQgXJ0ecXJfQ.png", "width": "214", "height": "81", "credit": "", "caption": "Shape, Image by Author"}, "4": {"item_id": "3140651372", "image_id": "4", "src": "https://miro.medium.com/max/432/1*9eYXgKGXPvclq-v70qxtVA.png", "width": "216", "height": "78", "credit": "", "caption": "Color Value, Image by Author"}, "5": {"item_id": "3140651372", "image_id": "5", "src": "https://miro.medium.com/max/450/1*KZMW3FZtxak4kcwGDWCdzA.png", "width": "225", "height": "82", "credit": "", "caption": "Orientation, Image by Author"}, "6": {"item_id": "3140651372", "image_id": "6", "src": "https://miro.medium.com/max/448/1*y3Z6M5kLQiNGyLRgwsx_RQ.png", "width": "224", "height": "76", "credit": "", "caption": "Texture, Image by Author"}, "7": {"item_id": "3140651372", "image_id": "7", "src": "https://miro.medium.com/max/434/1*f_Pe9c18m5Kzz_ddRAx9oQ.png", "width": "217", "height": "73", "credit": "", "caption": "Color Hue, Image by Author"}, "8": {"item_id": "3140651372", "image_id": "8", "src": "https://miro.medium.com/max/432/1*lrbLgVSflv5SDGd7ktWUdQ.png", "width": "216", "height": "158", "credit": "top", "caption": "Color for continuous"}, "9": {"item_id": "3140651372", "image_id": "9", "src": "https://miro.medium.com/max/624/1*wQn8CjXjCpn_Vmj3iUkxWA.png", "width": "312", "height": "218", "credit": "", "caption": "Similarity, Image by Author"}, "10": {"item_id": "3140651372", "image_id": "10", "src": "https://miro.medium.com/max/612/1*Iq50VPyxu3n4qFNbpyUfvg.png", "width": "306", "height": "229", "credit": "", "caption": "Law of Proximity, Image by Author"}, "11": {"item_id": "3140651372", "image_id": "11", "src": "https://miro.medium.com/max/700/1*SijBVLtlMTPTkksgXPH3pQ.png", "width": "350", "height": "228", "credit": "", "caption": "Continuity, Image by Author"}, "12": {"item_id": "3140651372", "image_id": "12", "src": "https://miro.medium.com/max/956/1*WHt0ArGqYU9OGUk4lZrBpg.png", "width": "478", "height": "369", "credit": "", "caption": "Law of Closure, Image by Author"}, "13": {"item_id": "3140651372", "image_id": "13", "src": "https://miro.medium.com/max/1112/1*4RymOT51VFn5LjnsLBeUjA.png", "width": "556", "height": "341", "credit": "", "caption": "Stacked percentage bar chart"}, "14": {"item_id": "3140651372", "image_id": "14", "src": "https://miro.medium.com/max/1110/1*SmO3hzPvHPJYWMAWjU-MvA.png", "width": "555", "height": "342", "credit": "", "caption": "Scatterplot"}, "15": {"item_id": "3140651372", "image_id": "15", "src": "https://miro.medium.com/max/1112/1*dJHxOvawe5omfqao_bTCpA.png", "width": "556", "height": "341", "credit": "", "caption": "Lollipop chart"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 374}, "3114676630": {"item_id": "3114676630", "resolved_id": "3114676645", "given_url": "https://towardsdatascience.com/what-is-perspective-warping-opencv-and-python-750e7a13d386?source=rss----7f60cf5620c9---4", "given_title": "What is Perspective Warping ? | OpenCV and Python", "favorite": "0", "status": "1", "time_added": "1600370788", "time_updated": "1612385105", "time_read": "1604363978", "time_favorited": "0", "sort_id": 74, "resolved_title": "What is Perspective Warping ? | OpenCV and Python", "resolved_url": "https://towardsdatascience.com/what-is-perspective-warping-opencv-and-python-750e7a13d386", "excerpt": "Computer Vision is all abuzz now. People everywhere are working on some form of Deep Learning based computer vision projects.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1184", "lang": "en", "time_to_read": 5, "top_image_url": "https://miro.medium.com/max/1050/1*mAz3hmjnG03rnD9MacYGng.jpeg", "tags": {"machine-learning": {"item_id": "3114676630", "tag": "machine-learning"}, "python": {"item_id": "3114676630", "tag": "python"}, "vision": {"item_id": "3114676630", "tag": "vision"}}, "authors": {"135854039": {"item_id": "3114676630", "author_id": "135854039", "name": "G SowmiyaNarayanan", "url": "https://towardsdatascience.com/@gsnWrites"}}, "image": {"item_id": "3114676630", "src": "https://miro.medium.com/max/2100/1*mAz3hmjnG03rnD9MacYGng.jpeg", "width": "1050", "height": "700"}, "images": {"1": {"item_id": "3114676630", "image_id": "1", "src": "https://miro.medium.com/max/2100/1*mAz3hmjnG03rnD9MacYGng.jpeg", "width": "1050", "height": "700", "credit": "", "caption": ""}, "2": {"item_id": "3114676630", "image_id": "2", "src": "https://miro.medium.com/max/5062/1*AE_faab3HWeif7aPgbeq6g.jpeg", "width": "2531", "height": "2531", "credit": "", "caption": ""}, "3": {"item_id": "3114676630", "image_id": "3", "src": "https://miro.medium.com/max/2100/1*VBC37s6C0e4XFgy4tvvv_w.jpeg", "width": "1050", "height": "700", "credit": "Source", "caption": "Base Image"}, "4": {"item_id": "3114676630", "image_id": "4", "src": "https://miro.medium.com/max/2100/1*NbJGW6mAyYllQ_LafavKZw.gif", "width": "1050", "height": "700", "credit": "GIF by Author", "caption": "Selecting Corner Points"}, "5": {"item_id": "3114676630", "image_id": "5", "src": "https://miro.medium.com/max/2100/1*DkuLxHnJ7ICFcQCKkNZ80w.png", "width": "1050", "height": "700", "credit": "Image by Author", "caption": "Warped Image"}, "6": {"item_id": "3114676630", "image_id": "6", "src": "https://miro.medium.com/max/1252/1*SnD4LJn16QdfLGPh3IiRIQ.png", "width": "626", "height": "417", "credit": "Image by Author", "caption": "Initial Mask"}, "7": {"item_id": "3114676630", "image_id": "7", "src": "https://miro.medium.com/max/2100/1*pSt04jsVgfJSwJogwvULhA.png", "width": "1050", "height": "700", "credit": "Image by Author", "caption": "Filled-In Mask"}, "8": {"item_id": "3114676630", "image_id": "8", "src": "https://miro.medium.com/max/2100/1*JJV-0wj8IWQ94Rnog7Q7Ow.png", "width": "1050", "height": "700", "credit": "Image by Author", "caption": "Inverted Mask"}, "9": {"item_id": "3114676630", "image_id": "9", "src": "https://miro.medium.com/max/2100/1*R2MFGMqsE5lrvLgklHBipA.png", "width": "1050", "height": "700", "credit": "Image by Author", "caption": "Masked Base Image"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 458}, "3088642854": {"item_id": "3088642854", "resolved_id": "3088642869", "given_url": "https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109?source=rss----7f60cf5620c9---4", "given_title": "YOLO v4 or YOLO v5 or PP-YOLO? Which should I use?", "favorite": "0", "status": "1", "time_added": "1598165247", "time_updated": "1638708525", "time_read": "1607569983", "time_favorited": "0", "sort_id": 75, "resolved_title": "YOLO v4 or YOLO v5 or PP-YOLO?", "resolved_url": "https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109", "excerpt": "Object detection is a computer vision task that involves predicting the presence of one or more objects, along with their classes and bounding boxes. YOLO (You Only Look Once) is a state of art Object Detector which can perform object detection in real-time with a good accuracy.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1552", "lang": "en", "time_to_read": 7, "top_image_url": "https://miro.medium.com/max/1200/1*9jTB1MbpJfvX_OYEvMefuw.jpeg", "tags": {"deep-learning": {"item_id": "3088642854", "tag": "deep-learning"}, "vision": {"item_id": "3088642854", "tag": "vision"}}, "authors": {"137574036": {"item_id": "3088642854", "author_id": "137574036", "name": "Chamidu Supeshala", "url": "https://medium.com/@chamidusupeshala"}}, "image": {"item_id": "3088642854", "src": "https://miro.medium.com/fit/c/56/56/1*nFnfRL7ma-uYF0T8wI-vgA.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3088642854", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*nFnfRL7ma-uYF0T8wI-vgA.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3088642854", "image_id": "2", "src": "https://miro.medium.com/max/3840/1*9jTB1MbpJfvX_OYEvMefuw.jpeg", "width": "1920", "height": "1280", "credit": "Image by author", "caption": "YOLO object detection"}, "3": {"item_id": "3088642854", "image_id": "3", "src": "https://miro.medium.com/max/970/1*AtsoKR91L6LlA4eseQmDNA.jpeg", "width": "485", "height": "465", "credit": "source: pjreddie.com", "caption": "Performance of YOLO on VOC 2007 and COCO datasets"}, "4": {"item_id": "3088642854", "image_id": "4", "src": "https://miro.medium.com/max/1076/1*EVPqmfh38YT5KDGXT950tA.png", "width": "538", "height": "422", "credit": "source: YOLO v4 paper", "caption": "The speed and accuracy of YOLO v4"}, "5": {"item_id": "3088642854", "image_id": "5", "src": "https://miro.medium.com/max/2128/1*AAGWRhZkyLE6Hod9aP8Pqg.png", "width": "1064", "height": "727", "credit": "source: PP-YOLO repo", "caption": "The speed and accuracy of PP-YOLO"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 601}, "3125359544": {"item_id": "3125359544", "resolved_id": "3125359579", "given_url": "https://towardsdatascience.com/yolo-v5-object-detection-tutorial-2e607b9013ef?source=rss----7f60cf5620c9---4", "given_title": "Yolo v5 Object Detection Tutorial", "favorite": "0", "status": "1", "time_added": "1601338609", "time_updated": "1638708525", "time_read": "1604368255", "time_favorited": "0", "sort_id": 76, "resolved_title": "Yolo v5 Object Detection Tutorial", "resolved_url": "https://towardsdatascience.com/yolo-v5-object-detection-tutorial-2e607b9013ef", "excerpt": "Object Detection is a task in Artificial Intelligence that focuses on detecting objects in images. Yolo V5 is one of the best available models for Object Detection at the moment. The great thing about this Deep Neural Network is that it is very easy to retrain the network on your own custom dataset.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "979", "lang": "en", "time_to_read": 4, "top_image_url": "https://miro.medium.com/max/876/1*idFzREuCIvlnizQWkwC-2g.jpeg", "tags": {"deep-learning": {"item_id": "3125359544", "tag": "deep-learning"}, "object-detection": {"item_id": "3125359544", "tag": "object-detection"}, "vision": {"item_id": "3125359544", "tag": "vision"}}, "authors": {"119019604": {"item_id": "3125359544", "author_id": "119019604", "name": "Joos Korstanje", "url": "https://medium.com/@jooskorstanje"}}, "image": {"item_id": "3125359544", "src": "https://miro.medium.com/fit/c/56/56/2*X1WxhA1JpeQX6MmmPvTMuQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3125359544", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*X1WxhA1JpeQX6MmmPvTMuQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3125359544", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*idFzREuCIvlnizQWkwC-2g.jpeg", "width": "700", "height": "561", "credit": "", "caption": "Yolo v5 Object Detection Tutorial. Photo by Stefan Cosma on Unsplash"}, "3": {"item_id": "3125359544", "image_id": "3", "src": "https://miro.medium.com/max/726/1*tDd09Mjjho35GQk1ycuN-g.png", "width": "363", "height": "437", "credit": "", "caption": "An example of object detection using the pre-trained Yolo V5 model. Source of original. Creative Commons Attribution-Share Alike 4.0 International."}, "4": {"item_id": "3125359544", "image_id": "4", "src": "https://miro.medium.com/max/472/1*XupA8TGTSGdZdjsrs16hkw.png", "width": "236", "height": "183", "credit": "", "caption": "The directory tree for training a Yolo V5 model"}, "5": {"item_id": "3125359544", "image_id": "5", "src": "https://miro.medium.com/max/1008/1*nVcy0L2Xxm-BuedAGcueKw.png", "width": "504", "height": "458", "credit": "", "caption": "Extract of the labels of one training image called “00333207”."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 379}, "955536225": {"item_id": "955536225", "resolved_id": "955536225", "given_url": "https://pjreddie.com/darknet/yolo/", "given_title": "YOLO: Real-Time Object Detection", "favorite": "0", "status": "1", "time_added": "1554245748", "time_updated": "1638708525", "time_read": "1567116388", "time_favorited": "0", "sort_id": 77, "resolved_title": "YOLO: Real-Time Object Detection", "resolved_url": "https://pjreddie.com/darknet/yolo/", "excerpt": "You only look once (YOLO) is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. YOLOv3 is extremely fast and accurate. In mAP measured at .5 IOU YOLOv3 is on par with Focal Loss but about 4x faster.", "is_article": "1", "is_index": "1", "has_video": "1", "has_image": "1", "word_count": "1833", "lang": "en", "time_to_read": 8, "tags": {"deep-learning": {"item_id": "955536225", "tag": "deep-learning"}, "vision": {"item_id": "955536225", "tag": "vision"}}, "authors": {"57511003": {"item_id": "955536225", "author_id": "57511003", "name": "Joseph Redmon", "url": ""}}, "image": {"item_id": "955536225", "src": "https://pjreddie.com/media/image/map50blue.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "955536225", "image_id": "1", "src": "https://pjreddie.com/media/image/map50blue.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "955536225", "image_id": "2", "src": "https://pjreddie.com/media/image/sayit.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "955536225", "image_id": "3", "src": "https://pjreddie.com/media/image/Screen_Shot_2018-03-24_at_10.48.42_PM.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "955536225", "image_id": "4", "src": "https://pjreddie.com/media/image/Screen_Shot_2018-03-24_at_10.53.04_PM.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "955536225", "video_id": "1", "src": "https://www.youtube.com/embed/MPU2HistivI", "width": "100", "height": "415", "type": "1", "vid": "MPU2HistivI", "length": "0"}}, "listen_duration_estimate": 710}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709934983}