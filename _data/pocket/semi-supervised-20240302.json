{"status": 1, "complete": 1, "list": {"3497845378": {"item_id": "3497845378", "resolved_id": "3497845382", "given_url": "https://lilianweng.github.io/lil-log/2021/12/05/semi-supervised-learning.html", "given_title": "Learning with not Enough Data Part 1: Semi-Supervised Learning", "favorite": "0", "status": "1", "time_added": "1638893191", "time_updated": "1638993127", "time_read": "1638993126", "time_favorited": "0", "sort_id": 0, "resolved_title": "Learning with not Enough Data Part 1: Semi-Supervised Learning", "resolved_url": "https://lilianweng.github.io/2021/12/05/semi-supervised-learning.html", "excerpt": "The performance of supervised learning tasks improves with more high-quality labels available. However, it is expensive to collect a large number of labeled samples. There are several paradigms in machine learning to deal with the scenario when the labels are scarce.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "5424", "lang": "en", "time_to_read": 25, "tags": {"machine-learning": {"item_id": "3497845378", "tag": "machine-learning"}, "semi-supervised": {"item_id": "3497845378", "tag": "semi-supervised"}}, "authors": {"76470090": {"item_id": "3497845378", "author_id": "76470090", "name": "Lilian Weng", "url": ""}}, "image": {"item_id": "3497845378", "src": "https://lilianweng.github.io/lil-log/assets/images/PI-model.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3497845378", "image_id": "1", "src": "https://lilianweng.github.io/lil-log/assets/images/PI-model.png", "width": "0", "height": "0", "credit": "", "caption": "Fig. 1. Overview of the Π-model. Two versions of the same input with different stochastic augmentation and dropout masks pass through the network and the outputs are expected to be consistent. (Image source: Laine & Aila (2017))"}, "2": {"item_id": "3497845378", "image_id": "2", "src": "https://lilianweng.github.io/lil-log/assets/images/temperal-ensembling.png", "width": "0", "height": "0", "credit": "", "caption": "Fig. 2. Overview of Temporal Ensembling. The per-sample EMA label prediction is the learning target. (Image source: Laine & Aila (2017))"}, "3": {"item_id": "3497845378", "image_id": "3", "src": "https://lilianweng.github.io/lil-log/assets/images/mean-teacher.png", "width": "0", "height": "0", "credit": "Image source: Tarvaninen & Valpola, 2017", "caption": "Fig. 3. Overview of the Mean Teacher framework."}, "4": {"item_id": "3497845378", "image_id": "4", "src": "https://lilianweng.github.io/lil-log/assets/images/mean-teacher-results.png", "width": "0", "height": "0", "credit": "in orange", "caption": "Fig. 4. Classification error on SVHN of Mean Teacher and the Π Model. The mean teacher"}, "5": {"item_id": "3497845378", "image_id": "5", "src": "https://lilianweng.github.io/lil-log/assets/images/consistency-training-with-noisy-samples.png", "width": "0", "height": "0", "credit": "", "caption": "Fig. 5. Consistency training with noisy samples."}, "6": {"item_id": "3497845378", "image_id": "6", "src": "https://lilianweng.github.io/lil-log/assets/images/ICT.png", "width": "0", "height": "0", "credit": "Image source: Verma et al. 2019", "caption": "Fig. 6. Overview of Interpolation Consistency Training. MixUp is applied to produce more interpolated samples with interpolated labels as learning targets."}, "7": {"item_id": "3497845378", "image_id": "7", "src": "https://lilianweng.github.io/lil-log/assets/images/UDA-image-results.png", "width": "0", "height": "0", "credit": "Image source: Xie et al. 2020", "caption": "Fig. 7. Comparison of various semi-supervised learning methods on CIFAR-10 classification. Fully supervised Wide-ResNet-28-2 and PyramidNet+ShakeDrop have an error rate of 5.4 and 2.7 respectively when trained on 50,000 examples without RandAugment."}, "8": {"item_id": "3497845378", "image_id": "8", "src": "https://lilianweng.github.io/lil-log/assets/images/UDA-language-results.png", "width": "0", "height": "0", "credit": "Image source: Xie et al. 2020", "caption": "Fig. 8. Comparison of UDA with different initialization configurations on various text classification tasks."}, "9": {"item_id": "3497845378", "image_id": "9", "src": "https://lilianweng.github.io/lil-log/assets/images/pseudo-label-segregation.png", "width": "0", "height": "0", "credit": "a", "caption": "Fig. 9. t-SNE visualization of outputs on MNIST test set by models training"}, "10": {"item_id": "3497845378", "image_id": "10", "src": "https://lilianweng.github.io/lil-log/assets/images/label-propagation.png", "width": "0", "height": "0", "credit": "Image source: Iscen et al. 2019", "caption": "Fig. 10. Illustration of how Label Propagation works."}, "11": {"item_id": "3497845378", "image_id": "11", "src": "https://lilianweng.github.io/lil-log/assets/images/MPL-results.png", "width": "0", "height": "0", "credit": "Image source: Pham et al. 2021", "caption": "Fig. 11. Comparison of Meta Pseudo Labels with other semi- or self-supervised learning methods on image classification tasks."}, "12": {"item_id": "3497845378", "image_id": "12", "src": "https://lilianweng.github.io/lil-log/assets/images/MixMatch.png", "width": "0", "height": "0", "credit": "K\\", "caption": "Fig. 12. The process of “label guessing” in MixMatch: averaging \\"}, "13": {"item_id": "3497845378", "image_id": "13", "src": "https://lilianweng.github.io/lil-log/assets/images/ReMixMatch.png", "width": "0", "height": "0", "credit": "Image source: Berthelot et al. 2020", "caption": "Fig. 13. Illustration of two improvements introduced in ReMixMatch over MixMatch."}, "14": {"item_id": "3497845378", "image_id": "14", "src": "https://lilianweng.github.io/lil-log/assets/images/DivideMix.png", "width": "0", "height": "0", "credit": "Image source: Junnan Li et al. 2020", "caption": "Fig. 14. DivideMix trains two networks independently to reduce confirmation bias. They run co-divide, co-refinement, and co-guessing together."}, "15": {"item_id": "3497845378", "image_id": "15", "src": "https://lilianweng.github.io/lil-log/assets/images/DivideMix-algo.png", "width": "0", "height": "0", "credit": "Image source: Junnan Li et al. 2020", "caption": "Fig. 15. The algorithm of DivideMix."}, "16": {"item_id": "3497845378", "image_id": "16", "src": "https://lilianweng.github.io/lil-log/assets/images/FixMatch.png", "width": "0", "height": "0", "credit": "Image source: Sohn et al. 2020", "caption": "Fig. 16. Illustration of how FixMatch works."}, "17": {"item_id": "3497845378", "image_id": "17", "src": "https://lilianweng.github.io/lil-log/assets/images/FixMatch-results.png", "width": "0", "height": "0", "credit": "Image source: Sohn et al. 2020", "caption": "Fig. 17. Performance of FixMatch and several other semi-supervised learning methods on image classification tasks."}, "18": {"item_id": "3497845378", "image_id": "18", "src": "https://lilianweng.github.io/lil-log/assets/images/self-training-pre-training.png", "width": "0", "height": "0", "credit": "a", "caption": "Fig. 18. The effect of"}, "19": {"item_id": "3497845378", "image_id": "19", "src": "https://lilianweng.github.io/lil-log/assets/images/big-self-supervised-model.png", "width": "0", "height": "0", "credit": "Left", "caption": "Fig. 19. A semi-supervised learning framework leverages unlabeled data corpus by"}, "20": {"item_id": "3497845378", "image_id": "20", "src": "https://lilianweng.github.io/lil-log/assets/images/big-self-supervised-model-results.png", "width": "0", "height": "0", "credit": "Image source: Chen et al. 2020", "caption": "Fig. 20. Comparison of performance by SimCLRv2 + semi-supervised distillation on ImageNet classification."}}, "listen_duration_estimate": 2100}, "2638987074": {"item_id": "2638987074", "resolved_id": "2638987074", "given_url": "https://semiengineering.com/will-open-source-eda-work/", "given_title": "Will Open-Source EDA Work?", "favorite": "0", "status": "1", "time_added": "1561549120", "time_updated": "1608576292", "time_read": "1561580547", "time_favorited": "0", "sort_id": 1, "resolved_title": "Will Open-Source EDA Work?", "resolved_url": "https://semiengineering.com/will-open-source-eda-work/", "excerpt": "Open-source EDA is back on the semiconductor industry’s agenda, spurred by growing interest in open-source hardware. But whether the industry embraces the idea with enough enthusiasm to make it successful is not clear yet. One of the key sponsors of this effort is the U.S.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1490", "lang": "en", "time_to_read": 7, "top_image_url": "https://i2.wp.com/semiengineering.com/wp-content/uploads/2019/06/Darpa2.png?fit=525%2C300&ssl=1", "tags": {"semi-supervised": {"item_id": "2638987074", "tag": "semi-supervised"}}, "authors": {"78654073": {"item_id": "2638987074", "author_id": "78654073", "name": "Kevin Fogarty", "url": "https://semiengineering.com/author/kevin-fogarty/"}}, "image": {"item_id": "2638987074", "src": "https://i0.wp.com/semiengineering.com/wp-content/uploads/2019/06/Darpachart.png?ssl=1", "width": "2050", "height": "1052"}, "images": {"1": {"item_id": "2638987074", "image_id": "1", "src": "https://i0.wp.com/semiengineering.com/wp-content/uploads/2019/06/Darpachart.png?ssl=1", "width": "2050", "height": "1052", "credit": "", "caption": ""}}, "listen_duration_estimate": 577}, "2671515847": {"item_id": "2671515847", "resolved_id": "2671515847", "given_url": "https://spectrum.ieee.org/nanoclast/semiconductors/devices/buried-power-lines-make-memory-faster", "given_title": "Buried Power Lines Make Memory Faster - IEEE Spectrum", "favorite": "1", "status": "1", "time_added": "1564157091", "time_updated": "1608345900", "time_read": "1564162314", "time_favorited": "1564162313", "sort_id": 2, "resolved_title": "Buried Power Lines Make Memory Faster", "resolved_url": "https://spectrum.ieee.org/nanoclast/semiconductors/devices/buried-power-lines-make-memory-faster", "excerpt": "When chipmakers announce that they’ve managed to pack yet more circuits onto a chip, it’s usually the smaller transistor that gets all the attention. But the interconnects that link transistors to form circuits also have to shrink.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "759", "lang": "en", "time_to_read": 3, "amp_url": "https://spectrum.ieee.org/nanoclast/semiconductors/devices/buried-power-lines-make-memory-faster.amp.html", "top_image_url": "https://spectrum.ieee.org/image/MzM0NTA1Nw.jpeg", "tags": {"semi-supervised": {"item_id": "2671515847", "tag": "semi-supervised"}}, "authors": {"75934394": {"item_id": "2671515847", "author_id": "75934394", "name": "Samuel K. Moore", "url": "https://spectrum.ieee.org/author/moore-samuel-k"}}, "image": {"item_id": "2671515847", "src": "https://spectrum.ieee.org/image/MzM0NTAyNw.jpeg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2671515847", "image_id": "1", "src": "https://spectrum.ieee.org/image/MzM0NTAyNw.jpeg", "width": "0", "height": "0", "credit": "", "caption": "Illustration: iStock"}}, "domain_metadata": {"name": "IEEE", "logo": "https://logo.clearbit.com/ieee.org?size=800", "greyscale_logo": "https://logo.clearbit.com/ieee.org?size=800&greyscale=true"}, "listen_duration_estimate": 294}, "2907036547": {"item_id": "2907036547", "resolved_id": "2906992931", "given_url": "https://towardsdatascience.com/semi-supervised-classification-of-unlabeled-data-pu-learning-81f96e96f7cb?source=rss----7f60cf5620c9---4", "given_title": "Semi-Supervised Classification of Unlabeled Data (PU Learning)", "favorite": "0", "status": "1", "time_added": "1583525177", "time_updated": "1612385105", "time_read": "1583785007", "time_favorited": "0", "sort_id": 3, "resolved_title": "Semi-Supervised Classification of Unlabeled Data (PU Learning)", "resolved_url": "https://towardsdatascience.com/semi-supervised-classification-of-unlabeled-data-pu-learning-81f96e96f7cb", "excerpt": "Suppose you have a dataset of payment transactions. Some of the transactions are labeled as fraud and the rest are labeled as authentic, and you are required to design a model that will distinguish between fraudulent and authentic transactions.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1562", "lang": "en", "time_to_read": 7, "top_image_url": "https://miro.medium.com/max/1200/0*ZJ0dg_-FtBq9wODJ", "tags": {"machine-learning": {"item_id": "2907036547", "tag": "machine-learning"}, "semi-supervised": {"item_id": "2907036547", "tag": "semi-supervised"}}, "authors": {"119850484": {"item_id": "2907036547", "author_id": "119850484", "name": "Alon Agmon", "url": "https://medium.com/@alon.agmon"}}, "image": {"item_id": "2907036547", "src": "https://miro.medium.com/fit/c/56/56/2*w0CIoS_-BJh89HRoEzc8kg.png", "width": "28", "height": "28"}, "images": {"1": {"item_id": "2907036547", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/2*w0CIoS_-BJh89HRoEzc8kg.png", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "2907036547", "image_id": "2", "src": "https://miro.medium.com/max/11520/0*ZJ0dg_-FtBq9wODJ", "width": "5760", "height": "3840", "credit": "Bruno Martins on Unsplash", "caption": ""}, "3": {"item_id": "2907036547", "image_id": "3", "src": "https://miro.medium.com/max/10368/0*OyLEuc5HukKhMw5I", "width": "5184", "height": "3456", "credit": "Antoine Dautry on Unsplash", "caption": ""}, "4": {"item_id": "2907036547", "image_id": "4", "src": "https://miro.medium.com/max/1432/1*siPJq16IQnAKL7KgnvR8cw.png", "width": "716", "height": "1044", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 605}, "2752291034": {"item_id": "2752291034", "resolved_id": "2752117749", "given_url": "https://www.japantimes.co.jp/?post_type=news&p=2236865", "given_title": "Japanese manufacturers use decades of experience to dominate key chemical m", "favorite": "1", "status": "1", "time_added": "1570620402", "time_updated": "1608339546", "time_read": "1570627923", "time_favorited": "1570627922", "sort_id": 4, "resolved_title": "Japanese manufacturers use decades of experience to dominate key chemical market for cutting-edge chips", "resolved_url": "https://www.japantimes.co.jp/news/2019/10/09/business/japanese-manufacturers-use-decades-experience-dominate-key-chemical-market-cutting-edge-chips/", "excerpt": "KAWASAKI – Tokyo Ohka Kogyo Co., JSR Corp. and Shin-Etsu Chemical Co.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1016", "lang": "en", "time_to_read": 5, "top_image_url": "https://cdn.japantimes.2xx.jp/wp-content/uploads/2019/10/n-tok-a-20191010-870x580.jpg", "tags": {"platforms": {"item_id": "2752291034", "tag": "platforms"}, "prodmgmt": {"item_id": "2752291034", "tag": "prodmgmt"}, "semi-supervised": {"item_id": "2752291034", "tag": "semi-supervised"}}, "authors": {"75853923": {"item_id": "2752291034", "author_id": "75853923", "name": "Osamu Tsukimori", "url": "https://www.japantimes.co.jp/author/int-osamu_tsukimori/"}}, "image": {"item_id": "2752291034", "src": "https://cdn.japantimes.2xx.jp/wp-content/uploads/2019/10/n-tok-b-20191010.jpg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2752291034", "image_id": "1", "src": "https://cdn.japantimes.2xx.jp/wp-content/uploads/2019/10/n-tok-b-20191010.jpg", "width": "0", "height": "0", "credit": "", "caption": "Behind the development of cutting-edge, palm-sized smartphones lies an advanced type of liquid that responds to beams of light to coat and etch finely detailed patterns on semiconductor circuit boards using a technique called photolithography. | BLOOMBERG"}, "2": {"item_id": "2752291034", "image_id": "2", "src": "https://cdn.japantimes.2xx.jp/wp-content/uploads/2019/10/n-tok-c-20191010.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "The Japan Times", "logo": "https://logo.clearbit.com/japantimes.co.jp?size=800", "greyscale_logo": "https://logo.clearbit.com/japantimes.co.jp?size=800&greyscale=true"}, "listen_duration_estimate": 393}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709419636}