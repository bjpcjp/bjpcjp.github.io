{"status": 1, "complete": 1, "list": {"930824186": {"item_id": "930824186", "resolved_id": "930824186", "given_url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "given_title": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "favorite": "0", "status": "1", "time_added": "1622248003", "time_updated": "1638708525", "time_read": "1622249141", "time_favorited": "0", "sort_id": 0, "resolved_title": "The Unreasonable Effectiveness of Recurrent Neural Networks", "resolved_url": "https://karpathy.github.io/2015/05/21/rnn-effectiveness/", "excerpt": "There’s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "7126", "lang": "en", "time_to_read": 32, "tags": {"deep-learning": {"item_id": "930824186", "tag": "deep-learning"}, "rnns": {"item_id": "930824186", "tag": "rnns"}}, "image": {"item_id": "930824186", "src": "https://karpathy.github.io/assets/rnn/diags.jpeg", "width": "0", "height": "0"}, "images": {"1": {"item_id": "930824186", "image_id": "1", "src": "https://karpathy.github.io/assets/rnn/diags.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "930824186", "image_id": "2", "src": "https://karpathy.github.io/assets/rnn/charseq.jpeg", "width": "70", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "930824186", "image_id": "3", "src": "https://karpathy.github.io/assets/rnn/latex4.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "930824186", "image_id": "4", "src": "https://karpathy.github.io/assets/rnn/latex3.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "930824186", "image_id": "5", "src": "https://karpathy.github.io/assets/rnn/under1.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "930824186", "image_id": "6", "src": "https://karpathy.github.io/assets/rnn/under2.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "930824186", "image_id": "7", "src": "https://karpathy.github.io/assets/rnn/under3.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "930824186", "image_id": "8", "src": "https://karpathy.github.io/assets/rnn/under4.jpeg", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 2758}, "2272181338": {"item_id": "2272181338", "resolved_id": "2272181343", "given_url": "http://offconvex.github.io/2018/07/27/approximating-recurrent/", "given_title": "When Recurrent Models Don't Need to be Recurrent", "favorite": "0", "status": "1", "time_added": "1532725281", "time_updated": "1638708525", "time_read": "1535747328", "time_favorited": "0", "sort_id": 1, "resolved_title": "When Recurrent Models Don't Need to be Recurrent", "resolved_url": "http://www.offconvex.org/2018/07/27/approximating-recurrent/", "excerpt": "In the last few years, deep learning practitioners have proposed a litany of different sequence models. Although recurrent neural networks were once the tool of choice, now models like the autoregressive Wavenet or the Transformer are replacing RNNs on a diverse set of tasks.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1446", "lang": "en", "time_to_read": 7, "tags": {"deep-learning": {"item_id": "2272181338", "tag": "deep-learning"}, "rnns": {"item_id": "2272181338", "tag": "rnns"}}, "authors": {"22960972": {"item_id": "2272181338", "author_id": "22960972", "name": "Moritz Hardt", "url": ""}}, "image": {"item_id": "2272181338", "src": "http://www.offconvex.org/assets/approx_recurrent/recurrent_net.png", "width": "500", "height": "250"}, "images": {"1": {"item_id": "2272181338", "image_id": "1", "src": "http://www.offconvex.org/assets/approx_recurrent/recurrent_net.png", "width": "500", "height": "250", "credit": "", "caption": ""}, "2": {"item_id": "2272181338", "image_id": "2", "src": "http://www.offconvex.org/assets/approx_recurrent/truncated_backprop.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 560}, "3493082098": {"item_id": "3493082098", "resolved_id": "3493082098", "given_url": "https://github.com/bjpcjp/scikit-and-tensorflow-workbooks/blob/master/ch14-Recurrent-NNs.ipynb", "given_title": "", "favorite": "0", "status": "1", "time_added": "1638239964", "time_updated": "1642439727", "time_read": "1642439727", "time_favorited": "0", "sort_id": 2, "resolved_title": "", "resolved_url": "https://github.com/bjpcjp/scikit-and-tensorflow-workbooks/blob/master/ch14-Recurrent-NNs.ipynb", "excerpt": "", "is_article": "0", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "0", "lang": "", "tags": {"deep-learning": {"item_id": "3493082098", "tag": "deep-learning"}, "rnns": {"item_id": "3493082098", "tag": "rnns"}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 0}, "964205254": {"item_id": "964205254", "resolved_id": "964205254", "given_url": "https://github.com/kjw0612/awesome-rnn", "given_title": "kjw0612/awesome-rnn: Recurrent Neural Network - A curated list of resources", "favorite": "0", "status": "1", "time_added": "1527454464", "time_updated": "1638708525", "time_read": "1527461126", "time_favorited": "0", "sort_id": 3, "resolved_title": "Awesome Recurrent Neural Networks", "resolved_url": "https://github.com/kjw0612/awesome-rnn", "excerpt": "Please feel free to pull requests, email Myungsub Choi (cms6539@gmail.com) or join our chats to add links.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3623", "lang": "en", "time_to_read": 16, "top_image_url": "https://opengraph.githubassets.com/7e7a53c07055db1538e5a018a6cd910b5fca93e571e03e094aefe2394840a3e0/kjw0612/awesome-rnn", "tags": {"deep-learning": {"item_id": "964205254", "tag": "deep-learning"}, "rnns": {"item_id": "964205254", "tag": "rnns"}}, "authors": {"30972199": {"item_id": "964205254", "author_id": "30972199", "name": "Machine Learning", "url": "https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/"}}, "image": {"item_id": "964205254", "src": "https://camo.githubusercontent.com/5dbac0213da25c445bd11f168587c11a200ba153ef3014e8408e462e410169b3/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667", "width": "0", "height": "0"}, "images": {"1": {"item_id": "964205254", "image_id": "1", "src": "https://camo.githubusercontent.com/5dbac0213da25c445bd11f168587c11a200ba153ef3014e8408e462e410169b3/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 1402}, "3832449628": {"item_id": "3832449628", "resolved_id": "3832449628", "given_url": "https://johanwind.github.io/2023/03/23/rwkv_overview.html", "given_title": "Hacker News", "favorite": "1", "status": "1", "time_added": "1680186586", "time_updated": "1680276982", "time_read": "1680276982", "time_favorited": "1680276968", "sort_id": 4, "resolved_title": "The RWKV language model: An RNN with the advantages of a transformer", "resolved_url": "https://johanwind.github.io/2023/03/23/rwkv_overview.html", "excerpt": "For a while, I’ve been following and contributing to the RWKV language model, an open source large language model with great potential. As ChatGPT and large language models in general have gotten a lot of attention recently, I think it’s a good time to write about RWKV.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "867", "lang": "en", "time_to_read": 4, "tags": {"deep-learning": {"item_id": "3832449628", "tag": "deep-learning"}, "nlp": {"item_id": "3832449628", "tag": "nlp"}, "rnns": {"item_id": "3832449628", "tag": "rnns"}, "transformers": {"item_id": "3832449628", "tag": "transformers"}}, "image": {"item_id": "3832449628", "src": "https://johanwind.github.io/images/rwkv_benchmark.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3832449628", "image_id": "1", "src": "https://johanwind.github.io/images/rwkv_benchmark.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 336}, "1880817729": {"item_id": "1880817729", "resolved_id": "1880817729", "given_url": "https://machinelearningmastery.com/rnn-unrolling/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1505357650", "time_updated": "1638708525", "time_read": "1528501393", "time_favorited": "0", "sort_id": 5, "resolved_title": "A Gentle Introduction to RNN Unrolling", "resolved_url": "https://machinelearningmastery.com/rnn-unrolling/", "excerpt": "Recurrent neural networks are a type of neural network where the outputs from previous time steps are fed as input to the current time step. This creates a network graph or circuit diagram with cycles, which can make it difficult to understand how information moves through the network.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "992", "lang": "en", "time_to_read": 5, "top_image_url": "https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-Unrolled-RNN-on-the-forward-pass.png", "tags": {"deep-learning": {"item_id": "1880817729", "tag": "deep-learning"}, "rnns": {"item_id": "1880817729", "tag": "rnns"}}, "authors": {"26997241": {"item_id": "1880817729", "author_id": "26997241", "name": "Jason Brownlee", "url": "https://machinelearningmastery.com/author/jasonb/"}}, "image": {"item_id": "1880817729", "src": "https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-an-RNN-with-a-cycle.png", "width": "147", "height": "323"}, "images": {"1": {"item_id": "1880817729", "image_id": "1", "src": "https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-an-RNN-with-a-cycle.png", "width": "147", "height": "323", "credit": "", "caption": "Example of an RNN with a cycle"}, "2": {"item_id": "1880817729", "image_id": "2", "src": "https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-Unrolled-RNN-on-the-forward-pass.png", "width": "350", "height": "317", "credit": "", "caption": "Example of Unrolled RNN on the forward pass"}, "3": {"item_id": "1880817729", "image_id": "3", "src": "https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-Unrolled-RNN-with-each-copy-of-the-network-as-a-layer.png", "width": "190", "height": "235", "credit": "", "caption": "Example of Unrolled RNN with each copy of the network as a layer"}, "4": {"item_id": "1880817729", "image_id": "4", "src": "https://machinelearningmastery.com/wp-content/uploads/2017/07/Cover-220.png", "width": "220", "height": "311", "credit": "", "caption": ""}}, "listen_duration_estimate": 384}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709419606}