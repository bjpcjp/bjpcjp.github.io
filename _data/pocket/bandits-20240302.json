{"status": 1, "complete": 1, "list": {"1403357004": {"item_id": "1403357004", "resolved_id": "1403357004", "given_url": "http://banditalgs.com/", "given_title": "http://banditalgs.com/", "favorite": "1", "status": "1", "time_added": "1532881480", "time_updated": "1673901447", "time_read": "1535747002", "time_favorited": "1535747001", "sort_id": 0, "resolved_title": "Bandit Algorithms", "resolved_url": "https://banditalgs.com/", "excerpt": "The Bayesian approach to learning starts by choosing a prior probability distribution over the unknown parameters of the world. Then, as the learner makes observation, the prior is updated using Bayes rule to form the posterior, which represents the new Continue Reading", "is_article": "0", "is_index": "1", "has_video": "0", "has_image": "1", "word_count": "453", "lang": "en", "top_image_url": "https://banditalgs.com/wp-content/uploads/2019/03/cropped-bandit-1.png", "tags": {"bandits": {"item_id": "1403357004", "tag": "bandits"}}, "image": {"item_id": "1403357004", "src": "https://banditalgs.com/bitnami/images/close.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "1403357004", "image_id": "1", "src": "https://banditalgs.com/bitnami/images/close.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 175}, "2147709334": {"item_id": "2147709334", "resolved_id": "2147709334", "given_url": "https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d", "given_title": "", "favorite": "0", "status": "1", "time_added": "1638225234", "time_updated": "1673901447", "time_read": "1658156541", "time_favorited": "0", "sort_id": 1, "resolved_title": "Multi-armed bandits for dynamic movie recommendations", "resolved_url": "https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d", "excerpt": "Brian O’Gorman has a PhD in Physics from UT Austin, and was most recently a consultant at Princeton Consultants. He was an Insight Data Science Fellow in the winter of 2018 in New York.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2185", "lang": "en", "time_to_read": 10, "top_image_url": "https://miro.medium.com/max/1200/0*k3FsfPU592DBgMfx.", "tags": {"bandits": {"item_id": "2147709334", "tag": "bandits"}, "recommenders": {"item_id": "2147709334", "tag": "recommenders"}}, "authors": {"152483084": {"item_id": "2147709334", "author_id": "152483084", "name": "Brian O'Gorman", "url": "https://medium.com/@bcogorman?source=post_page-----5eb8f325ed1d--------------------------------"}}, "image": {"item_id": "2147709334", "src": "https://miro.medium.com/max/3200/0*k3FsfPU592DBgMfx.", "width": "1600", "height": "620"}, "images": {"1": {"item_id": "2147709334", "image_id": "1", "src": "https://miro.medium.com/max/3200/0*k3FsfPU592DBgMfx.", "width": "1600", "height": "620", "credit": "", "caption": "Maybe they would like V for Vendetta?"}, "2": {"item_id": "2147709334", "image_id": "2", "src": "https://miro.medium.com/max/1920/0*kk1lIElXKC_Czg0X.", "width": "960", "height": "377", "credit": "left of the dashed line", "caption": "During an A/B test"}, "3": {"item_id": "2147709334", "image_id": "3", "src": "https://miro.medium.com/max/1920/0*vhIOHWnLaaPOt2yD.", "width": "960", "height": "375", "credit": "1–4", "caption": "A bandit algorithm begins with equal percentages of viewers being presented with each of the options"}, "4": {"item_id": "2147709334", "image_id": "4", "src": "https://miro.medium.com/max/1998/0*KdEIB1nFNlpYAjvL.", "width": "999", "height": "697", "credit": "", "caption": "Ground truth probability for a movie to be “liked” — i.e., given a rating of 5 by a viewer. In this set of movies, Star Wars is the most liked movie and Liar Liar is the least liked."}, "5": {"item_id": "2147709334", "image_id": "5", "src": "https://miro.medium.com/max/1948/1*SGfDtqhGnjUd24X-kLVShA.png", "width": "974", "height": "525", "credit": "Star Wars", "caption": "The percentage of time that each movie was recommended during the first 800 recommendations. At first, all movies are being recommended, but over time, the best performing option"}, "6": {"item_id": "2147709334", "image_id": "6", "src": "https://miro.medium.com/max/1728/1*VeanZZzLV_dAC3lEhNU_pg.png", "width": "864", "height": "720", "credit": "black", "caption": "Observed regret as a function of number of recommendations made for three different algorithms: Thompson Sampling"}}, "listen_duration_estimate": 846}, "1361367366": {"item_id": "1361367366", "resolved_id": "1361367366", "given_url": "https://blog.thedataincubator.com/2016/07/multi-armed-bandits-2/", "given_title": "", "favorite": "0", "status": "1", "time_added": "1638225234", "time_updated": "1706233547", "time_read": "1658190540", "time_favorited": "0", "sort_id": 2, "resolved_title": "Multi-Armed Bandits", "resolved_url": "https://blog.thedataincubator.com/2016/07/multi-armed-bandits-2/", "excerpt": "Special thanks to Brian Farris for contributing this post. Anyone who is involved in testing.", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "2084", "lang": "en", "time_to_read": 9, "top_image_url": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/MAB.jpg", "tags": {"algorithms-math": {"item_id": "1361367366", "tag": "algorithms-math"}, "analytics": {"item_id": "1361367366", "tag": "analytics"}, "bandits": {"item_id": "1361367366", "tag": "bandits"}}, "authors": {"93495402": {"item_id": "1361367366", "author_id": "93495402", "name": "Michael Li", "url": "http://www.thedataincubator.com/team.html#michael%20li"}}, "image": {"item_id": "1361367366", "src": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/MAB-300x209.jpg", "width": "377", "height": "263"}, "images": {"1": {"item_id": "1361367366", "image_id": "1", "src": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/MAB-300x209.jpg", "width": "377", "height": "263", "credit": "", "caption": ""}, "2": {"item_id": "1361367366", "image_id": "2", "src": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/epsilongreedy-300x182.png", "width": "436", "height": "265", "credit": "", "caption": ""}, "3": {"item_id": "1361367366", "image_id": "3", "src": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/bandit_eps_greedy-300x225.png", "width": "400", "height": "300", "credit": "", "caption": ""}, "4": {"item_id": "1361367366", "image_id": "4", "src": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/bandit_dynamic-300x225.png", "width": "340", "height": "255", "credit": "", "caption": ""}, "5": {"item_id": "1361367366", "image_id": "5", "src": "https://blog.thedataincubator.com/wp-content/uploads/2016/07/arm_values-300x200.png", "width": "341", "height": "227", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "1361367366", "video_id": "1", "src": "https://www.youtube.com/embed/p4NGSH819ZU?feature=oembed", "width": "500", "height": "375", "type": "1", "vid": "p4NGSH819ZU", "length": "0"}}, "listen_duration_estimate": 807}, "3268897680": {"item_id": "3268897680", "resolved_id": "3268897680", "given_url": "https://github.com/stitchfix/mab", "given_title": "", "favorite": "0", "status": "1", "time_added": "1616233004", "time_updated": "1673901447", "time_read": "1616237626", "time_favorited": "0", "sort_id": 3, "resolved_title": "stitchfix/mab", "resolved_url": "https://github.com/stitchfix/mab", "excerpt": "Mab is a library/framework for scalable and customizable multi-armed bandits. It provides efficient pseudo-random implementations of epsilon-greedy and Thompson sampling strategies.", "is_article": "1", "is_index": "1", "has_video": "0", "has_image": "1", "word_count": "1254", "lang": "en", "time_to_read": 6, "top_image_url": "https://repository-images.githubusercontent.com/340162521/d1711100-774c-11eb-86cb-e4a5c793ebc8", "tags": {"bandits": {"item_id": "3268897680", "tag": "bandits"}, "golang": {"item_id": "3268897680", "tag": "golang"}, "machine-learning": {"item_id": "3268897680", "tag": "machine-learning"}}, "image": {"item_id": "3268897680", "src": "https://user-images.githubusercontent.com/5180129/108548622-f2df8200-72a0-11eb-8cc2-b4f1e839dffd.png", "width": "720", "height": "0"}, "images": {"1": {"item_id": "3268897680", "image_id": "1", "src": "https://user-images.githubusercontent.com/5180129/108548622-f2df8200-72a0-11eb-8cc2-b4f1e839dffd.png", "width": "720", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3268897680", "image_id": "2", "src": "https://github.com/stitchfix/mab/actions/workflows/go.yml/badge.svg", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3268897680", "image_id": "3", "src": "https://camo.githubusercontent.com/d0881f2e0a3ca411a694d32a4ddcf12603ee81d8346b22cf82adf8312e3ced0d/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f7374697463686669782f6d6162", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3268897680", "image_id": "4", "src": "https://camo.githubusercontent.com/6f7b751cde62c5ad7f50c986d60eb76689d4c5762b54a281a693b952d471a16b/68747470733a2f2f706b672e676f2e6465762f62616467652f6769746875622e636f6d2f7374697463686669782f6d61622e737667", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3268897680", "image_id": "5", "src": "https://user-images.githubusercontent.com/5180129/108559544-4a391e80-72b0-11eb-825c-483aba3dcd18.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 485}, "2921113336": {"item_id": "2921113336", "resolved_id": "2921113336", "given_url": "https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae", "given_title": "", "favorite": "0", "status": "1", "time_added": "1584747869", "time_updated": "1673901447", "time_read": "1589542167", "time_favorited": "0", "sort_id": 4, "resolved_title": "How We Optimized Hero Images on Hotels.com using Multi-Armed Bandit Algorithms", "resolved_url": "https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae", "excerpt": "In this entry, we introduce the multi-armed bandit (MAB) optimization campaign we have recently run on Hotels.com™.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1705", "lang": "en", "time_to_read": 8, "top_image_url": "https://miro.medium.com/max/770/1*4u5qpI-eOTFBbUnH6dVH9w.png", "tags": {"bandits": {"item_id": "2921113336", "tag": "bandits"}}, "authors": {"128762737": {"item_id": "2921113336", "author_id": "128762737", "name": "Fedor Parfenov", "url": "https://medium.com/@parfenov.fedor"}}, "image": {"item_id": "2921113336", "src": "https://miro.medium.com/max/1600/1*4u5qpI-eOTFBbUnH6dVH9w.png", "width": "700", "height": "471"}, "images": {"1": {"item_id": "2921113336", "image_id": "1", "src": "https://miro.medium.com/max/1600/1*4u5qpI-eOTFBbUnH6dVH9w.png", "width": "700", "height": "471", "credit": "", "caption": "Some properties have a lot of beautiful and interesting images."}, "2": {"item_id": "2921113336", "image_id": "2", "src": "https://miro.medium.com/max/1600/1*gtAzq35dY1glvwO_Sp50PA.png", "width": "700", "height": "35", "credit": "", "caption": ""}, "3": {"item_id": "2921113336", "image_id": "3", "src": "https://miro.medium.com/max/2000/1*7O-gqJqFwP3IgcgdMmV2Tw.png", "width": "1000", "height": "566", "credit": "", "caption": ""}, "4": {"item_id": "2921113336", "image_id": "4", "src": "https://miro.medium.com/max/1600/1*lQF_BDgts_mbx5srlkyenQ.png", "width": "700", "height": "410", "credit": "regret", "caption": "A MAB algorithm quickly focuses on better options limiting the opportunity cost of exploring suboptimal options"}, "5": {"item_id": "2921113336", "image_id": "5", "src": "https://miro.medium.com/max/1600/1*GrkYzPHO_yjqYhHlCwfEvg.png", "width": "700", "height": "25", "credit": "", "caption": ""}, "6": {"item_id": "2921113336", "image_id": "6", "src": "https://miro.medium.com/max/2000/1*QBQM_j5UuKZiqckGNSFQew.png", "width": "1000", "height": "403", "credit": "", "caption": "A classic high-level architecture"}, "7": {"item_id": "2921113336", "image_id": "7", "src": "https://miro.medium.com/max/1600/1*nEIIgF6j6ckxOBBHUUAAXA.png", "width": "700", "height": "347", "credit": "", "caption": "Our MAB Algorithm was sitting on top of an A/B test so we can monitor the KPI’s impact."}, "8": {"item_id": "2921113336", "image_id": "8", "src": "https://miro.medium.com/max/1600/1*YFelTg0GTTuYvgFyeJS18A.png", "width": "700", "height": "40", "credit": "", "caption": ""}, "9": {"item_id": "2921113336", "image_id": "9", "src": "https://miro.medium.com/max/2000/1*JKRoaRq08J0GVKdPo-6Ibg.png", "width": "1000", "height": "467", "credit": "", "caption": "During Phase I, we observed a poor performance at first. Then the bandit algorithms progressively learned which images work better and improved the metric."}, "10": {"item_id": "2921113336", "image_id": "10", "src": "https://miro.medium.com/max/1600/1*AZpg_TdELqc_lgxbZedekg.png", "width": "700", "height": "48", "credit": "", "caption": ""}, "11": {"item_id": "2921113336", "image_id": "11", "src": "https://miro.medium.com/max/2000/1*tJTk_dyyeMKm5iovl1P02w.png", "width": "1000", "height": "532", "credit": "right column", "caption": "Here is an example of images from two properties that have been selected by the owner, the computer vision algorithm and the MAB algo, respectively. The users feedback proved to be very helpful to find gems within our image database"}}, "domain_metadata": {"name": "Medium", "logo": "https://logo.clearbit.com/medium.com?size=800", "greyscale_logo": "https://logo.clearbit.com/medium.com?size=800&greyscale=true"}, "listen_duration_estimate": 660}, "2856846274": {"item_id": "2856846274", "resolved_id": "2666103548", "given_url": "https://mlwhiz.com/blog/2019/07/21/bandits/?utm_campaign=bayesian-bandits-explained-simply&utm_medium=social_link&utm_source=missinglettr-linkedin", "given_title": "", "favorite": "0", "status": "1", "time_added": "1579360624", "time_updated": "1673901447", "time_read": "1582142974", "time_favorited": "0", "sort_id": 5, "resolved_title": "Bayesian Bandits explained simply", "resolved_url": "https://mlwhiz.com/blog/2019/07/21/bandits/", "excerpt": "Exploration and Exploitation play a key role in any business. And any good business will try to “explore” various opportunities where it can make a profit.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1604", "lang": "en", "time_to_read": 7, "top_image_url": "https://mlwhiz.com/images/bandits/1.png", "tags": {"bandits": {"item_id": "2856846274", "tag": "bandits"}, "bayes": {"item_id": "2856846274", "tag": "bayes"}}, "authors": {"8623619": {"item_id": "2856846274", "author_id": "8623619", "name": "Rahul Agarwal", "url": ""}}, "image": {"item_id": "2856846274", "src": "https://mlwhiz.com/images/bandits/2.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2856846274", "image_id": "1", "src": "https://mlwhiz.com/images/bandits/2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2856846274", "image_id": "2", "src": "https://mlwhiz.com/images/bandits/3.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "2856846274", "image_id": "3", "src": "https://mlwhiz.com/images/bandits/4.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2856846274", "image_id": "4", "src": "https://mlwhiz.com/images/bandits/5.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2856846274", "image_id": "5", "src": "https://mlwhiz.com/images/bandits/6.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2856846274", "image_id": "6", "src": "https://mlwhiz.com/images/bandits/7.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "7": {"item_id": "2856846274", "image_id": "7", "src": "https://mlwhiz.com/images/bandits/8.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "8": {"item_id": "2856846274", "image_id": "8", "src": "https://mlwhiz.com/images/bandits/9.jpg", "width": "0", "height": "0", "credit": "", "caption": ""}, "9": {"item_id": "2856846274", "image_id": "9", "src": "https://mlwhiz.com/images/bandits/10.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "10": {"item_id": "2856846274", "image_id": "10", "src": "https://mlwhiz.com/images/bandits/11.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "listen_duration_estimate": 621}, "3070986547": {"item_id": "3070986547", "resolved_id": "3070986547", "given_url": "https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/", "given_title": "Multi-Armed Bandits and the Stitch Fix Experimentation Platform", "favorite": "0", "status": "1", "time_added": "1600037807", "time_updated": "1673901447", "time_read": "1604363502", "time_favorited": "0", "sort_id": 6, "resolved_title": "Multi-Armed Bandits and the Stitch Fix Experimentation Platform", "resolved_url": "https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/", "excerpt": "Multi-armed bandits have become a popular alternative to traditional A/B testing for online experimentation at Stitch Fix. We’ve recently decided to extend our experimentation platform to include multi-armed bandits as a first-class feature.", "is_article": "1", "is_index": "0", "has_video": "1", "has_image": "1", "word_count": "3266", "lang": "en", "time_to_read": 15, "top_image_url": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/multi_armed_bandit.png", "tags": {"bandits": {"item_id": "3070986547", "tag": "bandits"}, "prodmgmt": {"item_id": "3070986547", "tag": "prodmgmt"}}, "image": {"item_id": "3070986547", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/figure1.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "3070986547", "image_id": "1", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/figure1.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "3070986547", "image_id": "2", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/equation1.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "3070986547", "image_id": "3", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/figure3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "3070986547", "image_id": "4", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/figure4.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "3070986547", "image_id": "5", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/equation2.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "videos": {"1": {"item_id": "3070986547", "video_id": "1", "src": "https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/multi_armed_bandit.mp4", "width": "0", "height": "0", "type": "5", "vid": "", "length": "0"}}, "listen_duration_estimate": 1264}, "329619125": {"item_id": "329619125", "resolved_id": "329619125", "given_url": "https://support.google.com/analytics/answer/2844870", "given_title": "", "favorite": "1", "status": "1", "time_added": "1508674161", "time_updated": "1673901447", "time_read": "1508717305", "time_favorited": "1508717304", "sort_id": 7, "resolved_title": "Multi-armed bandit experiments", "resolved_url": "https://support.google.com/analytics/answer/2844870", "excerpt": "This article describes the statistical engine behind Analytics Content Experiments. Analytics uses a multi-armed bandit approach to managing online experiments. A multi-armed bandit is a type of experiment where:", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2126", "lang": "en", "time_to_read": 10, "tags": {"bandits": {"item_id": "329619125", "tag": "bandits"}}, "authors": {"7054940": {"item_id": "329619125", "author_id": "7054940", "name": "Steven L. Scott", "url": ""}}, "image": {"item_id": "329619125", "src": "https://www.google.com/images/analytics/mabFigure1.png", "width": "0", "height": "0"}, "images": {"1": {"item_id": "329619125", "image_id": "1", "src": "https://www.google.com/images/analytics/mabFigure1.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "329619125", "image_id": "2", "src": "https://www.google.com/images/analytics/mabFigure2.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "3": {"item_id": "329619125", "image_id": "3", "src": "https://www.google.com/images/analytics/mabFigure3.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "329619125", "image_id": "4", "src": "https://www.google.com/images/analytics/mabFigure4.png", "width": "0", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "329619125", "image_id": "5", "src": "https://www.google.com/images/analytics/mabFigure5.png", "width": "0", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Google", "logo": "https://logo.clearbit.com/google.com?size=800", "greyscale_logo": "https://logo.clearbit.com/google.com?size=800&greyscale=true"}, "listen_duration_estimate": 823}, "3169033843": {"item_id": "3169033843", "resolved_id": "3169032687", "given_url": "https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb?source=rss----7f60cf5620c9---4", "given_title": "A Comparison of Bandit Algorithms", "favorite": "0", "status": "1", "time_added": "1605015964", "time_updated": "1706233547", "time_read": "1605030662", "time_favorited": "0", "sort_id": 8, "resolved_title": "A Comparison of Bandit Algorithms", "resolved_url": "https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb", "excerpt": "Over the course of this series we’ve looked at the framework and terminology that are used to define Multi-Armed Bandits.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "2453", "lang": "en", "time_to_read": 11, "top_image_url": "https://miro.medium.com/max/1200/0*e7T9wQtgE2cro-mC", "tags": {"algorithms-math": {"item_id": "3169033843", "tag": "algorithms-math"}, "bandits": {"item_id": "3169033843", "tag": "bandits"}, "machine-learning": {"item_id": "3169033843", "tag": "machine-learning"}}, "authors": {"141898819": {"item_id": "3169033843", "author_id": "141898819", "name": "Steve Roberts", "url": "https://medium.com/@tinkertytonk"}}, "image": {"item_id": "3169033843", "src": "https://miro.medium.com/fit/c/56/56/1*VYsKbsmbOHlYgCAFpJ4HKQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3169033843", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*VYsKbsmbOHlYgCAFpJ4HKQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3169033843", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*e7T9wQtgE2cro-mC", "width": "700", "height": "467", "credit": "Jason Dent on Unsplash", "caption": ""}, "3": {"item_id": "3169033843", "image_id": "3", "src": "https://miro.medium.com/max/128/1*B1k0BRVMwLxzypQKmCHH1A.gif", "width": "64", "height": "64", "credit": "", "caption": ""}, "4": {"item_id": "3169033843", "image_id": "4", "src": "https://miro.medium.com/max/640/0*-y8Vrj0zr3AZA-Dh.png", "width": "320", "height": "128", "credit": "", "caption": ""}, "5": {"item_id": "3169033843", "image_id": "5", "src": "https://miro.medium.com/max/512/1*et2_r7_Vah4d6TxMO9G5tA.gif", "width": "256", "height": "256", "credit": "", "caption": ""}, "6": {"item_id": "3169033843", "image_id": "6", "src": "https://miro.medium.com/max/608/1*n7K4fvvZ9PIYhDfuKPUZVg.png", "width": "304", "height": "173", "credit": "", "caption": ""}, "7": {"item_id": "3169033843", "image_id": "7", "src": "https://miro.medium.com/max/1400/1*0ts0nyaCgRKwjpE-ZM0W3A.png", "width": "700", "height": "560", "credit": "", "caption": "Figure 6.1: A comparison of bandit algorithms on the 5-socket power problem."}, "8": {"item_id": "3169033843", "image_id": "8", "src": "https://miro.medium.com/max/1400/1*YO12qBiS61vjhv5muBh3ew.png", "width": "700", "height": "420", "credit": "socket 4", "caption": "Figure 6.2: The reward distribution of 10 sockets. The socket order defines the relative goodness of the sockets, from lowest to highest output, with 10 being the best"}, "9": {"item_id": "3169033843", "image_id": "9", "src": "https://miro.medium.com/max/1400/1*fHZBb70t4RKd4IASEZgClA.png", "width": "700", "height": "420", "credit": "", "caption": "Figure 6.3: The density plot of socket outputs. Socket 4 gives the highest mean output and socket 3 the least."}, "10": {"item_id": "3169033843", "image_id": "10", "src": "https://miro.medium.com/max/612/1*GeSeMMpySGKVH1F3LNd-5w.png", "width": "306", "height": "173", "credit": "", "caption": ""}, "11": {"item_id": "3169033843", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*aylno0_OjJaPwPZ1EVo-Qw.png", "width": "700", "height": "560", "credit": "", "caption": "Figure 6.4: A comparison of bandit algorithms on the 10-socket power problem, with a spread of 0.2 seconds of charge."}, "12": {"item_id": "3169033843", "image_id": "12", "src": "https://miro.medium.com/max/604/1*SA6kwLYZccGy5lMzI8whsQ.png", "width": "302", "height": "168", "credit": "", "caption": ""}, "13": {"item_id": "3169033843", "image_id": "13", "src": "https://miro.medium.com/max/1400/1*fRW0mJbKGtD2feRFrLtCIA.png", "width": "700", "height": "560", "credit": "", "caption": "Figure 6.5: A comparison of bandit algorithms on the 100-socket power problem, with a spread of 0.1 seconds of charge."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 950}, "4002561373": {"item_id": "4002561373", "resolved_id": "4002561390", "given_url": "https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=rss----7f60cf5620c9---4", "given_title": "An Overview of Contextual Bandits", "favorite": "0", "status": "1", "time_added": "1706889928", "time_updated": "1706929325", "time_read": "1706929325", "time_favorited": "0", "sort_id": 9, "resolved_title": "An Overview of Contextual Bandits", "resolved_url": "https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034", "excerpt": "Imagine a scenario where you just started an A/B test that will be running for the next two weeks. However, after just a day or two, it is becoming increasingly clear that version A is working better for certain types of users, whereas version B is working better for another set of users.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "4411", "lang": "en", "time_to_read": 20, "top_image_url": "https://miro.medium.com/v2/resize:fit:1200/1*iFGYh1TRUNycQjYp0cPsQQ.png", "tags": {"bandits": {"item_id": "4002561373", "tag": "bandits"}, "machine-learning": {"item_id": "4002561373", "tag": "machine-learning"}}, "authors": {"187778298": {"item_id": "4002561373", "author_id": "187778298", "name": "Ugur Yildirim", "url": "https://medium.com/@uguryi"}}, "image": {"item_id": "4002561373", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*1wXJBnRyddpzCyYSh-rrbw.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "4002561373", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*1wXJBnRyddpzCyYSh-rrbw.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "4002561373", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1707}, "3142746538": {"item_id": "3142746538", "resolved_id": "3142746567", "given_url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18?source=rss----7f60cf5620c9---4", "given_title": "Bandit Algorithms", "favorite": "0", "status": "1", "time_added": "1602776184", "time_updated": "1673901447", "time_read": "1604361011", "time_favorited": "0", "sort_id": 10, "resolved_title": "Bandit Algorithms", "resolved_url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "excerpt": "Baby Robot is lost in the mall. Using Reinforcement Learning we want to help him find his way back to his mum. However, before he can even begin looking for her, he needs to recharge, from a set of power sockets that each give a slightly different amount of charge.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3440", "lang": "en", "time_to_read": 16, "top_image_url": "https://miro.medium.com/max/1200/0*Qh6b6kmOXe6Z87sG", "tags": {"bandits": {"item_id": "3142746538", "tag": "bandits"}}, "authors": {"141898819": {"item_id": "3142746538", "author_id": "141898819", "name": "Steve Roberts", "url": "https://medium.com/@tinkertytonk"}}, "image": {"item_id": "3142746538", "src": "https://miro.medium.com/fit/c/56/56/1*VYsKbsmbOHlYgCAFpJ4HKQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3142746538", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*VYsKbsmbOHlYgCAFpJ4HKQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3142746538", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*Qh6b6kmOXe6Z87sG", "width": "700", "height": "360", "credit": "el pepe on Unsplash", "caption": ""}, "3": {"item_id": "3142746538", "image_id": "3", "src": "https://miro.medium.com/max/128/1*B1k0BRVMwLxzypQKmCHH1A.gif", "width": "64", "height": "64", "credit": "", "caption": ""}, "4": {"item_id": "3142746538", "image_id": "4", "src": "https://miro.medium.com/max/640/1*ZQ3iLniVpZowr9UVByiEnQ.png", "width": "320", "height": "128", "credit": "", "caption": ""}, "5": {"item_id": "3142746538", "image_id": "5", "src": "https://miro.medium.com/max/512/1*et2_r7_Vah4d6TxMO9G5tA.gif", "width": "256", "height": "256", "credit": "", "caption": ""}, "6": {"item_id": "3142746538", "image_id": "6", "src": "https://miro.medium.com/max/458/1*wzdMxeBjRlu5_TFnBh6HNg.png", "width": "229", "height": "51", "credit": "", "caption": ""}, "7": {"item_id": "3142746538", "image_id": "7", "src": "https://miro.medium.com/max/864/1*Q_MrTZNMV0WW4yFoPlFnZg.png", "width": "432", "height": "288", "credit": "", "caption": ""}, "8": {"item_id": "3142746538", "image_id": "8", "src": "https://miro.medium.com/max/876/1*g_t8KovDrILH200mCsiB_g.png", "width": "438", "height": "600", "credit": "", "caption": "Table 1: Socket reward estimates for 20 time steps using Optimistic-Greedy algorithm."}, "9": {"item_id": "3142746538", "image_id": "9", "src": "https://miro.medium.com/max/1440/1*wy8Ur0MQp_RC3kqoVdUokQ.png", "width": "720", "height": "576", "credit": "", "caption": "Figure 3.1: Optimistic initialisation of the power socket outputs."}, "10": {"item_id": "3142746538", "image_id": "10", "src": "https://miro.medium.com/max/1400/1*4XE5AShnUI2IdzCxPkz7Dw.png", "width": "700", "height": "280", "credit": "", "caption": "Figure 3.2: Optimistic Initialisation."}, "11": {"item_id": "3142746538", "image_id": "11", "src": "https://miro.medium.com/max/1400/1*i8LyQ0_IsUl-mHvFYg1LtA.png", "width": "700", "height": "560", "credit": "", "caption": "Figure 3.3: Optimistic initialisation — mean total reward vs initial values."}, "12": {"item_id": "3142746538", "image_id": "12", "src": "https://miro.medium.com/max/3036/1*tE6QK3eEM7CvQfAWBP6fuQ.png", "width": "1518", "height": "360", "credit": "", "caption": "Figure 3.4: Epsilon-Greedy — 100 time steps per run."}, "13": {"item_id": "3142746538", "image_id": "13", "src": "https://miro.medium.com/max/604/1*yBZFUXHyfAiIsSC-W8rxCg.png", "width": "302", "height": "51", "credit": "", "caption": ""}, "14": {"item_id": "3142746538", "image_id": "14", "src": "https://miro.medium.com/max/900/1*DL2Hoy9ZdrwYKW2_6o2nPg.png", "width": "450", "height": "81", "credit": "", "caption": ""}, "15": {"item_id": "3142746538", "image_id": "15", "src": "https://miro.medium.com/max/410/1*qzck-69SP8YEHxnrXVjflw.png", "width": "205", "height": "65", "credit": "", "caption": ""}, "16": {"item_id": "3142746538", "image_id": "16", "src": "https://miro.medium.com/max/2880/1*M8BOD-oHh1qFGCklPMVYxQ.png", "width": "1440", "height": "432", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1332}, "3920580085": {"item_id": "3920580085", "resolved_id": "3920580085", "given_url": "https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac", "given_title": "Dynamic Pricing with Multi-Armed Bandit: Learning by Doing!", "favorite": "0", "status": "1", "time_added": "1692213338", "time_updated": "1692452179", "time_read": "1692452179", "time_favorited": "0", "sort_id": 11, "resolved_title": "Dynamic Pricing with Multi-Armed Bandit: Learning by Doing", "resolved_url": "https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac", "excerpt": "In the vast world of decision-making problems, one dilemma is particularly owned by Reinforcement Learning strategies: exploration versus exploitation.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "3563", "lang": "en", "time_to_read": 16, "top_image_url": "https://miro.medium.com/v2/resize:fit:1200/0*PtB_85QrbCPNJXXb", "tags": {"bandits": {"item_id": "3920580085", "tag": "bandits"}, "machine-learning": {"item_id": "3920580085", "tag": "machine-learning"}, "pricing": {"item_id": "3920580085", "tag": "pricing"}, "prodmgmt": {"item_id": "3920580085", "tag": "prodmgmt"}}, "authors": {"141251645": {"item_id": "3920580085", "author_id": "141251645", "name": "Massimiliano Costacurta", "url": "https://medium.com/@massi.costacurta"}}, "image": {"item_id": "3920580085", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*GHEj_mFZvyJsDSCCebgllA.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "3920580085", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*GHEj_mFZvyJsDSCCebgllA.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "3920580085", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1379}, "3580803792": {"item_id": "3580803792", "resolved_id": "3580803803", "given_url": "https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db?source=rss----7f60cf5620c9---4", "given_title": "Multi-Armed Bandit Algorithms: Thompson Sampling", "favorite": "0", "status": "1", "time_added": "1648126221", "time_updated": "1673901447", "time_read": "1648407968", "time_favorited": "0", "sort_id": 12, "resolved_title": "Multi-Armed Bandit Algorithms: Thompson Sampling", "resolved_url": "https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db", "excerpt": "Imagine that there is a row of slot machines in front of you. You do not know which machine will give you the best chance to win.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "1230", "lang": "en", "time_to_read": 6, "top_image_url": "https://miro.medium.com/max/1200/0*9YMbE-00b8VRceig", "tags": {"bandits": {"item_id": "3580803792", "tag": "bandits"}, "machine-learning": {"item_id": "3580803792", "tag": "machine-learning"}}, "authors": {"144787845": {"item_id": "3580803792", "author_id": "144787845", "name": "Sophia Yang", "url": "https://sophiamyang.medium.com"}}, "image": {"item_id": "3580803792", "src": "https://miro.medium.com/max/1400/0*9YMbE-00b8VRceig", "width": "700", "height": "442"}, "images": {"1": {"item_id": "3580803792", "image_id": "1", "src": "https://miro.medium.com/max/1400/0*9YMbE-00b8VRceig", "width": "700", "height": "442", "credit": "", "caption": "Image by Author"}, "2": {"item_id": "3580803792", "image_id": "2", "src": "https://miro.medium.com/max/1400/0*bXFRxSjfQKuvgmy7", "width": "700", "height": "414", "credit": "", "caption": "Image by Author"}, "3": {"item_id": "3580803792", "image_id": "3", "src": "https://miro.medium.com/max/1400/0*Ar4xE2uO9-LFWTCp", "width": "700", "height": "432", "credit": "", "caption": "Image by Author"}, "4": {"item_id": "3580803792", "image_id": "4", "src": "https://miro.medium.com/max/1400/0*qxo-nE2iKTpRC6oR", "width": "700", "height": "437", "credit": "", "caption": "Image by Author"}, "5": {"item_id": "3580803792", "image_id": "5", "src": "https://miro.medium.com/max/1400/0*K3TPhHxlIWQjczxn", "width": "700", "height": "438", "credit": "", "caption": "Image by Author"}, "6": {"item_id": "3580803792", "image_id": "6", "src": "https://miro.medium.com/max/1400/0*_eN4pYep_Gvxk2jp", "width": "700", "height": "347", "credit": "", "caption": "Image by Author"}, "7": {"item_id": "3580803792", "image_id": "7", "src": "https://miro.medium.com/max/1400/0*6w0Jv59z3WGQmq5m", "width": "700", "height": "339", "credit": "", "caption": "Image by Author"}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 476}, "3276160570": {"item_id": "3276160570", "resolved_id": "3276160570", "given_url": "https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d", "given_title": "Thompson Sampling using Conjugate Priors", "favorite": "0", "status": "1", "time_added": "1615328315", "time_updated": "1673901447", "time_read": "1615372196", "time_favorited": "0", "sort_id": 13, "resolved_title": "Thompson Sampling using Conjugate Priors", "resolved_url": "https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d", "excerpt": "Baby Robot has entered a charging room containing 5 different power sockets. Each of these sockets returns a slightly different amount of charge. We want to get Baby Robot charged up in the minimum amount of time, so we need to locate the best socket and then use it until charging is complete.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "4066", "lang": "en", "time_to_read": 18, "top_image_url": "https://miro.medium.com/freeze/max/712/1*_Gqr8su3G4inxRnEuTbC4Q.gif", "tags": {"bandits": {"item_id": "3276160570", "tag": "bandits"}, "machine-learning": {"item_id": "3276160570", "tag": "machine-learning"}}, "authors": {"141898819": {"item_id": "3276160570", "author_id": "141898819", "name": "Steve Roberts", "url": "https://medium.com/@tinkertytonk"}}, "image": {"item_id": "3276160570", "src": "https://miro.medium.com/fit/c/56/56/1*VYsKbsmbOHlYgCAFpJ4HKQ.jpeg", "width": "28", "height": "28"}, "images": {"1": {"item_id": "3276160570", "image_id": "1", "src": "https://miro.medium.com/fit/c/56/56/1*VYsKbsmbOHlYgCAFpJ4HKQ.jpeg", "width": "28", "height": "28", "credit": "", "caption": ""}, "2": {"item_id": "3276160570", "image_id": "2", "src": "https://miro.medium.com/max/1400/1*_Gqr8su3G4inxRnEuTbC4Q.gif", "width": "700", "height": "382", "credit": "All images by author", "caption": ""}, "3": {"item_id": "3276160570", "image_id": "3", "src": "https://miro.medium.com/max/640/0*2rMEv1b5M8WXEzeO.png", "width": "320", "height": "128", "credit": "", "caption": ""}, "4": {"item_id": "3276160570", "image_id": "4", "src": "https://miro.medium.com/max/474/1*7vjUrKTI3GqLlrS8KE7vLg.png", "width": "237", "height": "40", "credit": "", "caption": ""}, "5": {"item_id": "3276160570", "image_id": "5", "src": "https://miro.medium.com/max/780/1*4trvlnDS0lKTlc0Bvs1kRw.png", "width": "390", "height": "100", "credit": "", "caption": ""}, "6": {"item_id": "3276160570", "image_id": "6", "src": "https://miro.medium.com/max/1326/1*gXifNZaQHj6m6RIMooElDA.png", "width": "663", "height": "590", "credit": "true mean = 8 and true variance = 5", "caption": "Posterior update for an unknown mean and known variance"}, "7": {"item_id": "3276160570", "image_id": "7", "src": "https://miro.medium.com/max/1338/1*2aKZT7W-ORzy51uRWya2lQ.png", "width": "669", "height": "482", "credit": "", "caption": "The posterior distribution, shown over increasing trials, for a normal conjugate prior"}, "8": {"item_id": "3276160570", "image_id": "8", "src": "https://miro.medium.com/max/702/1*tuaavDGzIxf780IJBwbszA.png", "width": "351", "height": "157", "credit": "", "caption": "Update equations for the gamma conjugate prior"}, "9": {"item_id": "3276160570", "image_id": "9", "src": "https://miro.medium.com/max/1326/1*Slxex5m0UoMyFXmhWJ7aHg.png", "width": "663", "height": "590", "credit": "true mean = 8 and true variance = 5", "caption": "Posterior update for a known mean and unknown variance"}, "10": {"item_id": "3276160570", "image_id": "10", "src": "https://miro.medium.com/max/1318/1*MCn1dzUGBnQY2IrfTndHfw.png", "width": "659", "height": "536", "credit": "", "caption": "The posterior distribution, shown over increasing trials, for a gamma conjugate prior"}, "11": {"item_id": "3276160570", "image_id": "11", "src": "https://miro.medium.com/max/1126/1*LN3s1_mM7muV7QzhstgkCg.png", "width": "563", "height": "211", "credit": "", "caption": ""}, "12": {"item_id": "3276160570", "image_id": "12", "src": "https://miro.medium.com/max/1326/1*6nfjhSko2crm8QBCL9r6MQ.png", "width": "663", "height": "590", "credit": "true mean = 8 and true variance = 5", "caption": "Posterior update for an unknown mean and unknown variance"}, "13": {"item_id": "3276160570", "image_id": "13", "src": "https://miro.medium.com/max/1354/1*9aAknz8rSRVjYTHeB1KOtw.png", "width": "677", "height": "387", "credit": "", "caption": "Power output distributions of the sockets with unique mean and variance."}, "14": {"item_id": "3276160570", "image_id": "14", "src": "https://miro.medium.com/max/1400/1*NF6-QHn8pcGbw7v02-Xrag.png", "width": "700", "height": "638", "credit": "", "caption": "Thompson Sampling on sockets with an underlying normal distribution of unknown mean and variance."}, "15": {"item_id": "3276160570", "image_id": "15", "src": "https://miro.medium.com/max/1400/1*rhNpYXNrI217V6KDzGXLXg.gif", "width": "700", "height": "382", "credit": "", "caption": "The posterior estimate updates of the 5 sockets, shown over 100 trials."}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 1574}, "3779110245": {"item_id": "3779110245", "resolved_id": "3779061308", "given_url": "https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html", "given_title": "Introduction to Multi-Armed Bandit Problems", "favorite": "0", "status": "1", "time_added": "1672766554", "time_updated": "1706233547", "time_read": "1673123609", "time_favorited": "0", "sort_id": 14, "resolved_title": "Introduction to Multi-Armed Bandit Problems", "resolved_url": "https://www.kdnuggets.com/introduction-to-multi-armed-bandit-problems.html", "excerpt": "A multi-armed bandit (MAB) is a machine learning framework that uses complex algorithms to dynamically allocate resources when presented with multiple choices.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "0", "word_count": "1225", "lang": "en", "time_to_read": 6, "top_image_url": "https://www.kdnuggets.com/wp-content/uploads/popovic_introduction_multiarmed_bandit_problems_1.png", "tags": {"algorithms-math": {"item_id": "3779110245", "tag": "algorithms-math"}, "bandits": {"item_id": "3779110245", "tag": "bandits"}, "machine-learning": {"item_id": "3779110245", "tag": "machine-learning"}}, "authors": {"176577985": {"item_id": "3779110245", "author_id": "176577985", "name": "Alex Popovic", "url": "https://www.kdnuggets.com/author/alex-popovic"}}, "domain_metadata": {"name": "KDnuggets", "logo": "https://logo.clearbit.com/kdnuggets.com?size=800", "greyscale_logo": "https://logo.clearbit.com/kdnuggets.com?size=800&greyscale=true"}, "listen_duration_estimate": 474}, "2897542358": {"item_id": "2897542358", "resolved_id": "2897542358", "given_url": "https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits/", "given_title": "Exploring the fundamentals of multi-armed bandits", "favorite": "0", "status": "1", "time_added": "1582807598", "time_updated": "1673901447", "time_read": "1583785007", "time_favorited": "0", "sort_id": 15, "resolved_title": "Exploring the fundamentals of multi-armed bandits", "resolved_url": "https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits/", "excerpt": "If you haven’t ever been in a casino, you may have found yourself asking one very pertinent question: On which slot machine am I going to hit the jackpot? Standing in front of a bank of identical-looking machines, you have only instinct to go on.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "717", "lang": "en", "time_to_read": 3, "top_image_url": "https://www.microsoft.com/en-us/research/uploads/prod/2020/02/MSFT_Research_MultiarmedBandit_final_1400X788_final.png", "tags": {"bandits": {"item_id": "2897542358", "tag": "bandits"}, "machine-learning": {"item_id": "2897542358", "tag": "machine-learning"}}, "authors": {"65108360": {"item_id": "2897542358", "author_id": "65108360", "name": "Alex Slivkins", "url": "https://www.microsoft.com/en-us/research/people/slivkins/"}}, "image": {"item_id": "2897542358", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/02/MSFT_Research_MultiarmedBandit_final_1400X788_final.png", "width": "1400", "height": "788"}, "images": {"1": {"item_id": "2897542358", "image_id": "1", "src": "https://www.microsoft.com/en-us/research/uploads/prod/2020/02/MSFT_Research_MultiarmedBandit_final_1400X788_final.png", "width": "1400", "height": "788", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Microsoft", "logo": "https://logo.clearbit.com/microsoft.com?size=800", "greyscale_logo": "https://logo.clearbit.com/microsoft.com?size=800&greyscale=true"}, "listen_duration_estimate": 278}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709419167}