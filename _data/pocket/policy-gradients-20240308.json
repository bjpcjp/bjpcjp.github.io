{"status": 1, "complete": 1, "list": {"2749835414": {"item_id": "2749835414", "resolved_id": "2749835414", "given_url": "https://github.com/MrSyee/pg-is-all-you-need", "given_title": "MrSyee/pg-is-all-you-need: Policy Gradient is all you need! A step-by-step ", "favorite": "0", "status": "1", "time_added": "1570503860", "time_updated": "1638708525", "time_read": "1576355404", "time_favorited": "0", "sort_id": 0, "resolved_title": "PG is all you need!", "resolved_url": "https://github.com/MrSyee/pg-is-all-you-need", "excerpt": "PG is all you need! This is a step-by-step tutorial for Policy Gradient algorithms from A2C to SAC, including learning acceleration methods using demonstrations for treating real applications with sparse rewards.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "428", "lang": "en", "top_image_url": "https://opengraph.githubassets.com/87fcc101c22001f008eee32094a10014807c52f01c6904f756b04ae958eb7fd9/MrSyee/pg-is-all-you-need", "tags": {"deep-learning": {"item_id": "2749835414", "tag": "deep-learning"}, "policy-gradients": {"item_id": "2749835414", "tag": "policy-gradients"}}, "authors": {"112381369": {"item_id": "2749835414", "author_id": "112381369", "name": "emoji key", "url": "https://allcontributors.org/docs/en/emoji-key"}}, "image": {"item_id": "2749835414", "src": "https://camo.githubusercontent.com/3f29481ce9ea7caed48cceaa0255584ec5519f38b00fec3f5aaf7d9aff3cb5c8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616c6c5f636f6e7472696275746f72732d342d6f72616e67652e7376673f7374796c653d666c61742d737175617265", "width": "0", "height": "0"}, "images": {"1": {"item_id": "2749835414", "image_id": "1", "src": "https://camo.githubusercontent.com/3f29481ce9ea7caed48cceaa0255584ec5519f38b00fec3f5aaf7d9aff3cb5c8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f616c6c5f636f6e7472696275746f72732d342d6f72616e67652e7376673f7374796c653d666c61742d737175617265", "width": "0", "height": "0", "credit": "", "caption": ""}, "2": {"item_id": "2749835414", "image_id": "2", "src": "https://user-images.githubusercontent.com/17582508/76502245-0bd39680-6487-11ea-8f59-cbde1b841af9.gif", "width": "200", "height": "140", "credit": "", "caption": ""}, "3": {"item_id": "2749835414", "image_id": "3", "src": "https://avatars3.githubusercontent.com/u/17582508?v=4", "width": "100", "height": "0", "credit": "", "caption": ""}, "4": {"item_id": "2749835414", "image_id": "4", "src": "https://avatars3.githubusercontent.com/u/14961526?v=4", "width": "100", "height": "0", "credit": "", "caption": ""}, "5": {"item_id": "2749835414", "image_id": "5", "src": "https://avatars0.githubusercontent.com/u/43226417?v=4", "width": "100", "height": "0", "credit": "", "caption": ""}, "6": {"item_id": "2749835414", "image_id": "6", "src": "https://avatars0.githubusercontent.com/u/37307369?v=4", "width": "100", "height": "0", "credit": "", "caption": ""}}, "domain_metadata": {"name": "GitHub", "logo": "https://logo.clearbit.com/github.com?size=800", "greyscale_logo": "https://logo.clearbit.com/github.com?size=800&greyscale=true"}, "listen_duration_estimate": 166}, "4004054417": {"item_id": "4004054417", "resolved_id": "4004054417", "given_url": "https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40", "given_title": "Policy Gradients: The Foundation of RLHF", "favorite": "0", "status": "1", "time_added": "1707218812", "time_updated": "1708320665", "time_read": "1708320665", "time_favorited": "0", "sort_id": 1, "resolved_title": "Policy Gradients: The Foundation of RLHF", "resolved_url": "https://towardsdatascience.com/policy-gradients-the-foundation-of-rlhf-337346beef40", "excerpt": "Although useful for a variety of applications, reinforcement learning (RL) is a key component of the alignment process for large language models (LLMs) due to its use in reinforcement learning from human feedback (RLHF). Unfortunately, RL is less widely understood within the AI community.", "is_article": "1", "is_index": "0", "has_video": "0", "has_image": "1", "word_count": "328", "lang": "en", "top_image_url": "https://miro.medium.com/v2/resize:fit:1200/1*voPcx38gcf1rwmm-j12Mcw.jpeg", "tags": {"policy-gradients": {"item_id": "4004054417", "tag": "policy-gradients"}}, "authors": {"125209980": {"item_id": "4004054417", "author_id": "125209980", "name": "Cameron R. Wolfe", "url": ""}}, "image": {"item_id": "4004054417", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*JhmKo3dvmoRoEfnoU02oSQ.jpeg", "width": "44", "height": "44"}, "images": {"1": {"item_id": "4004054417", "image_id": "1", "src": "https://miro.medium.com/v2/resize:fill:88:88/1*JhmKo3dvmoRoEfnoU02oSQ.jpeg", "width": "44", "height": "44", "credit": "", "caption": ""}, "2": {"item_id": "4004054417", "image_id": "2", "src": "https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg", "width": "24", "height": "24", "credit": "", "caption": ""}}, "domain_metadata": {"name": "Towards Data Science", "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800", "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"}, "listen_duration_estimate": 127}}, "error": nil, "search_meta": {"search_type": "normal"}, "since": 1709934812}