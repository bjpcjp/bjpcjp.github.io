---
title: "tokens"
layout: default-foundation-20210515
date: 2025-06-04
tags: [tokens]
---

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/foundation-sites@6.7.5/dist/css/foundation.min.css">

<div class="grid-container">
  <div class="callout">
    <h2>tokens</h2>
  </div>

  <div class="grid-x grid-margin-x small-up-1 medium-up-2 large-up-3">
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2025-02-17T09:22:43.950Z
        tags: llms, tokens, python
        -->
        <div class="card-divider">
          <a href="https://www.marktechpost.com/2025/02/16/a-step-by-step-guide-to-setting-up-a-custom-bpe-tokenizer-with-tiktoken-for-advanced-nlp-applications-in-python/">A Step-by-Step Guide to Setting Up a Custom BPE Tokenizer with Tiktoken for Advanced NLP Applications in Python</a>
        </div>
        <div class="card-image">
          <img src="https://www.marktechpost.com/wp-content/uploads/2025/02/Screenshot-2025-02-16-at-10.17.33â€¯PM.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>A Step-by-Step Guide to Setting Up a Custom BPE Tokenizer with Tiktoken for Advanced NLP Applications in Python</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-03-18T22:10:03.000Z
        tags: nlp, tokens
        -->
        <div class="card-divider">
          <a href="https://www.marktechpost.com/2024/03/09/unlocking-the-best-tokenization-strategies-how-greedy-inference-and-sage-lead-the-way-in-nlp-models">Unlocking the Best Tokenization Strategies: How Greedy Inference and SaGe L</a>
        </div>
        <div class="card-image">
          <img src="https://www.marktechpost.com/wp-content/uploads/2024/03/Screenshot-2024-03-09-at-10.30.54-PM.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>The inference method is crucial for NLP models in subword tokenization. Methods like BPE, WordPiece, and UnigramLM offer distinct mappings, but their performance differences must be better understood. Implementations like Huggingface Tokenizers often need to be clearer or limit inference choices, complicating compatibility with vocabulary learning algorithms. Whether a matching inference method is necessary or optimal for tokenizer vocabularies is uncertain. Previous research focused on developing vocabulary construction algorithms such as BPE, WordPiece, and UnigramLM, exploring optimal vocabulary size and multilingual vocabularies. Some studies examined the effects of vocabularies on downstream performance, information theory, and cognitive plausibility. Limited work on</p>
        </div>
      </div>
    </div>
  </div>
</div>
