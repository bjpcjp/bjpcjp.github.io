---
title: "transformers"
layout: default-foundation-20210515
date: 2025-06-04
tags: [transformers]
---

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/foundation-sites@6.7.5/dist/css/foundation.min.css">

<div class="grid-container">
  <div class="callout">
    <h2>transformers</h2>
  </div>

  <div class="grid-x grid-margin-x small-up-1 medium-up-2 large-up-3">
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2025-04-20T20:42:20.368Z
        tags: llms, transformers, language-linguistics
        -->
        <div class="card-divider">
          <a href="https://www.quantamagazine.org/to-make-language-models-work-better-researchers-sidestep-language-20250414/">To Make Language Models Work Better, Researchers Sidestep Language | Quanta Magazine</a>
        </div>
        <div class="card-image">
          <img src="https://www.quantamagazine.org/wp-content/uploads/2025/04/LLMNativeLanguage-crMyriamWares-Social.jpg" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>We insist that large language models repeatedly translate their mathematical processes into words. There may be a better way.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-12-15T11:42:06.068Z
        tags: llms, transformers
        -->
        <div class="card-divider">
          <a href="https://www.marktechpost.com/2024/12/14/how-llms-store-and-use-knowledge-this-ai-paper-introduces-knowledge-circuits-a-framework-for-understanding-and-improving-knowledge-storage-in-transformer-based-llms/">How LLMs Store and Use Knowledge? This AI Paper Introduces Knowledge Circuits: A Framework for Understanding and Improving Knowledge Storage in Transformer-Based LLMs</a>
        </div>
        <div class="card-image">
          <img src="https://www.marktechpost.com/wp-content/uploads/2024/12/Screenshot-2024-12-14-at-8.35.53%E2%80%AFPM.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Large language models (LLMs) can understand and generate human-like text by encoding vast knowledge repositories within their parameters. This capacity enables them to perform complex reasoning tasks, adapt to various applications, and interact effectively with humans. However, despite their remarkable achievements, researchers continue to investigate the mechanisms underlying the storage and utilization of knowledge in these systems, aiming to enhance their efficiency and reliability further. A key challenge in using large language models is their propensity to generate inaccurate, biased, or hallucinatory outputs. These problems arise from a limited understanding of how such models organize and access knowledge. Without clear</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2025-01-28T13:52:07.455Z
        tags: transformers, llms, attention
        -->
        <div class="card-divider">
          <a href="https://planetbanatt.net/articles/mla.html">On MLA</a>
        </div>
        <div class="card-image">
          <img src="https://planetbanatt.net/images/mla/manifold_perturbation.png" alt="cover image" style="max-width: 100%;">
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-12-12T19:27:10.808Z
        tags: llms, transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d?source=rss----7f60cf5620c9---4">Transformers Key-Value (KV) Caching Explained</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/resize:fit:1200/1*ub2DQhz0aHT0-Tyaw3hGkQ.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Speed up your LLM inference</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-11-11T18:44:44.743Z
        tags: hugging-face, nlp, transformers
        -->
        <div class="card-divider">
          <a href="https://www.marktechpost.com/2024/11/11/hugging-face-releases-sentence-transformers-v3-3-0-a-major-leap-for-nlp-efficiency/">Hugging Face Releases Sentence Transformers v3.3.0: A Major Leap for NLP Efficiency</a>
        </div>
        <div class="card-image">
          <img src="https://www.marktechpost.com/wp-content/uploads/2024/11/Screenshot-2024-11-11-at-9.58.28%E2%80%AFAM.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Natural Language Processing (NLP) has rapidly evolved in the last few years, with transformers emerging as a game-changing innovation. Yet, there are still notable challenges when using NLP tools to develop applications for tasks like semantic search, question answering, or document embedding. One key issue has been the need for models that not only perform well but also work efficiently on a range of devices, especially those with limited computational resources, such as CPUs. Models tend to require substantial processing power to yield high accuracy, and this trade-off often leaves developers choosing between performance and practicality. Additionally, deploying large models</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-08-24T00:00:00.000Z
        tags: ruby, transformers
        -->
        <div class="card-divider">
          <a href="https://github.com/ankane/transformers-ruby">ankane/transformers-ruby: State-of-the-art transformers for Ruby</a>
        </div>
        <div class="card-image">
          <img src="https://opengraph.githubassets.com/8938a007c9fa8dda076ca74c9caa2ef76718a7aa95af35704fea9e3794f051e3/ankane/transformers-ruby" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>State-of-the-art transformers for Ruby.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-07-20T23:01:10.000Z
        tags: attention, llms, transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26">Understanding Positional Embeddings in Transformers: From Absolute to Rotar</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/resize:fit:1200/1*EWz8ImltNHpDjMB8bOq_tQ.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>A deep dive into absolute, relative, and rotary positional embeddings with code examples</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-06-28T16:24:14.000Z
        tags: semiconductors, transformers
        -->
        <div class="card-divider">
          <a href="https://www.marktechpost.com/2024/06/26/meet-sohu-the-worlds-first-transformer-specialized-chip-asic">Meet Sohu:&nbsp;The&nbsp;World’s&nbsp;First&nbsp;Transformer&nbsp;Specialized Chip ASIC</a>
        </div>
        <div class="card-image">
          <img src="https://www.marktechpost.com/wp-content/uploads/2024/06/Screenshot-2024-06-26-at-8.35.01-PM-1024x525.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>The Sohu AI chip by Etched is a thundering breakthrough, boasting the title of the fastest AI chip to date. Its design is a testament to cutting-edge innovation, aiming to redefine the possibilities within AI computations and applications. At the center of Sohu's exceptional performance is its advanced processing capabilities, which enable it to handle complex computations at unprecedented speeds. With a capability of processing over 500,000 tokens per second on the Llama 70B model, the Sohu chip enables the creation of unattainable products with traditional GPUs. An 8xSohu server can effectively replace 160 H100 GPUs, showcasing their remarkable efficiency</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-04-16T18:23:33.000Z
        tags: transformers, vision
        -->
        <div class="card-divider">
          <a href="https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html">A Visual Guide to Vision Transformers | MDTURP</a>
        </div>
        <div class="card-section">
          <p>This is a visual guide (scroll story) to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-03-30T02:53:57.000Z
        tags: deep-learning, llms, transformers
        -->
        <div class="card-divider">
          <a href="https://thegradient.pub/mamba-explained">Mamba Explained</a>
        </div>
        <div class="card-image">
          <img src="https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&ixlib=rb-4.0.3&q=80&w=2000" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Is Attention all you need? Mamba, a novel AI model based on State Space Models (SSMs), emerges as a formidable alternative to the widely used Transformer models, addressing their inefficiency in processing long sequences.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-02-29T22:15:54.000Z
        tags: transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=rss----7f60cf5620c9---4">Position Embeddings for Vision Transformers, Explained</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*4SXrUaGxOcNmKG6e" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>The Math and the Code Behind Position Embeddings in Vision Transformers</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-02-29T22:15:43.000Z
        tags: attention, transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=rss----7f60cf5620c9---4">Attention for Vision Transformers, Explained</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*XLYx5OALyd914wu7" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>The Math and the Code Behind Attention Layers in Computer Vision</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2024-02-29T22:15:28.000Z
        tags: transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=rss----7f60cf5620c9---4">Vision Transformers, Explained</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*5r-4kq_qAbyFBnTt" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>A Full Walk-Through of Vision Transformers in PyTorch</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-10-05T22:46:06.000Z
        tags: arxiv, llms, transformers
        -->
        <div class="card-divider">
          <a href="https://arxiv.org/abs/2302.07730">[2302.07730] Transformer models: an introduction and catalog</a>
        </div>
        <div class="card-image">
          <img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory,...</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-09-25T23:23:55.000Z
        tags: transformers
        -->
        <div class="card-divider">
          <a href="https://dev.to/pavanbelagatti/hugging-face-101-a-tutorial-for-absolute-beginners-3b0l">Hugging Face 101: A Tutorial for Absolute Beginners!</a>
        </div>
        <div class="card-image">
          <img src="https://media.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fak3xsk6wk4hv3f6rhf6e.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Welcome to this beginner-friendly tutorial on sentiment analysis using Hugging Face's transformers...</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-09-25T23:22:21.000Z
        tags: generative, llms, transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161">Cracking Open the Hugging Face Transformers Library</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*Rkoquyw55K6qbFWF" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>A quick-start guide to using open-source LLMs</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-07-23T23:59:46.000Z
        tags: llms, pytorch, transformers
        -->
        <div class="card-divider">
          <a href="https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm">Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorc</a>
        </div>
        <div class="card-image">
          <img src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/pytorch-memory-hero.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>This article provides a series of techniques that can lower memory consumption in PyTorch (when training vision transformers and LLMs) by approximately 20x without sacrificing modeling performance and prediction accuracy.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-05-18T10:15:49.000Z
        tags: chatgpt, llms, reinforcement-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://thesequence.substack.com/p/edge-291-reinforcement-learning-with">Edge 291: Reinforcement Learning with Human Feedback</a>
        </div>
        <div class="card-image">
          <img src="https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1aaa4cc-10d6-4ada-bba0-f1f2f0793427_1024x1024.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>1) Reinforcement Learning with Human Feedback(RLHF) 2) The RLHF paper, 3) The transformer reinforcement learning framework.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-04-21T23:44:11.000Z
        tags: llms, transformers
        -->
        <div class="card-divider">
          <a href="https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency">Meta has built a massive new language AI—and it’s giving it away for free</a>
        </div>
        <div class="card-image">
          <img src="https://wp.technologyreview.com/wp-content/uploads/2022/05/tiles2-1.jpeg?resize=1200,600" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Facebook’s parent company is inviting researchers to pore over and pick apart the flaws in its version of GPT-3</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-04-19T17:21:17.000Z
        tags: transformers
        -->
        <div class="card-divider">
          <a href="https://txt.cohere.ai/what-are-transformer-models">What Are Transformer Models and How Do They Work?</a>
        </div>
        <div class="card-image">
          <img src="https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F06%2FTransformer-Models.jpg&w=640&q=75" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Transformer models are one of the most exciting new developments in machine learning. They were introduced in the paper Attention is All You Need. Transformers can be used to write stories, essays, poems, answer questions, translate between languages, chat with humans, and they can even pass exams that are hard for humans! But what are they? You’ll be happy to know that the architecture of transformer models is not that complex, it simply is a concatenation of some very useful components, each o</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-04-19T00:46:03.000Z
        tags: llms, transformers
        -->
        <div class="card-divider">
          <a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">Hacker News</a>
        </div>
        <div class="card-image">
          <img src="https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>A Cross-Section of the Most Relevant Literature To Get Up to Speed</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-03-31T15:36:22.000Z
        tags: deep-learning, nlp, rnns, transformers
        -->
        <div class="card-divider">
          <a href="https://johanwind.github.io/2023/03/23/rwkv_overview.html">Hacker News</a>
        </div>
        <div class="card-section">
          <p>I explain what is so unique about the RWKV language model.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-02-26T00:41:45.000Z
        tags: transformers
        -->
        <div class="card-divider">
          <a href="https://arxiv.org/abs/2302.10360">Optical Transformers</a>
        </div>
        <div class="card-image">
          <img src="https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>The rapidly increasing size of deep-learning models has caused renewed and growing interest in alternatives to digital computers to dramatically reduce the energy cost of running state-of-the-art...</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2023-02-07T22:09:44.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2">Hacker News</a>
        </div>
        <div class="card-section">
          <p>Many new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post — restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length. Notations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2022-12-18T22:40:22.000Z
        tags: deep-learning, machine-vision, pytorch, transformers
        -->
        <div class="card-divider">
          <a href="https://github.com/lucidrains/vit-pytorch">lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch</a>
        </div>
        <div class="card-image">
          <img src="https://opengraph.githubassets.com/9edc731f2a99d5a99777494dd7aaa43716ebad2f0f59b90b9e38e48aaecebb2c/lucidrains/vit-pytorch" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch - lucidrains/vit-pytorch</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2022-09-20T22:31:27.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada">All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Unders</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/resize:fit:1200/0*Wvg_pNDViACfg-IK.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Attention, Self-Attention, Multi-head Attention, Masked Multi-head Attention, Transformers, BERT, and GPT</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2022-09-20T22:31:11.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021">All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Unders</a>
        </div>
        <div class="card-image">
          <img src="https://miro.medium.com/v2/resize:fit:925/1*Rv_pntt-N2WL7LMbIptHxQ.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Attention, Self-Attention, Multi-head Attention, and Transformers</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-11-29T16:07:52.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://e2eml.school/transformers.html">Transformers</a>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-07-05T12:05:38.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j">GPT-J-6B: 6B JAX-Based Transformer – Aran Komatsuzaki</a>
        </div>
        <div class="card-image">
          <img src="https://arankomatsuzaki.wordpress.com/wp-content/uploads/2021/05/jax_logo.png?w=1200" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>Summary: We have released GPT-J-6B, 6B JAX-based (Mesh) Transformer LM (Github).GPT-J-6B performs nearly on par with 6.7B GPT-3 (or Curie) on various zero-shot down-streaming tasks.You can try out …</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-06-15T00:14:36.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://arxiv.org/pdf/2106.04554.pdf">2106 <span style="font-size: 1em;">📄</span></a>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-06-03T22:24:23.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://email.mg2.substack.com/c/eJwlkE1vwyAMhn9NuS3iIwFy4LDLjjtMvUd8uJSVQARkVf79SCtZtmzLev0-VjfwuRxqy7WhMy3t2EAleNYIrUFBe4WyBKcIYdMoZ-TU6IicJAp1uRWAVYeoWtkBbbuJweoWcjoPqJwZFeiuRiLsbI1xYJnUTFvDRi2oBYuJJOP81tW7C5AsKPiDcuQEKKp7a1u9sM8L_erhQ7vvZrB57c13gFh_svfQm2vRqd5yWaHUj-vecgk6VhQUxZRgjll_fqR8IAOfhaZMymniBBNiwFCwhhOjRw5YssuIV0-HupvatH2cYqgo87vZ363v_On2Nexml17XPYV2LJC0ieDeHNqb5ovM4iFB6ZTdopsinFJBMRZYdjIv2x0UE1hwwQjqqi73q6QcwBZBlxSSfwI84vEPRvWPwA">NielsRogge/Transformers-Tutorials: This repository contains demos I made with the Transformers library by HuggingFace.</a>
        </div>
        <div class="card-image">
          <img src="https://opengraph.githubassets.com/4f3c8f4c810cac2343d9c600ee9266cf541122b3afad7d56b25d31a26739f230/NielsRogge/Transformers-Tutorials" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>This repository contains demos I made with the Transformers library by HuggingFace. - NielsRogge/Transformers-Tutorials</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-05-29T00:40:47.000Z
        tags: deep-learning, transformers
        -->
        <div class="card-divider">
          <a href="https://jalammar.github.io/illustrated-transformer">The Illustrated Transformer – Jay Alammar – Visualizing machine learning on</a>
        </div>
        <div class="card-section">
          <p>Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)   Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese  Watch: MIT’s Deep Learning State of the Art lecture referencing this post  Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others  In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.  The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.  2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:     A High-Level Look Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-05-22T14:26:39.000Z
        tags: chatbots, deep-learning, nlp, transformers
        -->
        <div class="card-divider">
          <a href="https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication">Understanding Transformers, the machine learning model behind GPT-3</a>
        </div>
        <div class="card-image">
          <img src="https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2021%2F05%2FAI-Transformers-abstract-hed.jpg&signature=dab3715e95a415da68eaecb9b4aebcc7" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>How this novel neural network architecture changes the way we analyze complex data types, and powers revolutionary models like GPT-3 and BERT.</p>
        </div>
      </div>
    </div>
    <div class="cell">
      <div class="card" style="margin-bottom: 1rem; border-radius: 5px;">
        <!--
        created: 2021-05-18T16:56:07.000Z
        tags: deep-learning, nlp, transformers
        -->
        <div class="card-divider">
          <a href="https://theaisummer.com/transformer">How Transformers work in deep learning and NLP: an intuitive introduction | AI Summer</a>
        </div>
        <div class="card-image">
          <img src="https://theaisummer.com/static/6122618d7e1466853e88473ba375cdc7/ee604/transformer.png" alt="cover image" style="max-width: 100%;">
        </div>
        <div class="card-section">
          <p>An intuitive understanding on Transformers and how they are used in Machine Translation. After analyzing all subcomponents one by one such as self-attention and positional encodings , we explain the principles behind the Encoder and Decoder and why Transformers work so well</p>
        </div>
      </div>
    </div>
  </div>
</div>
